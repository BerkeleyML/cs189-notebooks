{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# [HW10] Problem: Expectation Maximization (EM) Algorithm: A closer look!\n",
                "\n",
                "Import necessary Python packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.stats import norm\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Define relevant functions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_data(mu_1, mu_2, n_examples, sigma_1 = 0.5, sigma_2 = 0.5):\n",
                "    # Generate sample data points from 2 distributions: (mu_1, sigma_1) and (mu_2, sigma_2)\n",
                "    d_1 = np.random.normal(mu_1, sigma_1, n_examples)\n",
                "    d_2 = np.random.normal(mu_2, sigma_2, n_examples)\n",
                "    x = np.concatenate([d_1, d_2])\n",
                "    return d_1, d_2, x\n",
                "\n",
                "def plot_data(d_1, d_2):\n",
                "    # Plot scatter plot of data samples, labeled by class\n",
                "    plt.figure()\n",
                "    plt.scatter(d_1, np.zeros(len(d_1)), c='b', s=80., marker='+')\n",
                "    plt.scatter(d_2, np.zeros(len(d_2)), c='r', s=80.)\n",
                "    plt.title(\"Sample data using: mu = \" + str(mu) + \" n_train = \" + str(len(d_1)+len(d_2)))\n",
                "    plt.show()\n",
                "    return\n",
                "\n",
                "def plot_data_and_distr(d_1, d_2, mu_1, mu_2, sigma_1=0.5, sigma_2=0.5, title = \"\"):\n",
                "    # Plot scatter plot of data samples overlayed with distribution of estimated means: mu_1 and mu_2\n",
                "    plt.scatter(d_1, np.zeros(len(d_1)), c='b')\n",
                "    scale = [min(mu_1-3*sigma_1, mu_2-3*sigma_2), max(mu_1+3*sigma_1, mu_2+3*sigma_2)]\n",
                "    plt.scatter(d_2, np.zeros(len(d_2)), c='r')\n",
                "    x_axis = np.arange(scale[0], scale[1], 0.001)\n",
                "    plt.plot(x_axis, norm.pdf(x_axis,mu_1, sigma_1), c='b')\n",
                "    x_axis = np.arange(scale[0], scale[1], 0.001)\n",
                "    plt.plot(x_axis, norm.pdf(x_axis,mu_2,sigma_2), c='r')\n",
                "    plt.title(title)\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def grad_ascent(x, mu, mu_true, sigma = 0.5, iterations = 250):\n",
                "    # Run gradient ascent on the likelihood of a point belonging to a class and compare the estimates of the mean\n",
                "    #     at each iteration with the true mean of the distribution\n",
                "    # Note: the original dataset comes from 2 distributions centered at mu and -mu, which this takes into account\n",
                "    #     with each update\n",
                "    diff_mu = []\n",
                "    alpha = 0.05\n",
                "    for i in range(iterations):\n",
                "        phi_1 = np.exp(-np.square(x-mu)/(2*np.square(sigma)))\n",
                "        phi_2 = np.exp(-np.square(x+mu)/(2*np.square(sigma)))\n",
                "        w = phi_1/(phi_1 + phi_2)\n",
                "        em = (1/len(x))*np.sum((2*w - 1)*x)\n",
                "        mu = mu*(1-alpha) + alpha*em\n",
                "        diff_mu.append(np.abs(mu-mu_true))\n",
                "    return mu, sigma,diff_mu\n",
                "\n",
                "\n",
                "def em(x, mu, mu_true, sigma = 0.5, iterations = 250):\n",
                "    # Run the EM algorithm to find the estimated mean of the dataset\n",
                "    # Note: the original dataset comes from 2 distributions centered at mu and -mu, which this takes into account\n",
                "    #     with each update\n",
                "    diff_mu = np.zeros(iterations)\n",
                "    for i in range(iterations):\n",
                "        phi_1 = np.exp(-np.square(x-mu)/(2*np.square(sigma)))\n",
                "        phi_2 = np.exp(-np.square(x+mu)/(2*np.square(sigma)))\n",
                "        w = phi_1/(phi_1 + phi_2)\n",
                "\n",
                "        mu = (1/len(x))*np.sum((2*w - 1)*x)\n",
                "        diff_mu[i]  = np.abs(mu-mu_true)\n",
                "    return mu, sigma,diff_mu\n",
                "\n",
                "def kmeans(x, mu, mu_true, sigma = 0.5, iterations = 250):\n",
                "    # Run the K means algorithm to find the estimated mean of the dataset\n",
                "    # Note: the original dataset comes from 2 distributions centered at mu and -mu, which this takes into account\n",
                "    #     with each update\n",
                "    diff_mu = np.zeros(iterations)\n",
                "\n",
                "    for i in range(iterations):\n",
                "        mu_1 = mu\n",
                "        mu_2 = -mu\n",
                "        set1 = []\n",
                "        set2 = []\n",
                "        for x_i in x:\n",
                "            if np.abs(x_i - mu_1) <= np.abs(x_i - mu_2):\n",
                "                set1.append(x_i)\n",
                "            else:\n",
                "                set2.append(x_i)\n",
                "        mu_1_new = np.mean(set1)\n",
                "        mu_2_new = np.mean(set2)\n",
                "        # Estimates two means and combines them to get mu for the next iteration\n",
                "        mu = np.abs(len(set1)*mu_1_new - len(set2)*mu_2_new)/(len(set1)+len(set2))\n",
                "        diff_mu[i]  = np.abs(mu-mu_true)\n",
                "    return mu, sigma, diff_mu\n",
                "\n",
                "\n",
                "def plot_differences(iterations, diff_mu_ga, diff_mu_em, mu, n_examples):\n",
                "    # Make plot comparing convergence of means to true mean for gradient descent and EM\n",
                "    plt.plot(np.arange(iterations), diff_mu_ga, c = 'r', label='GD')\n",
                "    plt.plot(np.arange(iterations), diff_mu_em, c = 'b', label='EM')\n",
                "    plt.legend()\n",
                "    plt.title(\"Difference between estimated and true mean when: mu = \" + str(mu) + \" n_train = \" + str(n_examples))\n",
                "    plt.xlabel(\"iterations for Training\")\n",
                "    plt.ylabel(\"Absolute value of difference to true mean\")\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def plot_ll(x, scale=[-5, 5]):\n",
                "    # if you want to visualize the likelihood function as a function of mu\n",
                "    mus = np.linspace(scale[0], scale[1], 200)\n",
                "    ll = np.zeros(mus.shape)\n",
                "    for j, mu in enumerate(mus):\n",
                "        ll[j] = 0.\n",
                "        for xi in x:\n",
                "            ll[j] += np.log(np.exp(-(xi-mu)**2/2)+np.exp(-(xi+mu)**2/2)) - np.log(2*np.sqrt(2*np.pi))\n",
                "    plt.plot(mus, ll)\n",
                "    plt.show()\n",
                "    return\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (l)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##############################################################################\n",
                "# Set this to True if you want to visualize the distribution estimated by each method\n",
                "visualize_distr = False\n",
                "##############################################################################\n",
                "\n",
                "#np.random.seed(12312)\n",
                "mu_list = [.5, 3.]\n",
                "n_train_list = [50]\n",
                "mu_start = 0.1\n",
                "\n",
                "for mu in mu_list:\n",
                "    for n_train in n_train_list:\n",
                "        d_1, d_2, x = generate_data(mu, -mu, n_examples = n_train)\n",
                "        plot_data(d_1, d_2)\n",
                "\n",
                "        # Code for part m\n",
                "\n",
                "        mu_ga, sigma_gd, diff_mu_ga = grad_ascent(x, mu_start, mu)\n",
                "        mu_em, sigma_em, diff_mu_em = em(x, mu_start, mu)\n",
                "        if visualize_distr:\n",
                "          plot_data_and_distr(d_1, d_2, mu_ga, -mu_ga, title = \"Estimated distribution using Gradient Ascent\")\n",
                "          plot_data_and_distr(d_1, d_2, mu_em, -mu_em, title = \"Estimated distribution using EM\")\n",
                "        print (\"------------\")\n",
                "        print(\"n_points:\", n_train*2, \", True mean:{:.3f}, GA (final) estimate:{:.3f}, EM (final) estimate:{:.3f}\".format(mu, mu_ga, mu_em)) #, \"GA (final) mean: {.3f}\", mu_ga, \"EM (final) mean: {.3f}\", mu_em)\n",
                "        print (\"------------\")\n",
                "        plot_differences(250, diff_mu_ga, diff_mu_em, mu, 2*n_train)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (m)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##############################################################################\n",
                "# Set this to True if you want to visualize the distribution estimated by each method\n",
                "visualize_distr = False\n",
                "##############################################################################\n",
                "\n",
                "#np.random.seed(12312)\n",
                "mu_list = [.5, 3.]\n",
                "n_train_list = [50]\n",
                "mu_start = 0.1\n",
                "\n",
                "for mu in mu_list:\n",
                "    for n_train in n_train_list:\n",
                "        d_1, d_2, x = generate_data(mu, -mu, n_examples = n_train)\n",
                "        plot_data(d_1, d_2)\n",
                "\n",
                "        mu_ga, sigma_gd, diff_mu_ga = grad_ascent(x, mu_start, mu)\n",
                "        mu_em, sigma_em, diff_mu_em = em(x, mu_start, mu)\n",
                "        mu_k, sigma_k, diff_mu_k = kmeans(x, mu_start, mu)\n",
                "        if visualize_distr:\n",
                "          plot_data_and_distr(d_1, d_2, mu_ga, -mu_ga, title = \"Estimated distribution using Gradient Ascent\")\n",
                "          plot_data_and_distr(d_1, d_2, mu_em, -mu_em, title = \"Estimated distribution using EM\")\n",
                "          plot_data_and_distr(d_1, d_2, mu_k, -mu_k, title = \"Estimated distribution using K Means\")\n",
                "        print (\"------------\")\n",
                "        print(\"True mean:{:.3f}, GA (final) estimate:{:.3f}, EM (final) estimate:{:.3f}, K-Means (final) estimate:{:.3f}\".format(mu, mu_ga, mu_em, mu_k))\n",
                "        print (\"------------\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}