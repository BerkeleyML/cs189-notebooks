{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# [HW10] Problem: Expectation Maximization (EM) Algorithm: In Action\n",
                "\n",
                "Import necessary Python packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import division\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Define relevant functions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#This function generate random mean and covariance\n",
                "def gauss_params_gen(num_clusters, num_dims, factor):\n",
                "    mu = np.random.randn(num_clusters,num_dims)*factor\n",
                "    sigma = np.random.randn(num_clusters,num_dims,num_dims)\n",
                "    for k in range(num_clusters):\n",
                "        sigma[k] = np.dot(sigma[k],sigma[k].T)\n",
                "\n",
                "    return (mu, sigma)\n",
                "\n",
                "#Given mean and covariance generate data\n",
                "def data_gen(mu, sigma, num_clusters, num_samples):\n",
                "    labels = []\n",
                "    X = []\n",
                "    cluster_prob = np.array([np.random.rand() for k in  range(num_clusters)])\n",
                "    cluster_num_samples = (num_samples * cluster_prob / sum(cluster_prob)).astype(int)\n",
                "    cluster_num_samples[-1] = num_samples-sum(cluster_num_samples[:-1])\n",
                "\n",
                "    for k, ks in enumerate(cluster_num_samples):\n",
                "        labels.append([k]*ks)\n",
                "        X.append(np.random.multivariate_normal(mu[k], sigma[k], ks))\n",
                "\n",
                "    # shuffle data\n",
                "    randomize = np.arange(num_samples)\n",
                "    np.random.shuffle(randomize)\n",
                "    X =  np.vstack(X)[randomize]\n",
                "    labels =  np.array(sum(labels,[]))[randomize]\n",
                "\n",
                "    return X, labels\n",
                "\n",
                "\n",
                "def data2D_plot(ax, x, labels, centers, cmap, title):\n",
                "    data = {'x0': x[:,0], 'x1': x[:,1], 'label': labels}\n",
                "    ax.scatter(data['x0'], data['x1'], c=data['label'], cmap=cmap, s=20, alpha=0.3)\n",
                "    ax.scatter(centers[:, 0], centers[:, 1], c=np.arange(np.shape(centers)[0]), cmap=cmap, s=50, alpha=1)\n",
                "    ax.scatter(centers[:, 0], centers[:, 1], c='black', cmap=cmap, s=20, alpha=1)\n",
                "    ax.title.set_text(title)\n",
                "\n",
                "def plot_init_means(x, mus, algs, fname):\n",
                "    import matplotlib.cm as cm\n",
                "    fig = plt.figure()\n",
                "    plt.scatter(x[:,0], x[:,1], c='gray', cmap='viridis', s=20, alpha= 0.4, label='data')\n",
                "    for mu, alg, clr in zip(mus, algs, cm.viridis(np.linspace(0, 1, len(mus)))):\n",
                "        plt.scatter(mu[:,0], mu[:, 1], c=clr, s=50, label=alg)\n",
                "        plt.scatter(mu[:, 0], mu[:, 1], c='black', s=10, alpha=1)\n",
                "    legend = plt.legend(loc='upper right', fontsize='small')\n",
                "    plt.title('Initial guesses for centroids')\n",
                "    fig.savefig(fname)\n",
                "\n",
                "def loss_plot(loss, title, xlabel, ylabel, fname):\n",
                "\tfig = plt.figure(figsize = (13, 6))\n",
                "\tplt.plot(np.array(loss))\n",
                "\tplt.title(title)\n",
                "\tplt.xlabel(xlabel)\n",
                "\tplt.ylabel(ylabel)\n",
                "\tfig.savefig(fname)\n",
                "\n",
                "\n",
                "def gaussian_pdf (X, mu, sigma):\n",
                "    # Gaussian probability density function\n",
                "    return np.linalg.det(sigma) ** -.5 ** (2 * np.pi) ** (-X.shape[1]/2.) \\\n",
                "                    * np.exp(-.5 * np.einsum('ij, ij -> i',\\\n",
                "                    X - mu, np.dot(np.linalg.inv(sigma) , (X - mu).T).T ) )\n",
                "\n",
                "def EM_initial_guess (num_dims, data, num_samples, num_clusters):\n",
                "    # randomly choose the starting centroids/means\n",
                "    # as num_clusters of the points from datasets\n",
                "    mu = data[np.random.choice(num_samples, num_clusters, False), :]\n",
                "\n",
                "    # initialize the covariance matrice for each gaussian\n",
                "    sigma = [np.eye(num_dims)] * num_clusters\n",
                "\n",
                "    # initialize the probabilities/weights for each gaussian\n",
                "    # begin with equal weight for each gaussian\n",
                "    alpha = [1./num_clusters] * num_clusters\n",
                "\n",
                "    return mu, sigma, alpha\n",
                "\n",
                "def EM_E_step (num_clusters, num_samples, data, mu, sigma, alpha):\n",
                "    ## Vectorized implementation of e-step equation to calculate the\n",
                "    ## membership for each of k -gaussians\n",
                "    Q = np.zeros((num_samples, num_clusters))\n",
                "    for k in range(num_clusters):\n",
                "        Q[:, k] = alpha[k] * gaussian_pdf(data, mu[k], sigma[k])\n",
                "\n",
                "    ## Normalize so that the responsibility matrix is row stochastic\n",
                "    Q = (Q.T / np.sum(Q, axis = 1)).T\n",
                "\n",
                "    return Q\n",
                "\n",
                "def EM_M_step (num_clusters, num_dims, num_samples, Q, data):\n",
                "\n",
                "    # M Step\n",
                "    ## calculate the new mean and covariance for each gaussian by\n",
                "    ## utilizing the new responsibilities\n",
                "    mu      = np.zeros((num_clusters, num_dims))\n",
                "    sigma   = np.zeros((num_clusters, num_dims, num_dims))\n",
                "    alpha = np.zeros(num_clusters)\n",
                "\n",
                "    ## The number of datapoints belonging to each gaussian\n",
                "    num_samples_per_cluster = np.sum(Q, axis = 0)\n",
                "\n",
                "    for k in range(num_clusters):\n",
                "        ## means\n",
                "        mu[k] = 1. / num_samples_per_cluster[k] * np.sum(Q[:, k] * data.T, axis = 1).T\n",
                "        centered_data = np.matrix(data - mu[k])\n",
                "\n",
                "        ## covariances\n",
                "        sigma[k] = np.array(1. / num_samples_per_cluster[k] * np.dot(np.multiply(centered_data.T,  Q[:, k]), centered_data))\n",
                "\n",
                "        ## and finally the probabilities\n",
                "        alpha[k] = 1. / (num_clusters*num_samples) * num_samples_per_cluster[k]\n",
                "\n",
                "    return mu, sigma, alpha\n",
                "\n",
                "def EM_log_likelihood_calc (num_clusters, num_samples, data, mu, sigma, alpha):\n",
                "    L = np.zeros((num_samples, num_clusters))\n",
                "    for k in range(num_clusters):\n",
                "        L[:, k] = alpha[k] * gaussian_pdf(data, mu[k], sigma[k])\n",
                "    return np.sum(np.log(np.sum(L, axis = 1)))\n",
                "\n",
                "\n",
                "def EM_calc (num_dims, num_samples, num_clusters, x):\n",
                "    log_likelihoods = []\n",
                "    labels \t\t\t= []\n",
                "    iter_cnt        = 0\n",
                "    epsilon         = 0.0001\n",
                "    max_iters       = 200\n",
                "    update          = 2*epsilon\n",
                "\n",
                "    # initial guess\n",
                "    mu, sigma, alpha = EM_initial_guess(num_dims, x, num_samples, num_clusters)\n",
                "    mus = [mu]\n",
                "    sigmas = [sigma]\n",
                "    alphas = [alpha]\n",
                "    while (update > epsilon) and (iter_cnt < max_iters):\n",
                "        iter_cnt += 1\n",
                "\n",
                "        # E - Step\n",
                "        Q = EM_E_step (num_clusters, num_samples, x, mu, sigma, alpha)\n",
                "\n",
                "        # M - Step\n",
                "        mu, sigma, alpha = EM_M_step (num_clusters, num_dims, num_samples, Q, x)\n",
                "\n",
                "        mus.append(mu)\n",
                "        sigmas.append(sigma)\n",
                "        alphas.append(alpha)\n",
                "\n",
                "        # Likelihood computation\n",
                "        log_likelihoods.append(EM_log_likelihood_calc(num_clusters, num_samples, x, mu, sigma, alpha))\n",
                "\n",
                "        # check convergence\n",
                "        if iter_cnt >= 2 :\n",
                "            update = np.abs(log_likelihoods[-1] - log_likelihoods[-2])\n",
                "\n",
                "        # logging\n",
                "        print(\"iteration {}, update {}\".format(iter_cnt, update))\n",
                "\n",
                "        # print current iteration\n",
                "        labels.append(np.argmax(Q, axis = 1))\n",
                "\n",
                "    return labels, log_likelihoods, {'mu': mus, 'sigma': sigmas, 'alpha': alphas}\n",
                "\n",
                "def kmeans_initial_guess (data, num_samples, num_clusters):\n",
                "    # randomly choose the starting centroids/means\n",
                "    # as num_clusters of the points from datasets\n",
                "    mu = data[np.random.choice(num_samples, num_clusters, False), :]\n",
                "    return mu\n",
                "\n",
                "def kmeans_get_labels(num_clusters, num_samples, num_dims, data, mu):\n",
                "    # set all dataset points to the best cluster according to minimal distance\n",
                "    #from centroid of each cluster\n",
                "    dist = np.zeros((num_clusters, num_samples))\n",
                "    for k in range(num_clusters):\n",
                "        dist[k] = np.linalg.norm(data - mu[k], axis=1)\n",
                "\n",
                "    labels = np.argmin(dist, axis=0)\n",
                "\n",
                "    return labels\n",
                "\n",
                "def kmeans_get_means(num_clusters, num_dims, data, labels):\n",
                "    # Compute the new means given the reclustering of the data\n",
                "    mu = np.zeros((num_clusters, num_dims))\n",
                "    for k in range(num_clusters):\n",
                "        idx_list = np.where(labels == k)[0]\n",
                "        if (len(idx_list) == 0):\n",
                "            r = np.random.randint(len(data))\n",
                "            mu[k] = data[r,:]\n",
                "        else:\n",
                "            mu[k] = np.mean(data[idx_list], axis=0)\n",
                "    return mu\n",
                "\n",
                "def kmeans_calc_loss(num_clusters, num_samples, data, mu, labels):\n",
                "    dist = np.zeros((num_samples, num_clusters))\n",
                "    for j in range(num_samples):\n",
                "        for k in range(num_clusters):\n",
                "            if (labels[j] == k) :\n",
                "                dist[j,k] = np.linalg.norm(data[j] - mu[k])\n",
                "    return sum(sum(dist))\n",
                "\n",
                "\n",
                "def k_means_calc (num_dims, num_samples, num_clusters, x):\n",
                "    loss            = []\n",
                "    labels\t\t\t= []\n",
                "    iter_cnt        = 0\n",
                "    epsilon         = 0.00001\n",
                "    max_iters       = 100\n",
                "    update          = 2*epsilon\n",
                "\n",
                "    # initial guess\n",
                "    mu = [kmeans_initial_guess(x, num_samples, num_clusters)]\n",
                "\n",
                "    while (update > epsilon) and (iter_cnt < max_iters):\n",
                "        iter_cnt += 1\n",
                "        # Assign labels to each datapoint based on centroid\n",
                "        labels.append(kmeans_get_labels(num_clusters, num_samples, num_dims, x, mu[-1]))\n",
                "\n",
                "        # Assign centroid based on labels\n",
                "        mu.append(kmeans_get_means(num_clusters, num_dims, x, labels[-1]))\n",
                "        # check convergence\n",
                "        if iter_cnt >= 2 :\n",
                "            update = np.linalg.norm(mu[-1] - mu[-2], None)\n",
                "\n",
                "        # Print distance to centroids vs iteration\n",
                "        loss.append(kmeans_calc_loss(num_clusters, num_samples, x, mu[-1], labels[-1]))\n",
                "\n",
                "        # logging\n",
                "        print(\"iteration {}, update {}\".format(iter_cnt, update))\n",
                "\n",
                "\n",
                "    return labels, loss, mu\n",
                "\n",
                "def k_qda_initial_guess (num_dims, data, num_samples, num_clusters):\n",
                "\t# randomly choose the starting centroids/means\n",
                "\t# as num_clusters of the points from datasets\n",
                "    mu = data[np.random.choice(num_samples, num_clusters, False), :]\n",
                "\n",
                "\t# initialize the covariance matrice for each gaussian\n",
                "    sigma = [np.eye(num_dims)] * num_clusters\n",
                "\n",
                "    return mu, sigma\n",
                "\n",
                "def k_qda_get_parms(num_clusters, num_dims, data, labels):\n",
                "\t## calculate the new mean and covariance for each gaussian\n",
                "    mu      = np.zeros((num_clusters, num_dims))\n",
                "    sigma   = np.zeros((num_clusters, num_dims, num_dims))\n",
                "\n",
                "    for k in range(num_clusters):\n",
                "        c_k = labels==k\n",
                "        if (len(data[c_k]) == 0):\n",
                "            r = np.random.randint(len(data))\n",
                "            mu[k] = data[r,:]\n",
                "        else:\n",
                "            mu[k] = np.mean(data[c_k], axis=0)\n",
                "\n",
                "        if (len(data[c_k]) > 1):\n",
                "            centered_data = np.matrix(data[c_k] - mu[k])\n",
                "            sigma[k] = np.array(1. / len(data[c_k]) * np.dot(centered_data.T, centered_data))\n",
                "        else:\n",
                "            sigma[k] = np.eye(num_dims)\n",
                "\n",
                "    return mu, sigma\n",
                "\n",
                "def k_qda_get_labels(num_clusters, num_samples, mu, sigma, data):\n",
                "    # set all dataset points to the best cluster according to best\n",
                "    # probability given calculated means and covariances\n",
                "    dist = np.zeros((num_clusters, num_samples))\n",
                "    for k in range(num_clusters):\n",
                "        data_center = (data - mu[k])\n",
                "        dist[k] = np.einsum('ij, ij -> i', data_center, np.dot(np.linalg.inv(sigma[k]) , data_center.T).T )\n",
                "        labels = np.argmin(dist, axis=0)\n",
                "    return labels\n",
                "\n",
                "\n",
                "def k_qda_calc(num_dims, num_samples, num_clusters, x):\n",
                "    loss            = []\n",
                "    labels\t\t\t= []\n",
                "    iter_cnt        = 0\n",
                "    epsilon         = 0.00001\n",
                "    max_iters       = 100\n",
                "    update          = 2*epsilon\n",
                "\n",
                "    # initial guess\n",
                "    mu, sigma = k_qda_initial_guess(num_dims, x, num_samples, num_clusters)\n",
                "    mus = [mu]\n",
                "    sigmas = [sigma]\n",
                "    while (update > epsilon) and (iter_cnt < max_iters):\n",
                "        iter_cnt += 1\n",
                "\n",
                "\t   # Assign labels to each datapoint based on probability\n",
                "        labels.append(k_qda_get_labels(num_clusters, num_samples, mus[-1], sigmas[-1], x))\n",
                "\n",
                "\t   # Assign centroid and covarince based on labels\n",
                "        mu, sigma = k_qda_get_parms(num_clusters, num_dims, x, labels[-1])\n",
                "\n",
                "        mus.append(mu)\n",
                "        sigmas.append(sigma)\n",
                "\n",
                "\t   # check convergence\n",
                "        if iter_cnt >= 2 :\n",
                "            update = np.linalg.norm(mus[-1] - mus[-2], None)\n",
                "            update += np.linalg.norm(sigmas[-1] - sigmas[-2], None)\n",
                "\n",
                "\t   # logging\n",
                "        print(\"iteration {}, update {}\".format(iter_cnt, update))\n",
                "    return labels, {'mu': mus, 'sigma': sigmas}\n",
                "\n",
                "\n",
                "def experiments(seed, factor, dir='plots', num_samples=500, num_clusters=3):\n",
                "\n",
                "    if not os.path.exists(dir):\n",
                "        os.makedirs(dir)\n",
                "\n",
                "    np.random.seed(seed)\n",
                "    num_dims     = 2\n",
                "\n",
                "    # generate data samples\n",
                "    (mu, sigma) = gauss_params_gen(num_clusters, num_dims, factor)\n",
                "    x, true_labels   = data_gen(mu, sigma, num_clusters, num_samples)\n",
                "    #### Expectation-Maximization\n",
                "    EM_labels, log_likelihoods, EM_parms = EM_calc (num_dims, num_samples, num_clusters, x)\n",
                "    #### K QDA\n",
                "    kqda_labels, kqda_parms = k_qda_calc(num_dims, num_samples, num_clusters, x)\n",
                "    #### K means\n",
                "    kmeans_labels, loss, kmean_mus = k_means_calc (num_dims, num_samples, num_clusters, x)\n",
                "\n",
                "    #Collect all results\n",
                "    labels = [true_labels, EM_labels[-1], kqda_labels[-1], kmeans_labels[-1]]\n",
                "    mus_fin = np.array([mu, EM_parms['mu'][-1], kqda_parms['mu'][-1], kmean_mus[-1]])\n",
                "    algs = np.array(['True', 'EM', 'KQDA', 'Kmeans'])\n",
                "\n",
                "    #### Plot\n",
                "    fig = plt.figure()\n",
                "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
                "    for i, (lbl,alg, mu) in enumerate(zip(labels, algs, mus_fin)):\n",
                "        ax = fig.add_subplot(2, 2, i+1)\n",
                "        data2D_plot(ax, x, lbl, mu, 'viridis', alg)\n",
                "\n",
                "    fname = os.path.join(dir, 'Results_s{}_f{}_n{}_k{}.png'.format(seed,factor,num_samples, num_clusters))\n",
                "    fig.savefig(fname)\n",
                "\n",
                "    mus_init = np.array([mu, EM_parms['mu'][0], kqda_parms['mu'][0], kmean_mus[0]])\n",
                "    init_mu_fname = os.path.join(dir, 'init_mu_s{}_f{}_n{}_k{}.png'.format(seed,factor, num_samples, num_clusters))\n",
                "    plot_init_means(x, mus_init, algs, init_mu_fname)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (a)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "experiments(seed=11, factor=1, num_samples=500, num_clusters=3)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (b)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "experiments(seed=63, factor=10, num_samples=500, num_clusters=3)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}