{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 1\n",
                "In this part we study the learning problem from the pdf numerically: our data is one-dimensional $\\{x_i\\}_{i=1}^n \\subseteq \\mathbb{R}$  and the labels are $\\{y_i = \\mathrm{sign}(x_i > \\theta)\\}_{i=1}^n$ where $\\theta$ is some unknown threshold (i.e. we have separable 1d data). We consider the following optimization problem:\n",
                "$$\\min_{m,c}\\sum_{i=1}^n \\log\\left(1 + \\exp(-m(x_i + c)y_i)\\right).$$\n",
                "\n",
                "As we saw in the theoretical part, the minimum of this objective is always located infinitely far from the origin, so we cannot take the true minimizer. Therefore, we consider the output of the gradient descent after some finite number of steps.\n",
                "\n",
                "The next cell contains implementations of the loss, its gradient and the gradient descent with constant step size.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#implementation of the gradient descent\n",
                "def loss_1d(x, y, m, c):\n",
                "    return np.mean(np.log(1 + np.exp(-m * (x - c) * y)))\n",
                "\n",
                "def grad_1d(x, y, m, c):\n",
                "#     inputs:\n",
                "#     x is a 1d vector\n",
                "#     y is a 1d vector of +/- 1 of the same length as x\n",
                "#     a, b are real numbers\n",
                "    return np.mean(-np.vstack(((x -c) * y, -m * y))/\n",
                "                   (1 + np.exp(m * (x - c) * y)), axis=1)\n",
                "\n",
                "def grad_descent_1d(x, y, m_init, c_init, step_size, n_iter):\n",
                "#     inputs:\n",
                "#     x is a 1d vector\n",
                "#     y is a 1d vector of +/- 1 of the same length as x\n",
                "#     a_init, b_init and step_size are real numbers\n",
                "#     n_iter is an  integer\n",
                "    w = np.array([m_init, c_init], dtype=float)\n",
                "    losses = [loss_1d(x, y, w[0], w[1])]\n",
                "    traj = [np.copy(w)]\n",
                "    for step_number in range(n_iter):\n",
                "        w -= step_size * grad_1d(x, y, w[0], w[1])\n",
                "        traj += [np.copy(w)]\n",
                "        losses += [loss_1d(x, y, w[0], w[1])]\n",
                "    return traj, losses\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**1) Look at the code and the output of the following cell. What does \"threshold with maximum margin\" mean in this case?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#generate 1d data\n",
                "def gen_1d_data(n=7):\n",
                "    x = np.sort(np.random.rand(n))\n",
                "    y = np.ones(n)\n",
                "    threshold_index = np.random.randint(n-1)\n",
                "    y[:threshold_index + 1] = -1\n",
                "    threshold = (x[threshold_index] + x[1 + threshold_index])/2#find the real max-margin threshold\n",
                "    return x, y, threshold\n",
                "\n",
                "x, y, threshold = gen_1d_data(n=7)\n",
                "\n",
                "#plot the data\n",
                "plt.scatter(x, np.zeros(len(x)), c = ['b' if label==1 else 'r' for label in y])\n",
                "plt.plot([threshold] * 2, [-0.1, 0.1], c='g', label='threshold with maximum margin')\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**2) Run the next cell several times. Don't hesitate to play with the amount of data, step size and number of iterations. What classifier does gradient descent converge to? Does it coincide with the result of the theoretical part? Is the convergence fast?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x, y, threshold = gen_1d_data(n=7)\n",
                "traj_1d, losses_1d = grad_descent_1d(x, y, m_init=1, c_init=0, step_size=0.05, n_iter=int(1e4))\n",
                "m_1d, c_1d = traj_1d[-1]\n",
                "print(\"true max margin threshold: \", threshold, \"\\nlearned threshold: \", c_1d )\n",
                "\n",
                "#plot how b evolves\n",
                "plt.plot([w[1] for w in traj_1d], label='c')\n",
                "plt.plot(threshold * np.ones(len(traj_1d)), label='max-margin treshold')\n",
                "plt.xlabel(\"iteration\")\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The next cell plots $m$ against iterations.\n",
                "\n",
                "**3) Does the plot below support the results from the theoretical part? What do you think is the rate of growth of m?** Don't forget that you can change the step size and the number of iterations above.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#plot how m evolves\n",
                "plt.plot([w[0] for w in traj_1d])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 2\n",
                "In the previous part we looked at learning an affine function parametrized as $m(x-c)$. This is different from the usual parametrization $w_0 x + w_1$. In this part we compare the two.\n",
                "\n",
                "Note that learning an affine function $f(x) = w_0 x + w_1$ on 1d data $\\{x_i\\}_{i=1}^n$ is the same as learning a linear function $l({ \\bf x}) = w_0 x_0 + w_1 x_1$ on the (lifted)   2d data $\\{{\\bf x}_i\\}_{i=1}^n = \\{(x_i, 1)^\\top\\}_{i=1}^n$. Therefore we can transition to linear classifiers for high-dimensional spaces: our data matrix is ${\\bf X} \\in \\mathbb{R}^{n \\times d}$ ($n$ is the number of data points, $d$ is the ambient dimension), and we still assume that we have two linearly separable classes: $y = \\mathrm{sign}({\\bf X\\theta})$ where sign is applied elementwise and ${\\bf \\theta}\\in \\mathbb{R}^d$ is some unknown weight vector. To learn the weights we consider the following optimization problem:\n",
                "$$\\min_{w \\in \\mathbb{R}^d} \\sum_{i=1}^n \\log\\left(1 + \\exp(-{\\bf w}^\\top {\\bf x}_i)\\right).$$\n",
                "\n",
                "For the same reason as before we consider the output of the gradient descent after some finite number of steps.\n",
                "\n",
                "The next cell contains implementations of the loss function, its gradient and gradient descent with constant step size.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# implement logistic loss and its gradient\n",
                "def log_losses(w, X, y):\n",
                "    #vector of losses in each data point\n",
                "    return np.log(1 + np.exp(-(X @ w) * y))\n",
                "\n",
                "def log_loss(w, X, y):\n",
                "    return np.mean(log_losses(w, X, y))\n",
                "\n",
                "def log_loss_gradients(w, X, y):\n",
                "    #n by d matrix: i-th row is the gradient in i-th data point\n",
                "    return -(X.T * y / (1 + np.exp((X @ w) * y))).T\n",
                "\n",
                "def log_loss_gradient(w, X, y):\n",
                "    return np.mean(log_loss_gradients(w, X, y), axis=0)\n",
                "\n",
                "#implement gradient descent\n",
                "def grad_descent(X, y, w_init, step_size, n_iter):\n",
                "    w = np.copy(w_init)\n",
                "    traj = [np.copy(w)]\n",
                "    losses = [log_loss(w, X, y)]\n",
                "    for _ in range(n_iter):\n",
                "        w -= step_size * log_loss_gradient(w, X, y)\n",
                "        traj += [np.copy(w)]\n",
                "        losses += [log_loss(w, X, y)]\n",
                "    return traj, losses\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**1) Run the next cell several times. Don't hesitate to play with the amount of data, step size and number of iterations. Do both methods converge to the same classifier? For which parametrization is convergence faster?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#generate the data as before\n",
                "x, y, threshold = gen_1d_data(n=10)\n",
                "\n",
                "\n",
                "#lift the data into 2d to learn affine function instead of linear\n",
                "X = np.vstack((x, np.ones(len(x)))).T\n",
                "\n",
                "#learn affine functions with two different parametrizations\n",
                "step_size = 0.1\n",
                "n_iter = int(1e4)\n",
                "traj_1d, losses_1d = grad_descent_1d(x, y, m_init=1, c_init=0, step_size=step_size, n_iter=n_iter)\n",
                "traj_gd, losses = grad_descent(X, y, w_init=np.array([1., 0.]), step_size=step_size, n_iter=n_iter)\n",
                "\n",
                "#compare learned thresholds:\n",
                "print(\"true max margin threshold: \", threshold,\n",
                "      \"\\n part 1 learned threshold: \", traj_1d[-1][1],\n",
                "      \"\\n part 2 learned threshold: \", -traj_gd[-1][1]/traj_gd[-1][0],)\n",
                "\n",
                "#compare evolution of thresholds\n",
                "plt.plot([w[1] for w in traj_1d], label='part 1 threshold')\n",
                "plt.plot([-w[1]/w[0] for w in traj_gd], label='part 2 threshold')\n",
                "plt.plot(threshold * np.ones(len(traj_gd)), label='max margin threshold')\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The next cell plots the norm of weights against iterations.\n",
                "\n",
                "**2) What do you think is the rate of growth of the norm of the weights? Is it qualitatively different from the rate of growth of $m$ from part 1?** Don't forget that you can change the step size and the number of iterations above.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#print magnitudes of weights:\n",
                "plt.plot([np.linalg.norm(w) for w in traj_gd])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the previous experiments it may seem that both ways of learning an affine function are almost the same and always lead to the same results. However, it is not always the case. In the following cell we only have two data points: $x_1 = -1$, $x_2 = 2$ with labels $y_1 = -1$, $y_2 = 1$.\n",
                "\n",
                "**3) Run the following cell. Do both descents converge to the same threshold? Can you choose such sample size and step size so that the \"part 2\" threshold converges to the maximum margin value?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#sometimes the predictions are different\n",
                "n = 2\n",
                "x = np.array([-1, 2])\n",
                "y = np.array([-1, 1])\n",
                "threshold = np.mean(x)\n",
                "\n",
                "#lift the data into 2d to learn affine function instead of linear\n",
                "X = np.vstack((x, np.ones(n))).T\n",
                "\n",
                "#learn affine functions with two different parametrizations\n",
                "n_iter = int(1e4)\n",
                "traj_1d, losses_1d = grad_descent_1d(x, y, m_init=1, c_init=0, step_size=0.1, n_iter=n_iter)\n",
                "\n",
                "#now start the new gradient descent at the weights found by the previous one\n",
                "traj_gd, losses_gd = grad_descent(X, y, w_init= np.array([1, -0.5]), step_size=10., n_iter=n_iter)\n",
                "\n",
                "#compare learned thresholds:\n",
                "print(\"true max margin threshold: \", threshold,\n",
                "      \"\\n part 1 learned threshold: \", -traj_1d[-1][1],\n",
                "      \"\\n part 2 learned threshold: \", -traj_gd[-1][1]/traj_gd[-1][0])\n",
                "\n",
                "#compare evolution of thresholds\n",
                "plt.plot([w[1] for w in traj_1d], label='part 1 threshold')\n",
                "plt.plot([-w[1]/w[0] for w in traj_gd], label='part 2 threshold')\n",
                "plt.plot(threshold * np.ones(len(traj_gd)), label='max margin threshold')\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What we've seen in the previous cell happened because the convergence of the gradient descent on lifted data is very slow. To demonstrate that, in the next cell we implement another version of gradient descent: instead of making a step proportional to the gradient,  we make a step in the direction of the gradient, buth the length of the step is proportional to $1/\\sqrt{i}$ where $i$ is the iteration number.\n",
                "\n",
                "**4) Make sure that your data is x = [-1, 2], y = [-1,1]. Run the code in the following cell. Do both descents converge to the maximum margin threshold now?** Note that since the gradient becomes effectively zero, some numerical issues arise with the rescaled GD after large number of steps.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grad_descent_rescaled(X, y, w_init, step_size, n_iter):\n",
                "    w = np.copy(w_init)\n",
                "    traj = [np.copy(w)]\n",
                "    losses = [log_loss(w, X, y)]\n",
                "    for step_number in range(n_iter):\n",
                "        grad = log_loss_gradient(w, X, y)\n",
                "        w -= step_size * grad / np.linalg.norm(grad) / (step_number + 1)**0.5\n",
                "        traj += [np.copy(w)]\n",
                "        losses += [log_loss(w, X, y)]\n",
                "    return traj, losses\n",
                "\n",
                "traj_gd_rescaled, losses_rescaled = grad_descent_rescaled(X, y, w_init= np.array([0., 0.]),\n",
                "                                                          step_size=2.,\n",
                "                                                          n_iter=n_iter)\n",
                "\n",
                "plt.plot([w[1] for w in traj_1d], label='part 1 threshold')\n",
                "plt.plot([-w[1]/w[0] for w in traj_gd_rescaled], label='part 2 threshold')\n",
                "plt.plot(threshold * np.ones(len(traj_gd)), label='max margin threshold')\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we managed to deal with the slow convergence, but was that the only issue why the predictions were different? The answer turns out to be \"no\"!\n",
                "\n",
                "**5) Now repeat the previous two tasks (3 and 4) but take the data to be x = [-1, 4], y = [-1,1]. Can you make rescaled descent to converge to the max margin threshold by tweaking step size and number of iterations? What if you set x = [-0.1, 0.4], y = [-1,1]?**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To understand what's going on we can look at the 2d picture. The code below plots the separatin line that our GD finds after lifting the data.\n",
                "\n",
                "**6) Try increasing the coordinate of the second data point (i.e. try x = [-1, 2], [-1, 2.5], [-1, 3] and so on). What happens with separating line when that coordinate becomes larger than 3? In what sense is this a maximum margin classifier?** Note that the scale on both axes is the same for convenience, so the angles are visualized correctly. **Can you explain why the scale of the data was so important in the previous experiment?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn\n",
                "n = 2\n",
                "x = np.array([-1, 4])\n",
                "y = np.array([-1, 1])\n",
                "threshold = np.mean(x)\n",
                "X = np.vstack((x, np.ones(n))).T\n",
                "\n",
                "#learn affine functions with two different parametrizations\n",
                "n_iter = int(1e4)\n",
                "\n",
                "traj_gd_rescaled, losses_rescaled = grad_descent_rescaled(X, y, w_init= np.array([0., 0.]),\n",
                "                                                          step_size=2.,\n",
                "                                                          n_iter=n_iter)\n",
                "\n",
                "#find the last weight vector which does not contain nan or inf\n",
                "w = np.array(traj_gd_rescaled)[ [max(w) < np.inf and min(w) > -np.inf for w in  traj_gd_rescaled], :][-1, :]\n",
                "\n",
                "\n",
                "fig = plt.figure(figsize=(20,20))\n",
                "plt.ylim([0., 1.1])\n",
                "plt.xlim(-2, 7)\n",
                "plt.scatter(X[:, 0], X[:,1], c = ['b' if label==1 else 'r' for label in y])\n",
                "\n",
                "plt.plot([threshold] * 2, [0, 1.1], c='g', label='threshold with maximum 1d margin')\n",
                "plt.plot([w[1], -w[1]], [-w[0], w[0]], c='y', label='separating line GD found in 2d')\n",
                "plt.gca().set_aspect('equal', adjustable='box')\n",
                "\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 3\n",
                "\n",
                "In this part we only work with high-dimensional linearly separable data. We've seen before that GD on logistic loss converges to the max margin separating solution. Let's compare our gradient descent with Newton's method in terms of the margin that they learn.\n",
                "\n",
                "The next cell contains the implementaion of the hessian of the loss and of the Newton's method for optimization.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#implement hessian and Newton's method\n",
                "\n",
                "def log_loss_hessian(w, X, y):\n",
                "    n, d = X.shape\n",
                "    exps = np.exp(-(X @ w) * y)\n",
                "    return (X.T * exps / (1 + exps)**2) @ X / n\n",
                "\n",
                "def newtons_method(X, y, w_init, n_iter):\n",
                "    n, d = X.shape\n",
                "    w = np.copy(w_init)\n",
                "    traj = [np.copy(w)]\n",
                "    losses = [log_loss(w, X, y)]\n",
                "    for _ in range(n_iter):\n",
                "        w -= 0.5 * np.linalg.pinv(log_loss_hessian(w, X, y)) @ log_loss_gradient(w, X, y)\n",
                "        traj += [np.copy(w)]\n",
                "        losses += [log_loss(w, X, y)]\n",
                "    return traj, losses\n",
                "\n",
                "\n",
                "def compute_margin(X, w, y):\n",
                "    return np.min(X @ (w/ np.linalg.norm(w)) * y)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**1) Run the following cell several times, and try different values of d and n. Which method converges faster and which finds solutions with better margins? What happens as d grows?**\n",
                "\n",
                "**2) When d > n the hessian is degenerate. How does our implementation of Newton's method work in this case?.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#generate separable data\n",
                "d = 20\n",
                "n = 15\n",
                "X = np.random.randn(n, d)\n",
                "true_weights = np.random.randn(d) #these are NOT MAX MARGIN weights\n",
                "y = np.sign(X @ true_weights).astype('int')\n",
                "\n",
                "w_init = np.zeros(d)\n",
                "n_iter = int(1e3)\n",
                "traj_gd, losses_gd = grad_descent_rescaled(X, y, w_init, step_size=0.5, n_iter=n_iter)\n",
                "traj_newton, losses_newton = newtons_method(X, y, w_init, n_iter=n_iter)\n",
                "\n",
                "#compute the margins and compare\n",
                "plt.plot(np.log(range(len(traj_gd))), [compute_margin(X, w, y) for w in traj_gd], label='gd margins')\n",
                "plt.plot(np.log(range(len(traj_newton))),[compute_margin(X, w, y) for w in traj_newton], label='newtons margins')\n",
                "plt.xlabel('log iterations')\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following cell plots how far each data point is from the decision boundary found by Newton's method that you ran in the previous cell.\n",
                "\n",
                "**3) Run the cell below after running Newton's method in the overparametrized regime (d > n). What do you see? How would you describe the solution that Newton's method finds in this regime?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "w = np.array(traj_newton[-1])\n",
                "plt.plot(X @ (w/ np.linalg.norm(w)) * y)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}