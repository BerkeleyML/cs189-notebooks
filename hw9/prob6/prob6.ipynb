{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LDA/QDA Classification of MNIST\n",
                "\n",
                "In this problem, we will apply LDA and QDA to classify a real dataset: MNIST, which is a set of 28x28 grayscale images of handwritten digits.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's first load the dataset and look at some images. Notice that we truncate the dataset so that our computation will run faster.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "! pip install mnist\n",
                "import mnist\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
                "from collections import defaultdict\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_data = mnist.train_images()[:10000]\n",
                "training_labels = mnist.train_labels()[:10000]\n",
                "test_data = mnist.test_images()[:1000]\n",
                "test_labels = mnist.test_labels()[:1000]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Look at some random images from our training set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "interact(lambda i: plt.imshow(training_data[i], cmap='gray'), i=IntSlider(min=0, max=100, continuous_update=False))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's first try to implement QDA. Recall that discriminative methods have two steps: first, we want to construct a model based on our training data - then, we want to use this model to make predictions.\n",
                "\n",
                "For simplicity, assume that all the digits are equally probable. Looking at our training data, this seems to be a valid assumption:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cnts = defaultdict(int)\n",
                "for label in training_labels:\n",
                "    cnts[label] += 1\n",
                "print(cnts)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For QDA, we assume our true distribution is of the form\n",
                "$$\n",
                "    \\text{Pr}(\\boldsymbol{X} | Y = k) = N(\\mu_k, \\Sigma_k)\n",
                "$$\n",
                "\n",
                "for each class $k$ - in our case, $k \\in \\{0, 1, \\ldots, 9\\}$. To find estimates $\\hat{\\mu}_k$ and $\\hat{\\Sigma}_k$, we use the MLE estimates.\n",
                "\n",
                "We have seen that they are what you would intuitively expect:\n",
                "$$\n",
                "    \\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i \\mid y_i = k} x_i,\n",
                "$$\n",
                "\n",
                "where $n_k$ is the number of points in class $k$, and\n",
                "$$\n",
                "    \\hat{\\Sigma}_k = \\frac{1}{n_k} \\sum_{i \\mid y_i = k} (x_i - \\hat{\\mu_k})(x_i - \\hat{\\mu}_k)^T.\n",
                "$$\n",
                "\n",
                "### Part (a)\n",
                "Fill in the below functions to compute these estimates.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_qda_params_single_class(data):\n",
                "    \"\"\"\n",
                "    Computes the mean and MLE covariance for the input data points from a single class.\n",
                "    training_data is an n * d matrix, where each row is a separate data point.\n",
                "    Returns a tuple (mu, sigma) with the desired output.\n",
                "    \"\"\"\n",
                "    n, d = data.shape\n",
                "    ### start qda_single_class ###\n",
                "\n",
                "    ### end qda_single_class ###\n",
                "    assert sigma.shape == (d, d), \"Sigma is not the right shape\"\n",
                "    return mu, sigma\n",
                "\n",
                "\n",
                "def compute_qda_params(data, labels):\n",
                "    \"\"\"\n",
                "    Computes the mean and MLE covariance for each class individually, given\n",
                "    labeled input data. Returns a list [(mu_0, sigma_0), (mu_1, sigma_1), ...]\n",
                "    with one entry for each class\n",
                "    \"\"\"\n",
                "    num_classes = 10\n",
                "    assert len(set(labels)) == num_classes\n",
                "\n",
                "    # \"flatten\" each sample point from a 2D array into a single row\n",
                "    data = data.reshape((data.shape[0], -1))\n",
                "    n, d = data.shape\n",
                "    params = []\n",
                "    ### start qda_multi_class ###\n",
                "\n",
                "    ### end qda_multi_class ###\n",
                "    assert len(params) == num_classes\n",
                "    return params\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "qda_params = compute_qda_params(training_data, training_labels)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's try to visualize these parameters. It is reasonable to expect that plotting the means of each class will produce a sort of \"representative\" image for each digit.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for mu, sigma in qda_params:\n",
                "    plt.imshow(mu.reshape(28, 28), cmap='gray')\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You should obtain images that look like digits. If not, you've probably made a mistake. Next, we'll try to classify some of our test data and see how well our approach works.\n",
                "\n",
                "### Part (b)\n",
                "Implement the below function, that takes in the `params` you computed in the previous part, as well as a test point, and tries to determine its label. Recall that\n",
                "$$\n",
                "    \\hat{y} = \\arg\\max_k \\left(- \\frac{1}{2}(x - \\hat{\\mu}_k)^T\\hat{\\Sigma}_k^{-1}(x - \\hat{\\mu}_k) - \\frac{1}{2} \\ln\\left(|{\\hat{\\Sigma}_k}|\\right)\\right)\n",
                "$$\n",
                "\n",
                "Your covariance matrices may be singular. This will be a problem, since you need to invert them and compute the log of their determinant, both of which are undefined operations on singular matrices. There are a number of ways to get around this - here, we will simply implement a \"hack\", by \"fuzzing\" the matrix in a manner that preserves its singular values, but lifts its zero singular values up to a small nonzero value. Specifically, let $\\text{fuzz}(\\Sigma) = \\Sigma + \\varepsilon I$, where $\\varepsilon$ is some small hyperparameter.\n",
                "\n",
                "We have implemented this `fuzz` function for you with $\\varepsilon = 10^{-6}$, as well as a helper function `compute_accuracy` to help you evaluate your results.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_accuracy(predicted_labels, test_labels):\n",
                "    predicted_labels = predicted_labels.reshape(-1)\n",
                "    test_labels = test_labels.reshape(-1)\n",
                "    n = len(predicted_labels)\n",
                "    assert n == len(test_labels)\n",
                "    return np.sum(predicted_labels == test_labels) / n\n",
                "\n",
                "eps = 1e-6\n",
                "\n",
                "def fuzz(matrix):\n",
                "    try:\n",
                "        np.linalg.inv(matrix)\n",
                "    except np.linalg.LinAlgError:\n",
                "        return matrix + np.eye(len(matrix)) * eps\n",
                "    else:\n",
                "        return matrix\n",
                "\n",
                "def classify(params, test_points):\n",
                "    \"\"\"\n",
                "    `params` is as generated by `compute_qda_params`\n",
                "    `test_points` is an array of test points, with one row for each point.\n",
                "\n",
                "    You should try to vectorize as much of your solution as possible, so it does\n",
                "    not take too long to run.\n",
                "    \"\"\"\n",
                "    # reshape test_points so each test point is in a single row\n",
                "    test_points = test_points.reshape(test_points.shape[0], -1)\n",
                "    n, d = test_points.shape\n",
                "    ### start classify ###\n",
                "\n",
                "    ### end classify ###\n",
                "    labels = labels.reshape(-1)\n",
                "    assert len(labels) == n, \"{} != {}\".format(len(labels), n)\n",
                "    return labels\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how well your classifier does on MNIST!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Train Accuracy:\", compute_accuracy(classify(qda_params, training_data), training_labels))\n",
                "print(\"Test Accuracy:\", compute_accuracy(classify(qda_params, test_data), test_labels))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (c)\n",
                "Another interesting thing to plot is the train and test accuracy as a function of the number of data points. This will tell us whether we are overfitting to the training set. Fill in the code blocks to generate the plot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def accuracy_vs_n(compute_params):\n",
                "    candidate_ns = range(100, 10000, 1000)\n",
                "    training_accuracies = []\n",
                "    test_accuracies = []\n",
                "    for n in candidate_ns:\n",
                "        n = int(n)\n",
                "        print(\"Evaluating with {} samples\".format(n))\n",
                "        ### start compute_qda_accuracies ###\n",
                "\n",
                "        ### end compute_qda_accuracies ###\n",
                "    plt.plot(candidate_ns, training_accuracies, label=\"Training\")\n",
                "    plt.plot(candidate_ns, test_accuracies, label=\"Test\")\n",
                "    plt.ylabel(\"Accuracy\")\n",
                "    plt.xlabel(\"Number of training points\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "accuracy_vs_n(compute_qda_params)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comment on these results. You should notice that the test accuracy fluctuates quite dramatically. One reason for this is because our estimates of the covariances for each label are very noisy, since we are only looking at one tenth of the data when calculating each covariance matrix.\n",
                "\n",
                "### start qda-comment ###\n",
                "\n",
                "### end qda-comment ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (d)\n",
                "LDA solves this problem, by making the assumption that the covariance matrices for each class are the same $\\Sigma = \\Sigma_k$ for all $k$. We compute our estimate $\\hat{\\Sigma}$ by simply averaging the covariance matrices for each class. In principle, this estimate should be less noisy since it is constructed using much more data, so our test accuracy should not vary by as much.\n",
                "\n",
                "Let's find out if this is the case. Implement the below function to compute the `params` to be passed into `classify` for LDA. You should reuse the `compute_qda_params` function that you previously implemented.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_lda_params(data, labels):\n",
                "    \"\"\"\n",
                "    Computes the mean for each class individually and the MLE covariance across all classes,\n",
                "    given labeled input data. Returns a list [(mu_0, sigma), (mu_1, sigma), ...]\n",
                "    with one entry for each class\n",
                "    \"\"\"\n",
                "    n, d = data.shape[0], data.shape[1] * data.shape[2]\n",
                "    ### start lda_multi_class ###\n",
                "\n",
                "    ### end lda_multi_class ###\n",
                "    assert all((sigma == params[0][1]).all() for mu, sigma in params), \"Covariances are not all the same!\"\n",
                "    return params\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now evaluate the LDA classifier using the `classify` function from earlier.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lda_params = compute_lda_params(training_data, training_labels)\n",
                "print(\"Train Accuracy:\", compute_accuracy(classify(lda_params, training_data), training_labels))\n",
                "print(\"Test Accuracy:\", compute_accuracy(classify(lda_params, test_data), test_labels))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And finally, we can run the LDA classifier with a variable number of input samples:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "accuracy_vs_n(compute_lda_params)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comment on your observations in both the LDA and QDA cases. Did you achieve the expected reduction in variation with LDA? What about in absolute terms - is it a better method than QDA? Why, or why not?\n",
                "\n",
                "### start lda-comment ###\n",
                "\n",
                "### end lda-comment ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (e)\n",
                "\n",
                "One thing we can investigate further is the per-digit accuracy of our two classifiers. It is reasonable to suppose that they might be much better at classifying some digits compared to others. Run the below function to plot their per-digit training and test accuracies. It will create two plots: one with the training accuracy for each digit as a function of the number of samples, and another with the test accuracy.\n",
                "\n",
                "This may take a while to run!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def digit_accuracy_vs_n(compute_params):\n",
                "    candidate_ns = range(100, 10000, 1000)\n",
                "    training_accuracies_by_digit = [[] for _ in range(10)]\n",
                "    test_accuracies_by_digit = [[] for _ in range(10)]\n",
                "    for n in candidate_ns:\n",
                "        n = int(n)\n",
                "        print(\"Evaluating with {} samples\".format(n))\n",
                "        params = compute_params(training_data[:n], training_labels[:n])\n",
                "        predicted_training_labels = classify(params, training_data[:n])\n",
                "        predicted_test_labels = classify(params, test_data)\n",
                "        for k in range(10):\n",
                "            training_indices_with_k = training_labels[:n] == k\n",
                "            test_indices_with_k = test_labels == k\n",
                "            training_accuracies_by_digit[k].append(compute_accuracy(\n",
                "                predicted_training_labels[training_indices_with_k], training_labels[:n][training_indices_with_k]\n",
                "            ))\n",
                "            test_accuracies_by_digit[k].append(compute_accuracy(\n",
                "                predicted_test_labels[test_indices_with_k], test_labels[test_indices_with_k]\n",
                "            ))\n",
                "\n",
                "    for k, training_accuracies in enumerate(training_accuracies_by_digit):\n",
                "        plt.plot(candidate_ns, training_accuracies, label=\"Digit {}\".format(k))\n",
                "    plt.xlabel(\"Number of training points\")\n",
                "    plt.ylabel(\"Training accuracy\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "    for k, test_accuracies in enumerate(test_accuracies_by_digit):\n",
                "        plt.plot(candidate_ns, test_accuracies, label=\"Digit {}\".format(k))\n",
                "    plt.xlabel(\"Number of training points\")\n",
                "    plt.ylabel(\"Test accuracy\")\n",
                "    plt.legend()\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For QDA\n",
                "digit_accuracy_vs_n(compute_qda_params)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For LDA\n",
                "digit_accuracy_vs_n(compute_lda_params)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comment on your results. Which digits were easy to classify? Which were harder? Why might this be the case? Did it differ between LDA and QDA?\n",
                "\n",
                "### start digit-comment ###\n",
                "\n",
                "### end digit-comment ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (f)\n",
                "\n",
                "Finally, we will return to our discussion of singular covariance matrices. Recall that the `fuzz` function perturbed our covariance matrix to lift the zero eigenvalues up to some small `eps`, a hyperparameter that we arbitrarily set to $10^{-6}$. Now, you should experiment with different choices for `eps`, to attempt to choose the optimal hyperparameter.\n",
                "\n",
                "For simplicity, we will only consider tuning `eps` to optimize the performance of LDA, since the accuracy of QDA with this few data points is too noisy to obtain reliable results.\n",
                "\n",
                "Implement the `tune_eps` and `compute_validation_score` functions. Consider what would be an appropriate range of hyperparameters, as well as how the test values will be spaced over that interval.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tune_eps(validation_score):\n",
                "    \"\"\"\n",
                "    validation_score() is a zero-argument function that returns\n",
                "    the validation accuracy of some model. `eps` is passed into it\n",
                "    by modifying it as a global variable.\n",
                "\n",
                "    You should try at least 10 different values of `eps`.\n",
                "    \"\"\"\n",
                "    global eps\n",
                "\n",
                "    best_eps = None\n",
                "    best_validation = 0\n",
                "\n",
                "    ### start tune_eps ###\n",
                "\n",
                "    ### end tune_eps ###\n",
                "\n",
                "    eps = best_eps\n",
                "    return best_eps\n",
                "\n",
                "def compute_validation_score(compute_params):\n",
                "    def validation_score():\n",
                "        # update these lines\n",
                "        hyper_tuning_training_data = ...\n",
                "        hyper_tuning_training_labels = ...\n",
                "        hyper_tuning_validation_data = ...\n",
                "        hyper_tuning_validation_labels = ...\n",
                "\n",
                "        ### start split_validation ###\n",
                "\n",
                "        ### end split_validation ###\n",
                "\n",
                "        return compute_accuracy(classify(\n",
                "            compute_params(hyper_tuning_training_data, hyper_tuning_training_labels),\n",
                "            hyper_tuning_validation_data,\n",
                "        ), hyper_tuning_validation_labels)\n",
                "\n",
                "    return validation_score\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Best epsilon for LDA:\", tune_eps(compute_validation_score(compute_lda_params)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Best epsilon for QDA:\", tune_eps(compute_validation_score(compute_qda_params)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Did your hyperparameter tuning work? Why or why not? Comment on your observations.\n",
                "\n",
                "### start eps-comment ###\n",
                "\n",
                "### end eps-comment ###\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}