{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# [HW 9] Loss Shaping, Noise, and Outliers\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Setup random seed\n",
                "np.random.seed(20)\n",
                "\n",
                "# Make fonts bold\n",
                "plt.rcParams['font.weight'] = 'bold'\n",
                "plt.rcParams['axes.labelweight'] = 'bold'\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Classification\n",
                "\n",
                "Here we define a classification problem where one of the classes is significantly underrepresented in the training data, but present equally in the test data. You will explore the performance of two objective functions in this situation and try a method for overcoming this class imbalance without changing the objective function.\n",
                "\n",
                "Below we create two classes drawn from separate 2D Gaussian distributions and with labels $y_i \\in \\{-1, 1\\}$. In the training data class 2 only has 50 samples out of 2050 total, but in the test data both classes have 5000 samples.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_dataset(X, y, title=\"\"):\n",
                "    props = {'fontweight': 'bold'}\n",
                "    if title:\n",
                "        plt.title(title, **props)\n",
                "    plt.scatter(X[y < 0.0, 0], X[y < 0.0, 1], s=10)\n",
                "    plt.scatter(X[y > 0.0, 0], X[y > 0.0, 1], s=10)\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Generate Train Data\n",
                "N_1 = 2000\n",
                "mean_1 = [3.0, 2.0]\n",
                "cov_1 = [[1, 0.0], [0.0, 1]]\n",
                "X_1 = np.random.multivariate_normal(mean_1, cov_1, N_1)\n",
                "y_1 = np.ones(N_1)\n",
                "\n",
                "N_2 = 50\n",
                "mean_2 = [0.0, 3.0]\n",
                "cov_2 = [[0.2, 0.0], [0.0, 0.2]]\n",
                "X_2 = np.random.multivariate_normal(mean_2, cov_2, N_2)\n",
                "y_2 = -1.0 * np.ones(N_2)\n",
                "\n",
                "X_train = np.concatenate((X_1, X_2), axis=0)\n",
                "Y_train = np.concatenate((y_1, y_2), axis=0)\n",
                "# Augment X_train with an extra feature (all equals to 1)\n",
                "X_train_aug = np.ones((X_train.shape[0], X_train.shape[1]+1))\n",
                "X_train_aug[:,:-1] = X_train\n",
                "\n",
                "\n",
                "### Generate Train/Test Data\n",
                "N_test_1 = 5000\n",
                "X_test_1 = np.random.multivariate_normal(mean_1, cov_1, N_test_1)\n",
                "y_test_1 = np.ones(N_test_1)\n",
                "\n",
                "N_test_2 = 5000\n",
                "X_test_2 = np.random.multivariate_normal(mean_2, cov_2, N_test_2)\n",
                "y_test_2 = -1.0 * np.ones(N_test_2)\n",
                "\n",
                "X_test = np.concatenate((X_test_1, X_test_2), axis=0)\n",
                "Y_test = np.concatenate((y_test_1, y_test_2), axis=0)\n",
                "# Augment X_test with an extra feature (all equals to 1)\n",
                "X_test_aug = np.ones((X_test.shape[0], X_test.shape[1]+1))\n",
                "X_test_aug[:,:-1] = X_test\n",
                "\n",
                "N_test = N_test_1 + N_test_2\n",
                "\n",
                "# Visualize the train dataset\n",
                "visualize_dataset(X_train, Y_train, \"Train Data\")\n",
                "\n",
                "# Visualize the test dataset\n",
                "visualize_dataset(X_test, Y_test, \"Test Data\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the decision boundary\n",
                "XX, YY = np.meshgrid(np.linspace(-8.0, 8.0, 500), np.linspace(-8.0, 8.0, 500))\n",
                "X_vis = np.stack([np.ravel(XX), np.ravel(YY)])\n",
                "X_vis_aug = np.ones((X_vis.shape[0]+1, X_vis.shape[1]))\n",
                "X_vis_aug[:-1,:] = X_vis\n",
                "X_vis_aug = X_vis_aug.T\n",
                "\n",
                "def visualize_classifier(ys):\n",
                "    \"\"\"Visualize the decision boundary for a classifier.\n",
                "\n",
                "    Input: a vector of classification decisions using X0_aug as the samples\n",
                "    \"\"\"\n",
                "    props = {'fontweight': 'bold'}\n",
                "    ZZ = np.resize(ys, (500, 500))\n",
                "    # Train\n",
                "    fig, ax = plt.subplots(figsize=(8,8))\n",
                "    ax.set_title(\"Train Data with Boundary\", **props)\n",
                "    ax.contourf(XX,YY,ZZ, cmap=\"coolwarm\", levels=np.linspace(-1000,1000,3))\n",
                "    ax.scatter(X_train[:,0], X_train[:,1], c=Y_train, cmap=\"coolwarm\", s=10)\n",
                "    ax.axis(\"equal\")\n",
                "\n",
                "    # Test\n",
                "    fig, ax = plt.subplots(figsize=(8,8))\n",
                "    ax.set_title(\"Test Data with Boundary\", **props)\n",
                "    ax.contourf(XX,YY,ZZ, cmap=\"coolwarm\", levels=np.linspace(-1000,1000,3))\n",
                "    ax.scatter(X_test[:,0], X_test[:,1], c=Y_test, cmap=\"coolwarm\", s=10)\n",
                "    ax.axis(\"equal\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mean Squared Error Loss\n",
                "\n",
                "We will tackle this problem first with linear least-squares regression, which you have seen many times before.\n",
                "## (a)\n",
                "**Using `X_train_aug`, `Y_train`, `X_test_aug`, and `Y_test` perform least-squares regression and report the prediction accuracy on the test set.** `X_*_aug` variables are $\\mathbf{X}$ samples augmented with the ones feature to allow the decision hyperplane to be offset from the origin.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def classifier_linear(Xtest, w1):\n",
                "    \"\"\"Classify the samples Xtest using weights w1.\"\"\"\n",
                "    return -1.0*(Xtest.dot(w1) < 0.0) + 1.0*(Xtest.dot(w1) > 0.0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Perform linear regression on the training data X_train_aug. Report the prediction accuracy with X_test_aug.\n",
                "### start mse_train ###\n",
                "\n",
                "### end mse_train ###\n",
                "\n",
                "# Report the classification accuracy on the test set\n",
                "ypred = classifier_linear(X_test_aug, w1)\n",
                "print(\"prediction accuracy on the test set is \", 1.0*sum(ypred == Y_test) / N_test)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Map the decision boundary with classifier_linear() and X_vis_aug,\n",
                "# then plot the boundary with visualize_classifier()\n",
                "y0 = classifier_linear(X_vis_aug, w1)\n",
                "visualize_classifier(y0)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (b)\n",
                "**Train on a dataset with copies of the underrepresented class, report prediction error, and visualize the decision boundary.** _Don't forget to show the equivalence in the PDF._\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Use copies of the underrepresented class to improve performance,\n",
                "#       report accuracy, and visualize the decision boundary.\n",
                "### start mse_copies ###\n",
                "\n",
                "### end mse_copies ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (c) Logistic Regression\n",
                "Now we will try solving the unaugmented problem using logistic regression, where we model the _probability_ that a data point corresponds to class 1:\n",
                "$$h_\\theta(\\mathbf{x}_i) = P(y_i=1 | \\mathbf{x}_i, \\mathbf{\\theta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^T \\mathbf{\\theta}}}.$$\n",
                "This technique is called logistic _regression_ because we are learning a regression model with the probability of membership in a class as the target, rather than a simple yes/no. When we need a hard decision we simply choose the class with the highest probability.\n",
                "\n",
                "$\\theta$ is typically found using maximum likelihood estimation with the log-likelihood\n",
                "$$\\max_\\theta \\log L(\\theta | \\mathbf{X}, \\mathbf{y}) = \\max_\\theta \\log \\left( \\prod_{i=1}^N P(y_i | \\mathbf{x}_i, \\mathbf{\\theta}) \\right)$$\n",
                "$$ = \\max_\\theta \\log \\left( \\prod_{i=1}^N h_\\theta(\\mathbf{x}_i)^{y_i} (1 - h_\\theta(\\mathbf{x}_i))^{(1-y_i)} \\right)$$\n",
                "$$ = \\max_\\theta \\sum_{i=1}^N \\left(y_i \\log h_\\theta(\\mathbf{x}_i) + (1-y_i) \\log (1 - h_\\theta(\\mathbf{x}_i)) \\right).$$\n",
                "Here we use $\\mathbf{y} \\in \\{0,1\\}^N$ rather than $\\{-1,1\\}^N$ as with the MSE loss. This loss function may be familiar to some of you as the cross-entropy loss. Here we are actually maximizing the log-likelihood, but we could equivalently minimize the negative log-likelihood.\n",
                "\n",
                "Unfortunately, there is no general closed-form solution to this optimization problem so we will use the solver included with `sklearn` to find our weights $\\mathbf{\\theta}$. The general usage pattern for `sklearn` models is to create the object, fit the training set, then predict.\n",
                "\n",
                "```python\n",
                "my_classifier = LogisticRegression(penalty='none')\n",
                "my_classifier.fit(training_data, training_labels)\n",
                "predictions = my_classifier.predict(test_data)\n",
                "```\n",
                "\n",
                "**Using the `LogisticRegression` class, train on data both with and without copies and report your test errors.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "# TODO: Train a logistic classifier on the original training data.\n",
                "#       Report test accuracy and visualize the decision boundary.\n",
                "### start logistic ###\n",
                "\n",
                "### end logistic ###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Train a logistic classifier on the copy-augmented training data.\n",
                "#       Report test accuracy and visualize the decision boundary.\n",
                "### start logistic_copies ###\n",
                "\n",
                "### end logistic_copies ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Discuss the sensitivity to class imbalance of the two loss functions in your writeup.**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Regression\n",
                "\n",
                "There are many loss functions that can be used when performing regression. Each has properties which make it suited to particular situations. There is no 'one size fits all' loss function. We will experiment with different data sets and loss functions to see how they perform below.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (d) Linear Model with Outliers\n",
                "We will learn a 1D linear function from data corrupted by additive Gaussian noise and multiplicative outliers.\n",
                "$$ f(x) = ax + b$$\n",
                "$$ y = \\delta(f(x) + \\epsilon),\\, \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
                "$$ \\delta = \\begin{cases} 1&\\text{w.p. } p \\\\ -5&\\text{w.p. } 1-p \\end{cases}$$\n",
                "\n",
                "**Train linear models on the data set created below with the $L1$, $L2$, and Huber loss functions.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_regression(X, y, w_true, model, X_test, loss='Null Loss'):\n",
                "    if model is None:\n",
                "        y_hat = np.zeros(X_test.shape[0])\n",
                "    else:\n",
                "        y_hat = model.predict(X_test)\n",
                "    plt.figure(figsize=[10, 6])\n",
                "    plt.plot(X_test[:, 1], X_test @ w_true, label='True Function')\n",
                "    plt.scatter(X[:, 1], y, color='darkorange', label='Train Data')\n",
                "    plt.plot(X_test[:, 1], y_hat, color='darkgreen', linewidth=2, label='Predicted Function')\n",
                "    plt.xlabel(\"$x$\")\n",
                "    plt.ylabel(\"$f(x)$\")\n",
                "    plt.legend()\n",
                "    true_mse = np.mean((X_test @ w_true - y_hat) ** 2)\n",
                "    plt.title(\"%s, True MSE %.2g\" % (loss, true_mse), fontweight='bold')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N = 200\n",
                "X = np.ones((N, 2))\n",
                "X[:, 1] = np.sort(np.random.uniform(-1, 1, N))\n",
                "X_test = np.ones((N * 10, 2))\n",
                "X_test[:, 1] = np.sort(np.random.uniform(-1, 1, N * 10))\n",
                "w_true = np.array([0.8, 2])\n",
                "y = X @ w_true + np.random.randn(N) * 0.5\n",
                "\n",
                "frac_outlier = 0.1\n",
                "y[np.random.choice(N, int(frac_outlier * N))] *= -5\n",
                "\n",
                "plot_regression(X, y, w_true, None, X_test)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### L2 Loss\n",
                "**Implement standard Ridge Regression to learn $\\hat{f}(x)$ and plot the result with `plot_regression`.**\n",
                "\n",
                "Any time you use an `sklearn` regressor with the ones-augmented $\\mathbf{X}$ data, you need to set `fit_intercept=False`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import Ridge\n",
                "\n",
                "# TODO: Train a linear model with ridge regression and plot the result.\n",
                "#       Don't forget to set fit_intercept=False\n",
                "rr = Ridge(alpha=0.0001, fit_intercept=False)\n",
                "### begin outliers_l2 ###\n",
                "rr.fit(X, y)\n",
                "rr_coef = rr.coef_\n",
                "print(rr_coef)\n",
                "plot_regression(X, y, w_true, rr, X_test, 'L2 Loss')\n",
                "### end outliers_l2 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### L1 Loss\n",
                "In order to solve the optimization problem with L1 loss we must use an iterative solver. `sklearn` provides a class which uses SGD to solve regression problems with several possible losses in `SGDRegressor`. **Use `SGDRegressor` with the `epsilon_insensitive` loss and `epsilon=0` to learn coefficients with the L1 loss** $L(\\mathbf{X},\\mathbf{y},\\mathbf{w}) = |\\mathbf{X}\\mathbf{w} - \\mathbf{y}|_1$. **Plot the result with `plot_regression`.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import SGDRegressor\n",
                "\n",
                "# TODO: Train a linear model with an L1 loss and plot the result.\n",
                "#       Don't forget to set fit_intercept=False.\n",
                "### begin outliers_l1 ###\n",
                "sgdr = SGDRegressor(loss='epsilon_insensitive', penalty='l2', alpha=0.0001, epsilon=0.0, fit_intercept=False)\n",
                "sgdr.fit(X, y)\n",
                "sgd_coef = sgdr.coef_\n",
                "print(sgd_coef)\n",
                "plot_regression(X, y, w_true, sgdr, X_test, 'L1 Loss')\n",
                "### end outliers_l1 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Huber Loss\n",
                "The Huber loss function is a hybrid of L1 and L2 losses.\n",
                "$$L_{Huber}(f(x), y, \\delta) = \\begin{cases} \\frac{1}{2}|f(x) - y|^2, & |f(x) - y| \\leq \\delta \\\\ \\delta|f(x) - y| - \\frac{1}{2} \\delta^2, & \\text{otherwise} \\end{cases} $$\n",
                "The L2 loss is an ubiased estimator of the mean, and the L1 loss is an ubiased estimator of the median, so the Huber loss attempts to maintain a good estimate of the mean while reducing sensitivity to outliers. $\\delta$ is a hyperparameter which represents how large the residual must be before a sample is considered an outlier. The Huber loss can also be modified for use in classification, but we won't ask you to use it here.\n",
                "\n",
                "`sklearn` includes a regression class specifically for the Huber loss in `HuberRegressor`. `sklearn`'s implementation uses a form that is invariant with scalings of $\\mathbf{X}$ and $\\mathbf{y}$, so you will have to tune the hyperparameter `epsilon` in your code. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html) for more details.\n",
                "\n",
                "**Use `HuberRegressor` to train a linear model with the Huber loss and plot the result with `plot_regression`.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import HuberRegressor\n",
                "\n",
                "# TODO: Train a linear model with an Huber loss and plot the result.\n",
                "#       Don't forget to set fit_intercept=False.\n",
                "### begin outliers_huber ###\n",
                "hr = HuberRegressor(epsilon=1.2, alpha=0.0001, fit_intercept=False)\n",
                "hr.fit(X, y)\n",
                "h_coef = hr.coef_\n",
                "print(h_coef)\n",
                "plot_regression(X, y, w_true, hr, X_test, 'Huber Loss')\n",
                "### end outliers_huber ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exponential Model with Multiplicative Input Noise\n",
                "Here we have an exponential model, but instead of additive output observation noise like you are used to the noise is multiplicative and applied to the input. In other words, instead of corrupted $y$ values we observe corrupted $x$ values. The $y$s we observe actually came from different $x$ values than we see in the training data.\n",
                "$$y = e^{zx},\\, z\\sim {U}[1-\\epsilon, 1+\\epsilon]$$\n",
                "We will attempt to learn this function with a polynomial model of degree $p$.\n",
                "$$\\hat{f}(x) = w_0 + \\sum_{i=1}^p w_i x^i$$\n",
                "Using the Taylor expansion of $e^x$ and $p=10$ we could achieve a test MSE of $\\approx 12$ (this value may fluctuate slightly depending on the exact test points chosen).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N = 1000\n",
                "p = 10\n",
                "x_max = 6\n",
                "w_true = np.ones(p + 1)\n",
                "X = np.ones((N, p + 1))\n",
                "X[:, 1] = np.sort(np.random.uniform(0, x_max, N))\n",
                "X_test = np.ones((N * 10, p + 1))\n",
                "X_test[:, 1] = np.sort(np.random.uniform(0, x_max, N * 10))\n",
                "for i in range(2, p + 1):\n",
                "    X[:, i] = X[:, 1] ** i\n",
                "    X_test[:, i] = X_test[:, 1] ** i\n",
                "    w_true[i] /= np.math.factorial(i)\n",
                "epsilon = 0.1\n",
                "y = np.exp(X[:, 1] * np.random.uniform(1 - epsilon, 1 + epsilon, N))\n",
                "\n",
                "plot_regression(X, y, w_true, None, X_test)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best possible approximation\n",
                "print(\"MSE for Taylor expansion:\", np.mean((np.exp(X_test[:,1]) - X_test @ w_true) **2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (e)\n",
                "**Train models to fit this data using $L1$, $L2$, and Huber losses. Plot the results and report the test errors.**\n",
                "\n",
                "You may have to adjust your hyperparameters significantly relative to the model with outliers, and not every loss function may produce a reasonable model regardless of hyperparameters.\n",
                "\n",
                "### L2 Loss\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### begin multiplicative_l2 ###\n",
                "rr = Ridge(alpha=.01, fit_intercept=False)\n",
                "rr.fit(X, y)\n",
                "rr_coef = rr.coef_\n",
                "print(rr_coef)\n",
                "plot_regression(X, y, w_true, rr, X_test, 'L2 Loss')\n",
                "### end multiplicative_l2 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### L1 Loss\n",
                "\n",
                "It turns out that, even though this is a convex problem, `sklearn`'s optimizer does a very poor job on this problem with an $L1$ loss. This is another example of why you should not blindly trust library implementations because they may behave in unexpected ways or fail in unexpected situations. We have given you the code we used to achieve a 'reasonable' result below. Using parameters similar to the outliers dataset gave nonsensical results with predictions on the order of $10^{11}$.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sgdr = SGDRegressor(loss='epsilon_insensitive', penalty='l2', alpha=1, epsilon=0.0, fit_intercept=False,\n",
                "                    eta0=7e-12)\n",
                "sgdr.fit(X, y)\n",
                "sgd_coef = sgdr.coef_\n",
                "print(sgd_coef)\n",
                "plot_regression(X, y, w_true, sgdr, X_test, 'L1 Loss')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Huber Loss\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### begin multiplicative_huber ###\n",
                "hr = HuberRegressor(epsilon=5, alpha=0.01, fit_intercept=False)\n",
                "hr.fit(X, y)\n",
                "h_coef = hr.coef_\n",
                "print(h_coef)\n",
                "plot_regression(X, y, w_true, hr, X_test, 'Huber Loss')\n",
                "### end multiplicative_huber ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (f) Log Relative Loss\n",
                "Since the function we are trying to learn is exponential, a natural modification to the regression problem is to use $\\log y$ as the regression target instead of $y$. The plot below shows what the target data looks like when log-transformed. This seems like a much easier regression problem to solve than the original one!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot of the log-transformed regression problem\n",
                "plot_regression(X, np.log(y), np.array([0,1]+[0]*9), None, X_test)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Using `sklearn`'s `TransformedTargetRegressor` meta-class, train on log-transformed $y$ values, plot the result, and report the test error.**\n",
                "\n",
                "`TransformedTargetRegressor` takes three inputs:\n",
                "- `regressor`: a regressor object, e.g. `Ridge(alpha=1, fit_intercept=False)`\n",
                "- `func`: the transformation to apply, e.g. `lambda x: np.power(x, 2)`\n",
                "- `inverse_func`: the inverse transformation to apply to predictions, e.g. `np.sqrt`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.compose import TransformedTargetRegressor\n",
                "\n",
                "# TODO: Use the TransformedTargetRegressor to train a Ridge model on log-\n",
                "#       transformed targets.\n",
                "#       Don't forget to set fit_intercept=False.\n",
                "### begin log_loss ###\n",
                "lrr = TransformedTargetRegressor(regressor=Ridge(alpha=1.0, fit_intercept=False),\n",
                "                                 func=np.log,\n",
                "                                 inverse_func=np.exp)\n",
                "lrr.fit(X, y)\n",
                "plot_regression(X, y, w_true, lrr, X_test, 'Log Relative Loss')\n",
                "### end log_loss ###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}