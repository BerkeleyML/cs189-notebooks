{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# [HW13] Problem: Gradient boosting and early stopping\n",
                "\n",
                "Import necessary Python packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.io import loadmat\n",
                "\n",
                "from sklearn.datasets import make_sparse_coded_signal\n",
                "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
                "from sklearn.metrics import zero_one_loss\n",
                "from sklearn.tree import DecisionTreeClassifier\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# globals\n",
                "n_estimators = 200\n",
                "DT1 = DecisionTreeClassifier(max_depth=1, min_samples_leaf=15)\n",
                "DT2 = DecisionTreeClassifier(max_depth=2, min_samples_leaf=15)\n",
                "DT4 = DecisionTreeClassifier(max_depth=4, min_samples_leaf=15)\n",
                "DT9 = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\n",
                "\n",
                "\"\"\"Loads the training data from the SPAM dataset used in HW12.\"\"\"\n",
                "def load_data():\n",
                "    # load data\n",
                "    data = loadmat(\"datasets/spam_data/spam_data.mat\")\n",
                "    # training data\n",
                "    data_, labels_ = data[\"training_data\"], np.squeeze(data[\"training_labels\"])\n",
                "    X_train, y_train = data_, labels_\n",
                "    # test data\n",
                "    y_test=[]\n",
                "    with open(\"datasets/spam_data/spam_test_labels.txt\",\"r\") as f:\n",
                "        for l in f.readlines():\n",
                "            y_test.append(int(l.split(\",\")[1]))\n",
                "    y_test = np.array(y_test)\n",
                "    X_test = data['test_data']\n",
                "\n",
                "    return X_train, y_train, X_test, y_test\n",
                "\n",
                "\"\"\"Runs the maching pursuit algorithm.\"\"\"\n",
                "def mp(y, X, w_true, y_test, X_test):\n",
                "    train_err = []\n",
                "    test_err = []\n",
                "    X_ = X\n",
                "    y = np.copy(y); X = np.copy(X)\n",
                "    curr = np.copy(y)\n",
                "    w_est = np.zeros(len(X[0]))\n",
                "    for j in range(len(X[0])):\n",
                "        i = np.argmax(np.abs(np.dot(X.T, curr)))\n",
                "        col = np.copy(X[:,i])\n",
                "        # use each column only once\n",
                "        X[:,i] = 0\n",
                "        w_est[i] = np.dot(col, curr)\n",
                "        curr = curr - col*w_est[i]\n",
                "        # error defined here as ||y - D x_hat||_2\n",
                "        train_err.append(np.linalg.norm(X_.dot(w_est) - y))\n",
                "        test_err.append(np.linalg.norm(X_test.dot(w_est)-y_test))\n",
                "\n",
                "    return w_est, train_err, test_err\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, y_train, X_test, y_test = load_data()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PART I\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(8,7))\n",
                "ax = fig.add_subplot(111)\n",
                "styles=[\"k-\", \"k--\", \"k-.\"]\n",
                "depths=[1,2,4]\n",
                "j=0\n",
                "# for each weak classifier, train it and an AdaBoost instance based on it\n",
                "for w in [DT1, DT2,DT4]:\n",
                "    # Weak classifier\n",
                "    w.fit(X_train, y_train)\n",
                "    err = 1.0 - w.score(X_train, y_train)\n",
                "    ax.plot([1, n_estimators], [err] * 2, styles[j],\n",
                "        label=\"Decision Tree, max depth %d (DT%d)\" % (depths[j],depths[j]))\n",
                "    # AdaBoost classifier\n",
                "    ada = AdaBoostClassifier(base_estimator=w,\n",
                "                             n_estimators=n_estimators,\n",
                "                             random_state=0)\n",
                "\n",
                "    ada_train_err = np.zeros((n_estimators,))\n",
                "    ada.fit(X_train, y_train)\n",
                "    for i, y_pred in enumerate(ada.staged_predict(X_train)):\n",
                "        ada_train_err[i] = zero_one_loss(y_pred, y_train)\n",
                "\n",
                "    smoothed = []\n",
                "    # use moving average filter to smooth plots -- done to make easier\n",
                "    # to see trends; you are encouraged to also plot 'ada_train_err' to\n",
                "    # see the actual error plots!!\n",
                "    for i in range(len(ada_train_err)):\n",
                "        temp = 0.\n",
                "        counter = 0.\n",
                "        for k in range(i-5, i+1):\n",
                "            if k >= 0:\n",
                "                temp += ada_train_err[k]\n",
                "                counter += 1.\n",
                "        smoothed.append(temp/counter)\n",
                "\n",
                "    ax.plot(np.arange(n_estimators) + 1, smoothed, styles[j],\n",
                "        label=\"AdaBoost on DT%d\" % depths[j],\n",
                "        color=\"red\")\n",
                "\n",
                "    j += 1\n",
                "\n",
                "ax.set_ylim((0.1, 0.3))\n",
                "ax.set_yscale('log')\n",
                "ax.set_xlabel(\"Number of Classifiers (for AdaBoost)\")\n",
                "ax.set_ylabel(\"Error [%], log scale\")\n",
                "ax.set_title(\"Weak Classifiers and AdaBoost vs. Training Error\")\n",
                "leg = ax.legend(loc='upper right', fancybox=True)\n",
                "leg.get_frame().set_alpha(0.7)\n",
                "plt.show()\n",
                "plt.close()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PART J\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(8,7))\n",
                "ax = fig.add_subplot(111)\n",
                "# Basline classifier (a \"deep\" tree)\n",
                "DT9.fit(X_train, y_train)\n",
                "err = 1.0 - DT9.score(X_test, y_test)\n",
                "ax.plot([1, n_estimators], [err] * 2, \"k-\",\n",
                "    label=\"Baseline Classifier -- Decision Tree, max depth 9\")\n",
                "# AdaBoost\n",
                "styles=[\"k-\", \"k--\", \"k-.\"]\n",
                "depths=[1,2,4]\n",
                "j=0\n",
                "# for each weak classifier, train an AdaBoost instance based on it\n",
                "for w in [DT1, DT2, DT4]:\n",
                "\n",
                "    # AdaBoost classifier\n",
                "    ada = AdaBoostClassifier(base_estimator=w,\n",
                "                             n_estimators=n_estimators,\n",
                "                             random_state=0)\n",
                "    ada_train_err = np.zeros((n_estimators,))\n",
                "\n",
                "    ada.fit(X_train, y_train)\n",
                "    for i, y_pred in enumerate(ada.staged_predict(X_test)):\n",
                "        ada_train_err[i] = zero_one_loss(y_pred, y_test)\n",
                "\n",
                "    smoothed = []\n",
                "    # use moving average filter to smooth plots -- done to make easier\n",
                "    # to see trends; you are encouraged to also plot 'ada_train_err' to\n",
                "    # see the actual error plots!!\n",
                "    for i in range(len(ada_train_err)):\n",
                "        temp = 0.\n",
                "        counter = 0.\n",
                "        for k in range(i-5, i+1):\n",
                "            if k >= 0:\n",
                "                temp += ada_train_err[k]\n",
                "                counter += 1.\n",
                "        smoothed.append(temp/counter)\n",
                "\n",
                "    ax.plot(np.arange(n_estimators) + 1, smoothed, styles[j],\n",
                "        label=\"AdaBoost on DT%d\" % depths[j],\n",
                "        color=\"red\")\n",
                "\n",
                "    j += 1\n",
                "\n",
                "ax.set_ylim((0.1, 0.3))\n",
                "ax.set_yscale('log')\n",
                "ax.set_xlabel(\"Number of Classifiers (for AdaBoost)\")\n",
                "ax.set_ylabel(\"Error [%], log scale\")\n",
                "ax.set_title(\"Decision Tree Classifier and AdaBoost vs. Test Error\")\n",
                "leg = ax.legend(loc='lower right', fancybox=True)\n",
                "leg.get_frame().set_alpha(0.7)\n",
                "plt.show()\n",
                "plt.close()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PART L\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(8,7))\n",
                "ax = fig.add_subplot(111)\n",
                "\n",
                "n_components = 100\n",
                "n_features = 30\n",
                "n_nonzero_coefs = 5\n",
                "# y = Xw; w is a sparse vector\n",
                "y_train, X_train, w = make_sparse_coded_signal(n_samples=1,\n",
                "                           n_components=n_components,\n",
                "                           n_features=n_features,\n",
                "                           n_nonzero_coefs=n_nonzero_coefs,\n",
                "                           random_state=0)\n",
                "# test set\n",
                "_, X_test, _ = make_sparse_coded_signal(n_samples=1,\n",
                "                           n_components=n_components,\n",
                "                           n_features=n_features,\n",
                "                           n_nonzero_coefs=n_nonzero_coefs,\n",
                "                           random_state=0)\n",
                "y_test = np.dot(X_test, w)\n",
                "\n",
                "np.random.seed(10)\n",
                "y_noised_train = y_train + 2e-1*np.random.randn(len(y_train))\n",
                "y_noised_test = y_test + 2e-1*np.random.randn(len(y_test))\n",
                "w_est, train_err, test_err = mp(y_noised_train, X_train, w,\n",
                "                                y_noised_test, X_test)\n",
                "\n",
                "ax.plot(np.arange(n_components), test_err, label=\"Maching Pursuit test error\")\n",
                "ax.plot(np.arange(n_components), train_err, label=\"Maching Pursuit train error\")\n",
                "ax.set_ylim((0., 2.0))\n",
                "ax.set_xlabel(\"Number of features used\")\n",
                "ax.set_ylabel(\"Reconstruction error\")\n",
                "ax.set_title(\"Maching Pursuit Train and Test Reconstruction Error\")\n",
                "\n",
                "leg = ax.legend(loc='upper right', fancybox=True)\n",
                "leg.get_frame().set_alpha(0.7)\n",
                "\n",
                "plt.show()\n",
                "plt.close()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}