{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# [HW 13] Neural Tangent Kernels\n",
                "\n",
                "## Important Notes:\n",
                "* The larger neural networks take significant amounts of memory to train. You will have to **run on Google [colab](http://colab.research.google.com/github/BerkeleyML/cs189-notebooks/hw13/prob4/prob4.ipynb) or your local machine for this notebook. Datahub will not work.**\n",
                "* In order to prevent duplicate images the animation generator and display code has to be broken into two cells. **You must re-run the upper cell before re-running the lower cell to get a new animation.**\n",
                "* Feel free to change the number of training samples as you work through the cells and comment on how the behavior changes with more or fewer samples.\n",
                "* You can also try switching between `nn.ReLU`, `nn.Tanh`, and any other activations you want to try. The only exception is the infinite-width deterministic kernel, for which we've only provided you with the ReLU activation version.\n",
                "* On a recent MacBook Pro, most cells take around 1 minute to run and the multiple networks cell takes 3-4 minutes. You may want to reduce the number of training steps while debugging your NTK implementation for faster iteration. Don't forget to reset the number of steps for your final submission.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Import and Helpers\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import copy\n",
                "import time\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.animation as animation\n",
                "from typing import List\n",
                "from IPython.display import HTML\n",
                "from matplotlib.patches import Ellipse\n",
                "plt.rcParams['figure.figsize'] = 10, 6\n",
                "plt.rcParams['font.size'] = 16\n",
                "plt.rcParams['animation.embed_limit'] = 50  # 50 MB for longer animations\n",
                "plt.rcParams['axes.labelweight'] = 'bold'\n",
                "colors = list(plt.cm.tab10.colors)\n",
                "np.random.seed(1234)\n",
                "torch.manual_seed(1234);\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def to_tensor(x):\n",
                "    return torch.from_numpy(x).float()\n",
                "\n",
                "\n",
                "def to_numpy(x):\n",
                "    return x.detach().cpu().numpy()\n",
                "\n",
                "\n",
                "# Dataset: N (x,y) pairs\n",
                "def gen_dataset(N):\n",
                "    x = np.random.rand(N) * 2 - 1\n",
                "    x = np.vstack([x, np.ones(N)]).T\n",
                "    x_t = to_tensor(x)\n",
                "    # y = np.random.normal(size=2)\n",
                "    y = 1 * np.sin(np.pi * x[:,0])\n",
                "    y_t = to_tensor(y).reshape(-1, 1)\n",
                "    return x, y, x_t, y_t\n",
                "\n",
                "\n",
                "# Training loop for GD on the neural networks\n",
                "def train_gd(ntk, x, y, epochs, lr):\n",
                "    opt = torch.optim.SGD(ntk.parameters(), lr=lr)\n",
                "    losses = np.zeros(epochs)\n",
                "    for e in range(epochs):\n",
                "        opt.zero_grad()\n",
                "        loss = torch.mean((y - ntk(x)) ** 2)\n",
                "        loss.backward()\n",
                "        opt.step()\n",
                "        losses[e] = loss.item()\n",
                "    return losses\n",
                "\n",
                "\n",
                "def infinite_kernel(X, depth):\n",
                "    \"\"\"\n",
                "    Calculate the infinite-width NTK for a network with 'depth' layers\n",
                "    and ReLU nonlinearities.\n",
                "\n",
                "    Adapted from https://github.com/LeoYu/neural-tangent-kernel-UCI.\n",
                "    See https://arxiv.org/pdf/1904.11955.pdf for a full derivation.\n",
                "    \"\"\"\n",
                "    K = np.zeros((depth, X.shape[0], X.shape[0]))\n",
                "    S = X @ X.T\n",
                "    H = np.zeros_like(S)\n",
                "    for d in range(depth):\n",
                "        H += S\n",
                "        K[d] = H\n",
                "        L = np.diag(S)\n",
                "        P = np.clip(np.sqrt(np.outer(L, L)), a_min = 1e-9, a_max = None)\n",
                "        Sn = np.clip(S / P, a_min = -1, a_max = 1)\n",
                "        S = (Sn * (np.pi - np.arccos(Sn)) + np.sqrt(1.0 - Sn * Sn)) * P / 2.0 / np.pi\n",
                "        H = H * (np.pi - np.arccos(Sn)) / 2.0 / np.pi\n",
                "    return K[depth - 1]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# (h.1) NTK Implementation\n",
                "\n",
                "Since we expect the change in parameters during training to be small for sufficiently large networks, we can approximate the gradient flow during training with a 1st order Taylor expansion. Letting $w$ be a vector of all the trainable parameters in our network, we have\n",
                "\n",
                "$$ y(\\mathbf{w}) = y(\\mathbf{w}_0) + \\nabla_{\\mathbf{w}_0} f(x; \\mathbf{w}_0) (\\mathbf{w} - \\mathbf{w}_0). $$\n",
                "\n",
                "Using this linearized version of the network we can then learn $\\mathbf{\\beta} = (\\mathbf{w} - \\mathbf{w}_0)$ using the linear regression or classification method of our choice. For this problem we will use the min-norm least squares regression solution, with $\\Phi(x) = \\nabla_\\mathbf{w} f(x; \\mathbf{w}_0)$ as the kernel featurization and $y(\\mathbf{w}) - y(\\mathbf{w}_0)$ as the target.\n",
                "\n",
                "In the cell below we have given you the infrastructure for a neural network and it's neural tangent kernel implementation. Note that this network includes a scaling parameter $\\alpha$ which allows smaller changes in the parameters $\\mathbf{w}$ to produce similar changes in the network function $f(x; \\mathbf{w})$. This helps smaller networks behave similarly to the NTK. You can see how $\\alpha$ is used in the `forward` method of the `NTK` class. You will also need to use the `self.layers0` member which contains a copy of the network parameters at initialization.\n",
                "\n",
                "**Implement the following:**\n",
                "* **The kernel prediction $\\hat{y} = y(\\mathbf(w)_0) + \\nabla_{\\mathbf{w}_0} f(x) (\\mathbf{w} - \\mathbf{w}_0)$**\n",
                "* **The kernel Gram matrix $K = \\Phi(x) \\Phi(x)^T$**\n",
                "* **The min-norm least squares solution for $\\mathbf{\\beta} = (\\mathbf{w} - \\mathbf{w}_0)$**\n",
                "\n",
                "This is the only coding you will need to do for this problem.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Linear layer initialized with N(0,1) weights and biases, with sqrt(1/dL) normalization\n",
                "# to ensure constant output variance with growing width.\n",
                "class NTKLinearLayer(nn.Linear):\n",
                "    def __init__(self, in_features, out_features, bias=False, beta=0.1):\n",
                "        self.beta = beta\n",
                "        super(NTKLinearLayer, self).__init__(in_features, out_features, bias=bias)\n",
                "        self.reinitialize()\n",
                "\n",
                "    def reinitialize(self):\n",
                "        nn.init.normal_(self.weight, mean=0, std=1)\n",
                "        if self.bias is not None:\n",
                "            nn.init.normal_(self.bias, mean=0, std=1)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return F.linear(x, self.weight / np.sqrt(self.in_features / 2), None if self.bias is None else self.bias * self.beta)\n",
                "\n",
                "    def extra_repr(self):\n",
                "        return 'in_features={}, out_features={}, bias={}, beta={}'.format(\n",
                "            self.in_features, self.out_features, self.bias is not None, self.beta\n",
                "        )\n",
                "\n",
                "\n",
                "class NTK(nn.Module):\n",
                "    def __init__(self, hidden_layers: List[int], activation, alpha=1, bias_beta=0.1):\n",
                "        \"\"\"\n",
                "        hidden_layers: list of ints specifying layer widths\n",
                "        activation: an activation from nn.{ReLU,Tanh,...} to use as the hidden layer activation\n",
                "        alpha: scaling on the output\n",
                "        beta: scaling on biases to reduce their influence during training\n",
                "        \"\"\"\n",
                "        super(NTK, self).__init__()\n",
                "        self.alpha = alpha\n",
                "        self.bias_beta = bias_beta\n",
                "        self.hidden_layers = hidden_layers\n",
                "        layer_sizes = [2] + hidden_layers + [1]\n",
                "        self.layers = nn.Sequential(\n",
                "            NTKLinearLayer(layer_sizes[0], layer_sizes[1], bias=False, beta=bias_beta),\n",
                "            *[layer for i in range(1, len(layer_sizes)-1)\n",
                "              for layer in (activation(),\n",
                "                            NTKLinearLayer(layer_sizes[i], layer_sizes[i+1], bias=False, beta=bias_beta))\n",
                "            ]\n",
                "        )\n",
                "        # Save a copy of the initial network\n",
                "        self.layers0 = nn.Sequential(\n",
                "            NTKLinearLayer(layer_sizes[0], layer_sizes[1], bias=False, beta=bias_beta),\n",
                "            *[layer for i in range(1, len(layer_sizes)-1)\n",
                "              for layer in (activation(),\n",
                "                            NTKLinearLayer(layer_sizes[i], layer_sizes[i+1], bias=False, beta=bias_beta))\n",
                "            ]\n",
                "        )\n",
                "        self.layers0.load_state_dict(self.layers.state_dict())\n",
                "        self.w0 = nn.utils.parameters_to_vector(self.parameters()).detach()\n",
                "        self.kernel = None\n",
                "        self.Phi = None\n",
                "        self.beta = None\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.alpha * self.layers(x)\n",
                "\n",
                "    def forward_kernel(self, x):\n",
                "        assert self.kernel is not None, \"Kernel must be initialized before calling forward_kernel\"\n",
                "        assert self.beta is not None, \"MNLS solution must be found before calling forward_kernel\"\n",
                "        # Get features for test x\n",
                "        out = self(x)\n",
                "        m, _ = out.shape\n",
                "        phi = torch.zeros(m, self.nparams(), requires_grad=False)\n",
                "        for i in range(m):\n",
                "            self.zero_grad()\n",
                "            out[i].backward(retain_graph=True)\n",
                "            p_grad = torch.tensor([], requires_grad=False)\n",
                "            for p in self.parameters():\n",
                "                p_grad = torch.cat((p_grad, p.grad.reshape(-1)))\n",
                "            phi[i, :] = p_grad\n",
                "        with torch.no_grad():\n",
                "            # TODO: Compute and return the kernel prediction y_hat\n",
                "            ### start forward_kernel ###\n",
                "\n",
                "            ### end forward_kernel ###\n",
                "\n",
                "    def nparams(self):\n",
                "        nparams = 0\n",
                "        for l in self.layers:\n",
                "            if isinstance(l, nn.Linear):\n",
                "                nparams += l.weight.numel()\n",
                "                if l.bias is not None:\n",
                "                    nparams += l.bias.numel()\n",
                "        return nparams\n",
                "\n",
                "    def reinitialize(self):\n",
                "        for l in self.layers:\n",
                "            if isinstance(l, nn.Linear):\n",
                "                l.reinitialize()\n",
                "        self.w0 = nn.utils.parameters_to_vector(self.parameters()).detach()\n",
                "        self.layers0.load_state_dict(self.layers.state_dict())\n",
                "        self.kernel = None\n",
                "        self.Phi = None\n",
                "        self.beta = None\n",
                "\n",
                "    def compute_kernel_mnls(self, X, y=None):\n",
                "        \"\"\"\n",
                "        Calculate the neural tangent kernel of the model on the inputs\n",
                "        and the min-norm least squares coefficients for this kernel (if\n",
                "        y is provided).\n",
                "        \"\"\"\n",
                "        # Forward pass on X so we can get gradients\n",
                "        out = self(X)\n",
                "        p = self.nparams()\n",
                "        n, outdim = out.shape\n",
                "        assert outdim == 1, \"Output dimension must be 1\"\n",
                "\n",
                "        # This is the transposed Jacobian (grad y(w))^T)\n",
                "        Phi = torch.zeros(n, p, requires_grad=False)\n",
                "        for i in range(n):\n",
                "            # Find gradient vector induced by this data point\n",
                "            self.zero_grad()\n",
                "            out[i].backward(retain_graph=True)\n",
                "            p_grad = torch.tensor([], requires_grad=False)\n",
                "            for p in self.layers.parameters():\n",
                "                p_grad = torch.cat((p_grad, p.grad.reshape(-1)))\n",
                "            Phi[i, :] = p_grad\n",
                "\n",
                "        self.Phi = Phi\n",
                "        # TODO: Compute the tangent kernel Gram matrix K\n",
                "        ### start compute_gram ###\n",
                "\n",
                "        ### end compute_gram ###\n",
                "        self.kernel = K\n",
                "\n",
                "        # If y is provided, compute the MNLS weights\n",
                "        if y is not None:\n",
                "            # TODO: Compute the MNLS regression weights for the neural tangent kernel\n",
                "            # Hint: Be sure to properly account for y(w0)\n",
                "            ### start beta ###\n",
                "\n",
                "            ### end beta ###\n",
                "            self.beta = beta\n",
                "\n",
                "    def norm_diff_relative(self):\n",
                "        w = nn.utils.parameters_to_vector(self.parameters())\n",
                "        return torch.norm(w - self.w0) / torch.norm(self.w0)\n",
                "\n",
                "    def parameters(self):\n",
                "        return self.layers.parameters()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# (h.2) Evolution of Hidden Weights\n",
                "\n",
                "The plots below demonstrate the behavior of increasingly large networks during training with gradient descent.\n",
                "\n",
                "The first animation visualizes the weight matrix between the hidden layers as training progresses. You should see that the smaller networks' weights vary more widely, while the larger networks look almost constant.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "x, y, x_t, y_t = gen_dataset(5)\n",
                "ntks = [NTK([h,h], nn.ReLU, alpha=1) for h in [5, 10, 50, 100]]\n",
                "fig, axes = plt.subplots(2,2,figsize=[16,16]);\n",
                "imgs = np.array([[axes[j,i].matshow(\n",
                "    ntks[i+j*2].layers[2].weight.detach(), vmin=-1.5, vmax=1.5)\n",
                "                  for j in range(2)] for i in range(2)])\n",
                "\n",
                "fig.subplots_adjust(right=0.85)\n",
                "cbar_ax = fig.add_axes([0.9, 0.15, 0.025, 0.7])\n",
                "fig.colorbar(imgs[0][0], cax=cbar_ax)\n",
                "\n",
                "def animate(i, epochs=1):\n",
                "    for i in range(2):\n",
                "        for j in range(2):\n",
                "            train_gd(ntks[i+j*2], x_t, y_t, epochs=epochs, lr=1)\n",
                "            imgs[i,j].set_array(ntks[i+j*2].layers[2].weight.detach())\n",
                "    return imgs.flatten()\n",
                "\n",
                "ani = animation.FuncAnimation(\n",
                "    fig, animate, fargs=(10,), interval=30, blit=True, save_count=100);\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t0 = time.time()\n",
                "HTML(ani.to_jshtml())\n",
                "t1 = time.time()\n",
                "print(\"Rendered in {} minutes {:.1f} seconds\".format(int(np.floor((t1 - t0) / 60)), (t1 - t0) % 60))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameter Change Relative Norm\n",
                "\n",
                "The plots below show the relative size of the change in parameters during training, along with the training loss.\n",
                "\n",
                "In the space provided, **comment on the relationship between the relative change in weights and the training loss. Compare across the network sizes and across any stages you observe during training for a particular network.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x, y, x_t, y_t = gen_dataset(5)\n",
                "updates = 2000\n",
                "epochs_per_update = 1\n",
                "sizes = [5, 10, 50, 100]\n",
                "ntks = [NTK([h,h], nn.ReLU, alpha=1) for h in sizes]\n",
                "norm_diffs = np.zeros((updates+1, len(ntks)))\n",
                "train_losses = np.zeros((updates, len(ntks)))\n",
                "t0 = time.time()\n",
                "for u in range(updates):\n",
                "    for n, ntk in enumerate(ntks):\n",
                "        losses = train_gd(ntk, x_t, y_t, epochs=epochs_per_update, lr=.02)\n",
                "        norm_diffs[u+1, n] = ntk.norm_diff_relative()\n",
                "        train_losses[u, n] = losses[-1]\n",
                "t1 = time.time()\n",
                "print(\"Trained in {} minutes {:.1f} seconds\".format(int(np.floor((t1 - t0) / 60)), (t1 - t0) % 60))\n",
                "\n",
                "plt.figure(figsize=[10,12])\n",
                "plt.subplot(2,1,1)\n",
                "for i in range(len(ntks)):\n",
                "    plt.plot(np.arange(updates+1) * epochs_per_update, norm_diffs[:, i], label=\"hidden_size = {}\".format(sizes[i]))\n",
                "plt.legend()\n",
                "plt.xlabel(\"Epochs\")\n",
                "plt.ylabel(r\"$\\frac{\\|w - w_0\\|}{\\|w_0\\|}$\", fontsize=24, fontweight='bold')\n",
                "\n",
                "plt.subplot(2,1,2)\n",
                "for i in range(len(ntks)):\n",
                "    plt.plot(np.arange(updates) * epochs_per_update, train_losses[:, i], label=\"hidden_size = {}\".format(sizes[i]))\n",
                "plt.legend()\n",
                "plt.xlabel(\"Epochs\")\n",
                "plt.ylabel(\"Train Loss\");\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### start relative_norm_sol ###\n",
                "\n",
                "### end relative_norm_sol ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# (h.3) Single Net GD Training vs NTK Least Squares Regression\n",
                "\n",
                "The animation below shows the NTK regression function and the training process of gradient descent for a relatively large network. In the space provided, **comment on the difference between the NTK regression function and the function learned by gradient descent once it is mostly converged.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "N = 5\n",
                "x, y, x_t, y_t = gen_dataset(N)\n",
                "x_test = np.vstack([np.linspace(-1,1,100), np.ones(100)]).T\n",
                "x_test_t = to_tensor(x_test)\n",
                "# Initialize network and finite width kernel\n",
                "ntk = NTK([500,500,500], nn.ReLU, alpha=1)\n",
                "ntk.compute_kernel_mnls(x_t, y_t)\n",
                "# Calculate and use infinite width kernel\n",
                "Kinf = infinite_kernel(np.vstack([x, x_test]), 4)\n",
                "Ktrain = Kinf[:N, :N]\n",
                "Ktest_train = Kinf[N:, :N]\n",
                "y0 = to_numpy(ntk(x_t))\n",
                "yinf = to_numpy(ntk(x_test_t)) + Ktest_train @ np.linalg.pinv(Ktrain, hermitian=True, rcond=1e-9) @ (y.reshape(-1, 1) - y0)\n",
                "\n",
                "# Plotting\n",
                "fig, ax = plt.subplots()\n",
                "plt.plot(x_test[:,0], to_numpy(ntk.forward_kernel(x_test_t)), linewidth=2, label=\"Kernel MNLS\")\n",
                "plt.plot(x_test[:,0], yinf, linewidth=2, label=\"$\\infty$ Width Kernel MNLS\", c='k')\n",
                "line, = plt.plot(x_test[:,0], to_numpy(ntk(x_test_t)), label=\"GD\")\n",
                "plt.scatter(x[:,0], y, label=\"Training Points\")\n",
                "plt.legend()\n",
                "plt.xlim([-1, 1])\n",
                "plt.ylim(-1.5, 1.5)\n",
                "plt.xlabel(\"$x$\")\n",
                "plt.ylabel(\"$y$\")\n",
                "\n",
                "def animate(i):\n",
                "    if i == 0:\n",
                "        return line,\n",
                "    train_gd(ntk, x_t, y_t, epochs=i, lr=.01)\n",
                "    line.set_ydata(to_numpy(ntk(x_test_t)))\n",
                "    return line,\n",
                "\n",
                "ani = animation.FuncAnimation(\n",
                "    fig, animate, interval=30, blit=True, save_count=200)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t0 = time.time()\n",
                "HTML(ani.to_jshtml())\n",
                "t1 = time.time()\n",
                "print(\"Rendered in {} minutes {:.1f} seconds\".format(int(np.floor((t1 - t0) / 60)), (t1 - t0) % 60))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### start single_net_gd_ntk ###\n",
                "\n",
                "### end single_net_gd_ntk ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# (h.4) Multiple Net GD Training vs NTK Least Squares Regression\n",
                "\n",
                "The animation below shows the NTK regression function and the training process of gradient descent for many relatively large networks. In the space provided, **comment on the variance in learned functions between the NTK function, between the GD trained functions, and across the two groups. Compare them to the infinite width deterministic kernel. Try running more than once to see if the behavior is consistent.**\n",
                "\n",
                "Dotted lines represent the NTK regression functions. Solid lines represent the GD trained regression functions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "N = 5\n",
                "x, y, x_t, y_t = gen_dataset(N)\n",
                "n_ntks = 10\n",
                "x_test = np.vstack([np.linspace(-1,1,100), np.ones(100)]).T\n",
                "x_test_t = to_tensor(x_test)\n",
                "\n",
                "fig, ax = plt.subplots()\n",
                "ntks = [NTK([100,100,100], nn.ReLU, alpha=1) for _ in range(n_ntks)]\n",
                "label = True\n",
                "ntk_line = None\n",
                "for i, ntk in enumerate(ntks):\n",
                "    ntk.compute_kernel_mnls(x_t, y_t)\n",
                "    if label:\n",
                "        ntk_line, = ax.plot(x_test[:,0], to_numpy(ntk.forward_kernel(x_test_t)), ':',\n",
                "                            label=\"NTK regression\", c=colors[i])\n",
                "        label = False\n",
                "    else:\n",
                "        ax.plot(x_test[:,0], to_numpy(ntk.forward_kernel(x_test_t)), ':', c=colors[i])\n",
                "\n",
                "# Calculate and use infinite width kernel\n",
                "Kinf = infinite_kernel(np.vstack([x, x_test]), 4)\n",
                "Ktrain = Kinf[:N, :N]\n",
                "Ktest_train = Kinf[N:, :N]\n",
                "y0 = to_numpy(ntk(x_t))\n",
                "yinf = to_numpy(ntk(x_test_t)) + Ktest_train @ np.linalg.pinv(Ktrain, hermitian=True, rcond=1e-9) @ (y.reshape(-1, 1) - y0)\n",
                "linf, = plt.plot(x_test[:,0], yinf, linewidth=2, label=\"$\\infty$ Width Kernel MNLS\", c='k')\n",
                "\n",
                "lines = []\n",
                "for i in range(n_ntks):\n",
                "    if i == 0:\n",
                "        line, = ax.plot(x_test[:,0], to_numpy(ntks[i](x_test_t)), label=\"GD regression\", c=colors[i])\n",
                "    else:\n",
                "        line, = ax.plot(x_test[:,0], to_numpy(ntks[i](x_test_t)), c=colors[i])\n",
                "    lines.append(line)\n",
                "sc = ax.scatter(x[:,0], y, label=\"Training Points\")\n",
                "plt.xlim([-1, 1])\n",
                "plt.ylim(-1.5, 1.5)\n",
                "plt.xlabel(\"$x$\")\n",
                "plt.ylabel(\"$y$\")\n",
                "plt.legend(handles=[ntk_line, lines[0], linf, sc])\n",
                "\n",
                "def animate(i):\n",
                "    if i > 0:\n",
                "        for ntk in ntks:\n",
                "            train_gd(ntk, x_t, y_t, epochs=i, lr=.01)\n",
                "    for i in range(len(lines)):\n",
                "        lines[i].set_ydata(to_numpy(ntks[i](x_test_t)))\n",
                "    return lines\n",
                "\n",
                "ani = animation.FuncAnimation(\n",
                "    fig, animate, interval=50, blit=True, save_count=200);\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t0 = time.time()\n",
                "HTML(ani.to_jshtml())\n",
                "t1 = time.time()\n",
                "print(\"Rendered in {} minutes {:.1f} seconds\".format(int(np.floor((t1 - t0) / 60)), (t1 - t0) % 60))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### start mult_net_gd_ntk ###\n",
                "\n",
                "### end mult_net_gd_ntk ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# (h.5) Kernel Ellipses and Convergence\n",
                "\n",
                "For the animation below we will work with only 2 training samples, which allows us to easily visualize the $\\mathbb{R}^{2\\times2}$ kernel Gram matrix. The animation below shows an ellipse determined by the eigenvalues and eigenvectors of the kernel matrix for the network _determined at that point in training_ and a history of the predictions during training. The axes are $\\hat{y}_1$ and $\\hat{y}_2$, the predictions given by the network for the two training points $x_1$ and $x_2$. The ellipses are centered at $(y_1, y_2)$ to show the desired outputs of the network. The colors of the ellipses are matched to the output history scatter points.\n",
                "\n",
                "In the space provided, **comment on the relationship between the convergence behavior of $\\hat{y}_1$ and $\\hat{y}_2$ and the kernel ellipses. Also comment on the stability of the kernel matrix during training for larger  and smaller networks.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def kernel_to_angle(A):\n",
                "    l, Q = np.linalg.eigh(A)\n",
                "    q1 = Q[0]\n",
                "    ang = np.arctan2(q1[1], q1[0]) * 180/np.pi\n",
                "    return 1/l[0], 1/l[1], ang\n",
                "\n",
                "\n",
                "def update_ellipse(e, w, h, ang):\n",
                "    e.width = w\n",
                "    e.height = h\n",
                "    e.angle = ang\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "x, y, x_t, y_t = gen_dataset(2)\n",
                "print(x,y)\n",
                "\n",
                "updates = 150\n",
                "center = y_t\n",
                "c = 1\n",
                "fig, ax = plt.subplots()\n",
                "\n",
                "sizes = [5,10,50,100,500]\n",
                "ntks = [NTK([h, h, h], nn.ReLU, alpha=1) for h in sizes]\n",
                "ellipses = []\n",
                "yhats = []\n",
                "scs = []\n",
                "for i, ntk in enumerate(ntks):\n",
                "    ntk.compute_kernel_mnls(x_t, y_t)\n",
                "    K = ntk.kernel\n",
                "    w, h, angle = kernel_to_angle(K)\n",
                "    ell = Ellipse(center, w*c, h*c, angle, fill=False, color=colors[i], linewidth=3, alpha=0.7)\n",
                "    ellipses.append(ell)\n",
                "    ax.add_patch(ell)\n",
                "\n",
                "    yhat = torch.zeros((updates, 2))\n",
                "    yhat[0, :] = ntk(x_t).detach().T\n",
                "    yhat[:, 0] = yhat[0, 0]\n",
                "    yhat[:, 1] = yhat[0, 1]\n",
                "    yhats.append(yhat)\n",
                "    sc = ax.scatter(yhat[:,0], yhat[:,1], color=colors[i], label='Size {}'.format(sizes[i]))\n",
                "    scs.append(sc)\n",
                "\n",
                "ax.set_xlim([-2, 2])\n",
                "ax.set_ylim([-2, 2])\n",
                "ax.legend(loc='upper left');\n",
                "ax.set_xlabel(\"$\\hat{y}_1$\")\n",
                "ax.set_ylabel(\"$\\hat{y}_2$\")\n",
                "\n",
                "def animate(i, epochs):\n",
                "    if i == 0:\n",
                "        return scs + ellipses\n",
                "    for n, ntk in enumerate(ntks):\n",
                "        train_gd(ntk, x_t, y_t, epochs=epochs, lr=.05)\n",
                "        ntk.compute_kernel_mnls(x_t, y_t)\n",
                "        K = ntk.kernel\n",
                "        w, h, a = kernel_to_angle(K)\n",
                "        update_ellipse(ellipses[n], w*c, h*c, a)\n",
                "        yhats[n][i, :] = ntk(x_t).detach().T\n",
                "        scs[n].set_offsets(yhats[n])\n",
                "    return scs + ellipses\n",
                "\n",
                "\n",
                "ani = animation.FuncAnimation(\n",
                "    fig, animate, fargs=(1,), interval=50, blit=True, save_count=updates);\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t0 = time.time()\n",
                "HTML(ani.to_jshtml())\n",
                "t1 = time.time()\n",
                "print(\"Rendered in {} minutes {:.1f} seconds\".format(int(np.floor((t1 - t0) / 60)), (t1 - t0) % 60))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### start ellipse_convergence ###\n",
                "\n",
                "### end ellipse_convergence ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# (h.6) Infinite Width Deterministic Kernel\n",
                "\n",
                "It turns out that as the width of the hidden layers goes to infinity, the neural tangent kernel becomes deterministic. We can calculate it based on the number of hidden layers and the activation function, as you see in the function `infinite_kernel` below.\n",
                "\n",
                "The following animation demonstrates the convergence of kernels from networks of increasing size, shown on the left, to the deterministic infinite width kernel on the right. In the space provided, **comment on the differences between the kernel for small networks and the infinite width kernel, and how they converge.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "x, y, x_t, y_t = gen_dataset(5)\n",
                "\n",
                "Kinf = infinite_kernel(x, 3)\n",
                "Kmin = np.min(Kinf)\n",
                "Kmax = np.max(Kinf)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
                "img_inf = axes[1].imshow(Kinf, vmin=Kmin, vmax=Kmax)\n",
                "axes[1].set_title(\"Infinite-width Kernel\")\n",
                "\n",
                "fig.subplots_adjust(right=0.85)\n",
                "cbar_ax = fig.add_axes([0.9, 0.15, 0.025, 0.7])\n",
                "fig.colorbar(img_inf, cax=cbar_ax)\n",
                "\n",
                "def animate(i, widths):\n",
                "    d = widths[i]\n",
                "    ntk = NTK([d,d], nn.ReLU, alpha=1)\n",
                "    ntk.compute_kernel_mnls(x_t, y_t)\n",
                "    img = axes[0].imshow(ntk.kernel / 2 ** 2, vmin=Kmin, vmax=Kmax)\n",
                "    title = axes[0].set_title(\"Width={} Kernel\".format(d))\n",
                "    return img, title\n",
                "\n",
                "widths = [10,50,100,500,1000]\n",
                "ani = animation.FuncAnimation(\n",
                "    fig, animate, fargs=(widths,), interval=500, blit=True, save_count=len(widths));\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t0 = time.time()\n",
                "HTML(ani.to_jshtml())\n",
                "t1 = time.time()\n",
                "print(\"Rendered in {} minutes {:.1f} seconds\".format(int(np.floor((t1 - t0) / 60)), (t1 - t0) % 60))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### start infinite_width ###\n",
                "\n",
                "### end infinite_width ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# (h.7) Eigenvalues of Kernel Gram Matrices\n",
                "\n",
                "**Compare the eigenvalues and conditioning of the Gram matrices across the varying widths.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x, y, x_t, y_t = gen_dataset(25)\n",
                "\n",
                "widths = [10,50,100,500,1000]\n",
                "for d in widths:\n",
                "    ntk = NTK([d]*3, nn.ReLU, alpha=1)\n",
                "    ntk.compute_kernel_mnls(x_t, y_t)\n",
                "    eigs, _ = np.linalg.eigh(ntk.kernel / 2**3)\n",
                "    plt.plot(sorted(eigs)[::-1], 'o-', label='d=%d' % d)\n",
                "\n",
                "Kinf = infinite_kernel(x, 4)\n",
                "eigs, _ = np.linalg.eigh(Kinf)\n",
                "plt.plot(sorted(eigs)[::-1], 'o--', label='d=$\\infty$', linewidth=3, c='k')\n",
                "\n",
                "plt.legend()\n",
                "plt.yscale('log')\n",
                "plt.show();\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### start conditioning ###\n",
                "\n",
                "### end conditioning ###\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}