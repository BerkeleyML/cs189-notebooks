{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# k-means clustering\n",
                "\n",
                "In this notebook, we will experiment with k-means clustering, looking at cases when it succeeds and when it fails to work.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import scipy.linalg\n",
                "import matplotlib.pyplot as plt\n",
                "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
                "\n",
                "RESCALE_DATA = False # We'll modify this in part (f)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below is a partial implementation of k-means, initialized with means chosen randomly from the input set (Forgy initialization). Essentially, the algorithm is as follows: `update_means`, then `assign_clusters`, repeated until the cluster assignments stabilize. `update_means` sets the mean of each cluster to be its centroid of each cluster, then `assign_cluster` clusters points based on the mean closest to them.\n",
                "\n",
                "### Part (a)\n",
                "**Finish the implementation of `update_means` and `assign_clusters`.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def assign_clusters(data, means):\n",
                "    \"\"\"\n",
                "    Takes in a n x d data matrix, and a k x d matrix of the means.\n",
                "    Returns a length-n vector with the index of the closest mean to each data point.\n",
                "    \"\"\"\n",
                "    n, d = data.shape\n",
                "    k = means.shape[0]\n",
                "    assert d == means.shape[1], \"Means are of the wrong shape\"\n",
                "    out = np.zeros(n)\n",
                "    for i, x in enumerate(data):\n",
                "        # Set out[i] to be the cluster whose mean is closest to point x\n",
                "\n",
                "        ### start assign_cluster ###\n",
                "\n",
                "        ### end assign_cluster ###\n",
                "    return out\n",
                "\n",
                "def update_means(data, clusters):\n",
                "    \"\"\"\n",
                "    Takes in an n x d data matrix, and a length-n vector of the\n",
                "    cluster indices of each point.\n",
                "    Computes the mean of each cluster and returns a k x d matrix of the means.\n",
                "    \"\"\"\n",
                "    n, d = data.shape\n",
                "    assert len(clusters) == n\n",
                "    k = len(set(clusters))\n",
                "    cluster_means = []\n",
                "    for i in range(k):\n",
                "        # Set `cluster_mean` to be the mean of all points in cluster `i`\n",
                "        # (Assume at least one such point exists)\n",
                "\n",
                "        ### start update_means ###\n",
                "\n",
                "        ### end update_means ###\n",
                "        cluster_means.append(cluster_mean)\n",
                "    return np.array(cluster_means)\n",
                "\n",
                "def cost(data, clusters, means):\n",
                "    \"\"\"\n",
                "    Computes the sum of the squared distance between each point\n",
                "    and the mean of its associated cluster\n",
                "    \"\"\"\n",
                "    out = 0\n",
                "    n, d = data.shape\n",
                "    k = means.shape[0]\n",
                "    assert means.shape[1] == d\n",
                "    assert len(clusters) == n\n",
                "    for i in range(k):\n",
                "        out += np.linalg.norm(data[clusters == i] - means[i])\n",
                "    return out\n",
                "\n",
                "def k_means_cluster(data, k):\n",
                "    \"\"\"\n",
                "    Takes in an n x d data matrix and parameter `k`.\n",
                "    Yields the cluster means and cluster assignments after\n",
                "    each step of running k-means, in a 2-tuple.\n",
                "    \"\"\"\n",
                "    n, d = data.shape\n",
                "    means = data[np.random.choice(n, k, replace=False)]\n",
                "    assignments = assign_clusters(data, means)\n",
                "    while True:\n",
                "        yield means, assignments\n",
                "        means = update_means(data, assignments)\n",
                "        if RESCALE_DATA:\n",
                "            # This is for part (f), not part (a)\n",
                "            new_assignments = assign_clusters(*transform_points(data, assignments, means))\n",
                "        else:\n",
                "            new_assignments = assign_clusters(data, means)\n",
                "        if np.all(assignments == new_assignments):\n",
                "            yield means, assignments\n",
                "            print(\"Final cost = {}\".format(cost(data, assignments, means)))\n",
                "            break\n",
                "        assignments = new_assignments\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These are just some utility methods that will prove handy when conducting our experiments.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def final_k_means_cluster(data, k):\n",
                "    out = list(k_means_cluster(data, k))\n",
                "    return out[-1]\n",
                "\n",
                "def plot_clustering(data, means, assignments):\n",
                "    k = len(means)\n",
                "    for j in range(k):\n",
                "        plt.scatter(*data[assignments == j].T)\n",
                "    plt.scatter(*means.T, marker=\"x\", s=240, c=\"black\")\n",
                "    plt.show()\n",
                "\n",
                "def interact_clustering(data, logger):\n",
                "    history = list(logger)\n",
                "    k = history[0][0].shape[0]\n",
                "\n",
                "    def plotter(i):\n",
                "        plot_clustering(data, *history[i])\n",
                "\n",
                "    interact(plotter, i=IntSlider(min=0, max=len(history) - 1, continuous_update=False))\n",
                "\n",
                "def demo(classes, history=False):\n",
                "    for c in classes:\n",
                "        plt.scatter(*c.T)\n",
                "    plt.show()\n",
                "\n",
                "    points = np.vstack(classes)\n",
                "\n",
                "    if history:\n",
                "        interact_clustering(points, k_means_cluster(points, len(classes)))\n",
                "    else:\n",
                "        means, assignments = final_k_means_cluster(points, len(classes))\n",
                "        plot_clustering(points, means, assignments)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (b)\n",
                "Now that you've completed your implementation, let's see k-means in action! First, we will generate some points from two isotropic Gaussian distributions, stacked together. Our goal will be for k-means to separate out the points from each distribution.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gen_gaussian_points(n, mean, sigma):\n",
                "    return np.random.normal(mean, sigma, [n, 2])\n",
                "\n",
                "\n",
                "N = 100\n",
                "\n",
                "class_a = gen_gaussian_points(N, [-1, 0], [1, 1])\n",
                "class_b = gen_gaussian_points(N, [1, 0], [1, 1])\n",
                "\n",
                "points = np.vstack([class_a, class_b])\n",
                "\n",
                "plt.scatter(*class_a.T)\n",
                "plt.scatter(*class_b.T)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The above points are reasonably well separated, but there is some overlap between the distributions. Now we will run k-means clustering on this (unlabeled) set of points, to see how well they are separated.\n",
                "\n",
                "Run the below cell a couple of times and see how the clustering works. **Does the initial choice of means (indicated in green) matter, in this case? What happens if we try to fit 3 or more clusters, or if we vary the spacing between the Gaussian means?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "interact_clustering(points, k_means_cluster(points, 2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### start two-gaussians-comments ###\n",
                "\n",
                "### end two-gaussians-comments ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (c)\n",
                "Above, we saw the \"ideal\" case of k-means, with reasonable well-separated clusters each drawn from isotropic Gaussians. Now we will look at some datasets with non-ideal properties, on which k-means performs poorly for various results.\n",
                "\n",
                "One problem with k-means is that it can be sensitive to the initial choice of means. Below, we construct a dataset  with three equally spaced point sets of roughly equal size sampled from isotropic Gaussians, as well as a fourth point set slightly removed from the other three of a smaller size.\n",
                "\n",
                "**Run the below cell a few times. Does k-means always succesfully separate the four classes of points as you'd expect?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_a = gen_gaussian_points(N, [-3, -1], [1, 1])\n",
                "class_b = gen_gaussian_points(N, [3, -1], [1, 1])\n",
                "class_c = gen_gaussian_points(N, [0, 3], [1, 1])\n",
                "class_d = gen_gaussian_points(10, [0, 15], [1, 1])\n",
                "\n",
                "\n",
                "demo([class_a, class_b, class_c, class_d], history=False) # consider changing this to history=True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You should see that in some cases, k-means fails to separate the four groups into their original four distributions. What does it do instead? Why is this the case? How does this affect the cost of the final clustering? **Comment on your observations.** (Consider passing the `history=True` flag into the call to `demo` above, to see the iterations of k-means as it separates the points.)\n",
                "\n",
                "### start four-clusters ###\n",
                "\n",
                "### end four-clusters ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (d)\n",
                "We next consider an example with three clusters spaced on the x-axis. You should see that k-means does a reasonable job of separating the clusters, at least visually. Now look at the exact output of the algorithm, In particular, look at the estimated cluster means that it returns.  **How do they compare to the actual means of the true distribution of each cluster? Justify this difference. Will it affect the classification of a new test point?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_a = gen_gaussian_points(N, [-2, 0], [1, 1])\n",
                "class_b = gen_gaussian_points(N, [0, 0], [1, 1])\n",
                "class_c = gen_gaussian_points(N, [2, 0], [1, 1])\n",
                "\n",
                "points = np.vstack([class_a, class_b, class_c])\n",
                "\n",
                "means, assignments = final_k_means_cluster(points, 3)\n",
                "plot_clustering(points, means, assignments)\n",
                "\n",
                "print(means)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### start cluster-means ###\n",
                "\n",
                "### end cluster-means ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (e)\n",
                "Now, we will look at what happens when our Gaussians are no longer isotropic, so they have much greater variance in one dimension versus another. Below, we generate two very well separated clusters, but that have high variance in the y-dimension compared to the x-dimension. **Comment on what happens when we apply k-means to cluster them.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# what happens if the Gaussians are not isotropic?\n",
                "RESCALE_DATA = False\n",
                "\n",
                "class_a = gen_gaussian_points(N, [-3, 0], [1, 10])\n",
                "class_b = gen_gaussian_points(N, [3, 0], [1, 10])\n",
                "demo([class_a, class_b])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Can you justify what's going on? How might we modify k-means to fix this issue, if we had some prior knowledge about the distribution of our data?**\n",
                "\n",
                "### start anisotropic-observations ###\n",
                "\n",
                "### end anisotropic-observations ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (f)\n",
                "One such modification is to rescale our coordinate system in the `assign_clusters` step, so the average covariance of our clusters is identity. In other words, our algorithm would be to:\n",
                " - `update_means`\n",
                " - Compute the average covariance $\\Sigma$ of all the clusters (in a manner very similar to LDA)\n",
                " - `assign_clusters`, but only after transforming our points such that the cluster covariance is identity.\n",
                "and repeat until our assignments have stabilized.\n",
                "\n",
                "We will implement this approach and see how it compares to the naive approach that ignores the cluster covariance at each step.\n",
                "\n",
                "Algebraically, let $X_i$ be the set of all points in class $i$, with each point in a separate column. Then\n",
                "$$\n",
                "    \\Sigma = \\frac{1}{n} \\sum_{i=1}^k (X_i - \\mu_i)(X_i - \\mu_i)^T,\n",
                "$$\n",
                "and we want to find a transformation $T$ such that\n",
                "$$\n",
                "    I = \\frac{1}{n} \\sum_{i=1}^k (TX_i - T\\mu_i)(TX_i - T\\mu_i)^T.\n",
                "$$\n",
                "\n",
                "**Implement `transform_points`, which computes and applies the desired linear transformation to the input points to correct for the cluster covariance**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def transform_points(data, clusters, means):\n",
                "    \"\"\"\n",
                "    `data` is an n x d matrix containing our input points.\n",
                "    `clusters` is a length-n vector of the cluster indices of each point.\n",
                "    `means` is a k x d matrix of the means of each cluster.\n",
                "\n",
                "    We want to return the transformed data and means after applying the desired transformation.\n",
                "    *IMPORTANT* Do not modify the input arrays, create a new array for your output.\n",
                "    \"\"\"\n",
                "    n, d = data.shape\n",
                "    k = means.shape[0]\n",
                "    assert len(clusters) == n\n",
                "    assert means.shape[1] == d\n",
                "\n",
                "    # first, we need to compute the average covariance of each cluster\n",
                "    sigma = np.zeros((d, d))\n",
                "    for i in range(k):\n",
                "        # Computes the cluster covariance of the ith cluster, and add it\n",
                "        # to the running total in `sigma`\n",
                "        cluster_points = data[clusters == i]\n",
                "        demeaned_cluster_points = cluster_points - means[i]\n",
                "        sigma += demeaned_cluster_points.T @ demeaned_cluster_points\n",
                "\n",
                "    sigma /= n\n",
                "\n",
                "    # This transformation should \"undo\" `sigma` on the data points, so the averaged\n",
                "    # cluster covariance after the transformation is just I\n",
                "    transformation = ...\n",
                "\n",
                "    ### start transformation ###\n",
                "\n",
                "    ### end transformation ###\n",
                "\n",
                "    # Remember that each *row* of `data` is a point, not each *column*.\n",
                "    # Depending on your `transformation` above, you may want to edit the\n",
                "    # below line.\n",
                "    return data @ transformation, means @ transformation\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see what happens when we run k-means clustering with this new step, running on the same dataset as before.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RESCALE_DATA = True # include the `transform_points` step in `k_means_cluster`\n",
                "\n",
                "class_a = gen_gaussian_points(N, [-3, 0], [1, 10])\n",
                "class_b = gen_gaussian_points(N, [3, 0], [1, 10])\n",
                "demo([class_a, class_b])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Does it work? Remember from part (c) that k-means can be sensitive to initial conditions, so try running the above cell several times to get a sense of the possible outcomes. **Comment on your observations.**\n",
                "\n",
                "### start transform-comments ###\n",
                "\n",
                "### end transform-comments ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notion of updating our estimate of the distribution of our data each time step is an important idea that we will explore next week, when we introduce the \"expectation-maximization algorithm\". That algorithm also runs in a loop, alternating between estimating clusters, and trying to model the distribution of each cluster, so what you have designed above is sort of a \"lite\" version of that algorithm.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}