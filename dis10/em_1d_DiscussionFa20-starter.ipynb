{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 10\n",
    "\n",
    "Import necessary packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import experiment2D\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from pdb import set_trace as st\n",
    "from matplotlib.ticker import StrMethodFormatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): K-means Could Fail Sometimes (2D Example) ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous discussion, we learned a powerful method for clustering, K-means. In the following example, we show that K-means could sometimes fail. The k-means algorithm implementation is exactly what you have implented in Discussion 9.\n",
    "\n",
    "We introduce Expectationâ€“maximization (EM) algorithm in this discussion. In the following example, EM algorithm could successfully recover true data labels.\n",
    "\n",
    "Note that different colors correspond to different clusters; the black cross represents the mean of each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2D(data_seed=11, alg_seed=12312, factor=1, num_samples=500, num_clusters=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's initialize differently by changing alg_seed. Will K-means and EM work?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2D(data_seed=11, alg_seed=12, factor=1, num_samples=500, num_clusters=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further change alg_seed and data_seed and observe which algorithm has better performance?**\n",
    "\n",
    "If you have time, also play with num_samples and num_clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Understanding EM (1D Example) ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand EM intuitively in this part. First we need to define some auxiliary functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(mu_1, mu_2, n_examples, sigma_1 = 0.5, sigma_2 = 0.5):\n",
    "    # Generate sample data points from 2 distributions: (mu_1, sigma_1) and (mu_2, sigma_2)\n",
    "    d_1 = np.random.normal(mu_1, sigma_1, n_examples)\n",
    "    d_2 = np.random.normal(mu_2, sigma_2, n_examples)\n",
    "    x = np.concatenate([d_1, d_2])\n",
    "    return d_1, d_2, x\n",
    "\n",
    "def plot_data(d_1, d_2):\n",
    "    # Plot scatter plot of data samples, labeled by class\n",
    "    plt.figure()\n",
    "    plt.scatter(d_1, np.zeros(len(d_1)), c='b', s=80., marker='+')\n",
    "    plt.scatter(d_2, np.zeros(len(d_2)), c='r', s=80.)\n",
    "    plt.title(\"Sample data using: mu = \" + str(mu) + \" n_train = \" + str(len(d_1)+len(d_2)))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_data_and_distr(d_1, d_2, mu_1, mu_2, sigma_1=0.5, sigma_2=0.5, title = \"\"):\n",
    "    # Plot scatter plot of data samples overlayed with distribution of estimated means: mu_1 and mu_2\n",
    "\n",
    "    plt.scatter(d_1, np.zeros(len(d_1)), c='b')\n",
    "    scale = [min(mu_1-3*sigma_1, mu_2-3*sigma_2), max(mu_1+3*sigma_1, mu_2+3*sigma_2)]\n",
    "    plt.scatter(d_2, np.zeros(len(d_2)), c='r')\n",
    "    plt.plot([mu_1, -mu_1],[0, 0], 'kx', markersize=20)\n",
    "    x_axis = np.arange(scale[0], scale[1], 0.001)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mu_1, sigma_1), c='b')\n",
    "    x_axis = np.arange(scale[0], scale[1], 0.001)\n",
    "    print(\"Current mu=\"+str(mu_1))\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mu_2,sigma_2), c='r')\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(d_1, np.sign(d_1), \"b.\")\n",
    "    plt.plot(d_2, np.sign(d_2), \"r.\")\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.title(\"Weight contributed to cluster 0\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_and_distr_em(d_1, d_2, mu_1, mu_2, data, wi, sigma_1=0.5, sigma_2=0.5, title = \"\"):\n",
    "    # Plot scatter plot of data samples overlayed with distribution of estimated means: mu_1 and mu_2\n",
    "    plt.scatter(d_1, np.zeros(len(d_1)), c='b')\n",
    "    scale = [min(mu_1-3*sigma_1, mu_2-3*sigma_2), max(mu_1+3*sigma_1, mu_2+3*sigma_2)]\n",
    "    plt.scatter(d_2, np.zeros(len(d_2)), c='r')\n",
    "    plt.plot([mu_1, -mu_1],[0, 0], 'kx', markersize=20)\n",
    "    x_axis = np.arange(scale[0], scale[1], 0.001)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mu_1, sigma_1), c='b')\n",
    "    x_axis = np.arange(scale[0], scale[1], 0.001)\n",
    "    print(\"Current mu=\"+str(mu_1))\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mu_2,sigma_2), c='r')\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    #plt.plot(data[wi>=0], wi[wi>=0], 'b.')\n",
    "    #plt.plot(data[wi<0], wi[wi<0], 'r.')\n",
    "    plt.plot(d_1, wi[:len(d_1)], 'b.')\n",
    "    plt.plot(d_2, wi[len(d_1):], 'r.')\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))\n",
    "    plt.title(\"Weight contributed to cluster 0\")\n",
    "    plt.show()\n",
    "\n",
    "def process_em(iteration):\n",
    "    ### You can play with these two parameters\n",
    "    mu = 0.5\n",
    "    rand_seed = 10\n",
    "    ###\n",
    "\n",
    "    np.random.seed(rand_seed)\n",
    "    mu_start = 0.1\n",
    "    n_train = 50\n",
    "    iteration = int(iteration)\n",
    "    d_1, d_2, x = generate_data(mu, -mu, n_examples = n_train)\n",
    "    #plot_data(d_1, d_2)\n",
    "    mu_em, sigma_em, diff_mu_em, mus_em, ws = em(x, mu_start, mu, iterations=50)\n",
    "\n",
    "    plot_data_and_distr_em(d_1, d_2, mus_em[iteration], -mus_em[iteration], x, ws[iteration], title=\"Estimated distribution using EM at iter \"+str(iteration))\n",
    "\n",
    "    print(\"True mean:{:.3f}, EM (final) estimate:{:.3f}\".format(mu, mu_em))\n",
    "\n",
    "def process_kmeans(iteration):\n",
    "    ### You can play with these two parameters\n",
    "    mu = 0.5\n",
    "    rand_seed = 10\n",
    "    ###\n",
    "\n",
    "    np.random.seed(rand_seed)\n",
    "    mu_start = 0.1\n",
    "    n_train = 50\n",
    "    iteration = int(iteration)\n",
    "    d_1, d_2, x = generate_data(mu, -mu, n_examples = n_train)\n",
    "    #plot_data(d_1, d_2)\n",
    "    mu_k, sigma_k, diff_mu_k, mus_k = kmeans(x, mu_start, mu, iterations=50)\n",
    "\n",
    "    plot_data_and_distr(d_1, d_2, mus_k[iteration], -mus_k[iteration], title=\"Estimated distribution using K-means at iter \"+str(iteration))\n",
    "\n",
    "    print(\"True mean:{:.3f}, EM (final) estimate:{:.3f}\".format(mu, mu_k))\n",
    "def generate_iter_widget():\n",
    "    return widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=9,\n",
    "        step=1,\n",
    "        description='Iteration',\n",
    "        continuous_update= False)\n",
    "def visualize(iteration):\n",
    "    process_em(iteration)\n",
    "def visualizek(iteration):\n",
    "    process_kmeans(iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following one-dimensional mixture model:\n",
    "\\begin{align}\n",
    "  Z &\\sim \\text{Bernoulli}(0.5)\\\\\n",
    "  X\\vert Z=0 &\\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad \\text{and}\\\\\n",
    "  X\\vert Z=1 &\\sim \\mathcal{N}(-\\mu, \\sigma^2),\n",
    "\\end{align}\n",
    "i.e., $Z$ denotes the label of the Gaussian distribution from which $X$ is drawn.\n",
    "In other words, we have an equal weighted 2-mixture (since $Z$ takes value\n",
    "$0$ or $1$ with probability $0.5$ each) of Gaussians, where the variances and means for\n",
    "both mixtures are unknown.\n",
    "\n",
    "For simplicity, we set $\\sigma=0.5$\n",
    "\n",
    "***i) K-means***\n",
    "\n",
    "We have implemented this part for you. Since two clusters are centered at $\\mu$ and $-\\mu$, K-means can be considered using following two steps:\n",
    "\n",
    "    E-step: Assign each data point to cluster 0 (if it is closer to $\\mu$) or cluster 1 (if it is closer to $-\\mu$).\n",
    "\n",
    "    M-step: Update $\\mu_0$ and $\\mu_1$ based on current data points in the cluster.\n",
    "\n",
    "Note that in M-step, we set $\\mu=\\frac{|\\mu_0 - \\mu_1|}{2}$ for the specific distribution of this problem.\n",
    "\n",
    "Please take a look at the implementation and understand the procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(x, mu, mu_true, sigma = 0.5, iterations = 200):\n",
    "    # Run the K means algorithm to find the estimated mean of the dataset\n",
    "    # Note: the original dataset comes from 2 distributions centered at mu and -mu, which this takes into account\n",
    "    #     with each update\n",
    "    diff_mu = np.zeros(iterations)\n",
    "    mus = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        mu_1 = mu\n",
    "        mu_2 = -mu\n",
    "        set1 = []\n",
    "        set2 = []\n",
    "        #### This is like the E_step ####\n",
    "        for x_i in x:\n",
    "            if np.abs(x_i - mu_1) <= np.abs(x_i - mu_2):\n",
    "                set1.append(x_i)\n",
    "            else:\n",
    "                set2.append(x_i)\n",
    "        #### This is like the M_step ####\n",
    "        mu_0_new = np.mean(set1)\n",
    "        mu_1_new = np.mean(set2)\n",
    "        # Estimates two means and combines them to get mu for the next iteration\n",
    "        mu = np.abs(len(set1)*mu_0_new - len(set2)*mu_1_new)/(len(set1)+len(set2))\n",
    "        diff_mu[i]  = np.abs(mu-mu_true)\n",
    "        mus[i] = mu\n",
    "\n",
    "    return mu, sigma, diff_mu, mus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run K-means!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(visualizek,iteration=generate_iter_widget())\n",
    "interactive_plot\n",
    "# start with the initial condition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret the plots?\n",
    "- Different colors correspond to ground-truth labels. Blue -> cluster 0; red -> cluster 1.\n",
    "- Upper plot: black cross: current estimated $\\mu$ (and $-\\mu$). Current estimated distributions are also plotted.\n",
    "- Lower plot: X-axis: value of each 1D point; Y-axis: weight contributed to cluster 0.\n",
    "\n",
    "Pay attention to points near the boundary. Is the \"hard assignment\" good for this example?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ii) EM***\n",
    "\n",
    "EM algorithm can be understood as a softer version of K-means\n",
    "\n",
    "    E-step: For each data point x_i, instead of assigning a label, use a soft version: compute the probablity of each label.\n",
    "$$\\phi^0_i=\\frac{1}{\\sigma \\sqrt(2\\pi)}\\exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2})\\propto\\exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2})$$\n",
    "$$\\phi^1_i=\\frac{1}{\\sigma \\sqrt(2\\pi)}\\exp(-\\frac{(x_i+\\mu)^2}{2\\sigma^2})\\propto \\exp(-\\frac{(x_i+\\mu)^2}{2\\sigma^2})$$\n",
    "Using Bayes' rule for the weight contributed to cluster 0.\n",
    "$$w_i=\\frac{\\phi^0_i-\\phi^1_i}{\\phi^0_i+\\phi^1_i}$$\n",
    "<em>Note: There is some additional algebra. You will derive it in part j, problem 2 of HW10. It turns out to be of this form.</em>\n",
    "    \n",
    "    M-step: We can estimate parameter $\\mu$ (for cluster 0) based on the ``soft'' probability.\n",
    " $$\\mu = \\sum_i \\frac{w_i x_i}{N} $$ where $N$ is the number of data points. Similarly, use $-\\mu$ as the mean for cluster 1.\n",
    "\n",
    " <em> Note: when implementing $\\phi$, use what's after $\\propto$ as the constant will be cancelled out. <em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em(x, mu, mu_true, sigma = 0.5, iterations = 200):\n",
    "    # Run the EM algorithm to find the estimated mean of the dataset\n",
    "    # Note: the original dataset comes from 2 distributions centered at mu and -mu, which this takes into account\n",
    "    #     with each update\n",
    "    diff_mu = np.zeros(iterations)\n",
    "    mus = np.zeros(iterations)\n",
    "    phis = []\n",
    "    for i in range(iterations):\n",
    "\n",
    "        phi_0 = None\n",
    "        phi_1 = None\n",
    "        w = None\n",
    "\n",
    "        # Hint: use np.exp(-np.square(x-mu)/(2*np.square(sigma)))\n",
    "        ### start E_step ###\n",
    "\n",
    "        ### end E_step ###\n",
    "\n",
    "        mu = None\n",
    "        # Hint: use (1/len(x))*np.sum(w*x)\n",
    "        ### start M_step ###\n",
    "\n",
    "        ### end M_step ###\n",
    "        diff_mu[i]  = np.abs(mu-mu_true)\n",
    "        mus[i] = mu\n",
    "\n",
    "        phis.append((phi_0-phi_1)/(phi_0+phi_1))\n",
    "    phis = np.array(phis)\n",
    "\n",
    "    return mu, sigma,diff_mu, mus, phis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how learned distributions ($\\mu$ from M-step) and label assigment (from E-step) evolves for EM and K-means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(visualize,iteration=generate_iter_widget())\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Which algorithm recovers more accurate mean? What may cause the difference?**\n",
    "2. **Look at different iteration and understand how E-step and M-step work.**\n",
    "3. **Understand soft assignment by comparing weight plots.**\n",
    "4. **Change $\\mu$ (mu = XX in process_em and process_kmeans function) and random seed (rand_seed = XX in process_em and process_kmeans function) and observe the performance.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go back to the pdf worksheet for a more theoretical understanding of EM algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
