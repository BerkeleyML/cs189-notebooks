{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence Visualization\n",
    "\n",
    "In this notebook, we will run some simple simulations to verify your results.\n",
    "\n",
    "In particular, we will consider the coin-toss example, using weighted coins.\n",
    "\n",
    "Let's first set up some utility functions, and generate the true weights of our coins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_head = 0.60 + np.random.random() * 0.15\n",
    "print(\"True P(head) = {}\".format(p_head))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_widget(title):\n",
    "    return widgets.FloatSlider(\n",
    "        value=p_head,\n",
    "        min=0.01,\n",
    "        max=0.99,\n",
    "        step=0.01,\n",
    "        description=title,\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='f')\n",
    "\n",
    "\n",
    "def toss(num_coins, p=None):\n",
    "    p = p or p_head\n",
    "    return np.where(np.random.random(num_coins) > p, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up our utility functions, let's look at the distribution of our coin tosses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10000\n",
    "n = 100\n",
    "\n",
    "data = np.array([sum(toss(n)) for _ in range(num_trials)])\n",
    "\n",
    "plt.hist(data, bins=range(n))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice binomial distribution centered at $p_{head} n$, as we'd expect.\n",
    "\n",
    "## Part (a)\n",
    "Let's now look at our distribution from part (a). First, we will plot the number of possible ways to achieve a certain number of heads, out of 100 total coin tosses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return sum(p * np.log(1 / p))\n",
    "\n",
    "def log_num_possibilities(n, num_heads):\n",
    "    f_n = np.array([n - num_heads, num_heads])\n",
    "    return n * entropy(f_n / n)\n",
    "\n",
    "def num_possibilities(n, num_heads):\n",
    "    return np.exp(log_num_possibilities(n, num_heads))\n",
    "\n",
    "xs = range(1, 100)\n",
    "plt.plot(xs, [num_possibilities(n, num_heads) for num_heads in xs])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the vertical scale and focusing only on its relative magnitude, does this distribution match the histogram of the coin toss samples? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### begin (a) ###\n",
    "\n",
    "### end (a) ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "\n",
    "We will now compute and plot the probability of our observations (generated using `toss`) working under the hypothesis that the coin is fair (equally likely to be heads or tails). Note that we actually plot the log of our probability, to reduce numerical error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_p_empirical_type(n, num_heads, p_guess):\n",
    "    return log_num_possibilities(n, num_heads) + num_heads * np.log(p_guess) + (n - num_heads) * np.log(1 - p_guess)\n",
    "\n",
    "def p_empirical_type(n, num_heads, p_guess):\n",
    "    return np.exp(log_p_empirical_type(n, num_heads, p_guess))\n",
    "\n",
    "def plot_empirical_prob(p, p_guess):\n",
    "    candidate_ns = range(100, 1000)\n",
    "    p_observations = []\n",
    "    for n in candidate_ns:\n",
    "        num_heads = sum(toss(n, p))\n",
    "        p_observations.append(log_p_empirical_type(n, num_heads, p_guess))\n",
    "\n",
    "    plt.plot(candidate_ns, p_observations)\n",
    "\n",
    "p = prob_widget(\"True probability\")\n",
    "p_guess = prob_widget(\"Hypothesis probability\")\n",
    "interactive(plot_empirical_prob, p=p, p_guess=p_guess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on how the slope of the above plot varies with the true and hypothesized $p_{head}$. (be aware that the y-axis scale will change as you drag the slider!) When is it steepest? When is it flattest? How does this relate to your observations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### begin (b) ###\n",
    "\n",
    "### end (b) ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot normalizes the above probability by $1/n$. We return to the hypothesis of a fair coin and the true distribution that we chose at the beginning of the notebook, ignoring the values chosen with the sliders above.\n",
    "\n",
    "As you proved in the theory section, this quantity should converge to the (negation of the) KL divergence of the empirical model from the true one. Look at the plot and see whether it converges to the value we expect (computed below). Does it do so? (no response necessary, just observe the plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ns = range(100, 10000)\n",
    "p_observations = []\n",
    "for n in candidate_ns:\n",
    "    num_heads = sum(toss(n))\n",
    "    p_observations.append(log_p_empirical_type(n, num_heads, 0.5) / n)\n",
    "\n",
    "plt.plot(candidate_ns, p_observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(p, q):\n",
    "    return sum(p * np.log(p / q))\n",
    "\n",
    "f = np.array([1 - p_head, p_head]) # true distribution\n",
    "p = np.array([0.5, 0.5]) # hypothesis distribution\n",
    "\n",
    "kl = compute_kl(f, p)\n",
    "print(\"KL divergence: {}\".format(kl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
