{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "yT-0ntjx4BPI"
            },
            "source": [
                "# Homework: Few-Shot Learning via Auxiliary Labels\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gt7kWsEmu6em"
            },
            "source": [
                "Import necessary Python packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sklearn\n",
                "from sklearn.decomposition import PCA\n",
                "from matplotlib import pyplot as plt\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_squared_error\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uQ54NfyMuEoW"
            },
            "source": [
                "Let's firs generate the data. Note that we only generate it once for fair comparison of different methods in later parts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(0)\n",
                "\n",
                "# Define number of points for training and validatiion\n",
                "N_train = 1000\n",
                "N_val = 5000\n",
                "\n",
                "# Define dimensions of true latent variable and irrelavant features\n",
                "dim_t = 2\n",
                "dim_f = 20\n",
                "\n",
                "# Define dimension of data X\n",
                "dim_d = 200\n",
                "\n",
                "# Define noise level\n",
                "sigma_n = 0.001\n",
                "sigma_z = 1\n",
                "sigma_y = 1\n",
                "sigma_f = np.sqrt(0.5*0.75)\n",
                "\n",
                "# Define hyperparameter for the ridge regression\n",
                "ridge_lambda = 0.05\n",
                "\n",
                "def generate_T(N):\n",
                "    theta = np.random.uniform(low=0.0, high=2*np.pi, size=N)\n",
                "    L = np.vstack((0.5 * np.sin(theta), np.cos(theta))).transpose()\n",
                "    return L@orth_basis(2,2)\n",
                "#     return np.sign(np.random.rand(N, dim_t)) * 0.2\n",
                "\n",
                "\n",
                "def generate_f(N, dim_f=2):\n",
                "    F = np.random.normal(0, sigma_f, (N, dim_f))\n",
                "    return F\n",
                "\n",
                "def orth_basis(dim, dim_t):\n",
                "    ## This function creates orthogonal basis from random projection\n",
                "    random_state = np.random\n",
                "    H = np.eye(dim)\n",
                "    D = np.ones((dim,))\n",
                "    for n in range(1, dim):\n",
                "        x = random_state.normal(size=(dim-n+1,))\n",
                "        D[n-1] = np.sign(x[0])\n",
                "        x[0] -= D[n-1]*np.sqrt((x*x).sum())\n",
                "        # Householder transformation\n",
                "        Hx = (np.eye(dim-n+1) - 2.*np.outer(x, x)/(x*x).sum())\n",
                "        mat = np.eye(dim)\n",
                "        mat[n-1:, n-1:] = Hx\n",
                "        H = np.dot(H, mat)\n",
                "        # Fix the last sign such that the determinant is 1\n",
                "    D[-1] = (-1)**(1-(dim % 2))*D.prod()\n",
                "    # Equivalent to np.dot(np.diag(D), H) but faster, apparently\n",
                "    H = (D*H.T).T\n",
                "    return H[:, :dim_t]\n",
                "\n",
                "\n",
                "# Generate latent variable\n",
                "T = generate_T(N_train+N_val)\n",
                "# Generate irrelevant features\n",
                "F = generate_f(N_train+N_val, dim_f)\n",
                "\n",
                "# Generate data X\n",
                "noise = np.random.normal(0, sigma_n, (N_train+N_val, dim_d))\n",
                "V = orth_basis(dim_d, dim_t+dim_f)\n",
                "X = np.hstack((T, F))@np.transpose(V) + noise\n",
                "\n",
                "# Generate output y\n",
                "theta = np.random.rand(dim_t, 1)\n",
                "y = T@theta + np.random.normal(0, sigma_y, (N_train+N_val, 1))\n",
                "\n",
                "# Whitening the data\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(X)\n",
                "X_w = scaler.transform(X)\n",
                "\n",
                "# Split train/val\n",
                "X_val = X_w[-N_val:, :]\n",
                "y_val = y[-N_val:, :]\n",
                "X_train = X_w[:N_train, :]\n",
                "y_train = y[:N_train, :]\n",
                "T_train = T[:N_train, :]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "GEqVZSbhwAiG"
            },
            "source": [
                "Now let's generate auxiliary labels.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_Z(dim_k):\n",
                "    q = np.random.randn(dim_t, dim_k)\n",
                "    Z_train = T_train@q  + np.random.normal(0, sigma_z, (N_train, dim_k))\n",
                "\n",
                "    Z_clean = T_train@q\n",
                "    return q, Z_train, Z_clean\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DFhP3Q9i3cA3"
            },
            "source": [
                "Below are functions for different methods. You will need to implement missing parts for truncated SVD and solving $\\theta$.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tSVD(X, Z):\n",
                "    pass\n",
                "    ## TODO: implement the truncated SVD as part (b)\n",
                "    ## You are allowed to use np.linalg packages if necessary\n",
                "    ### start 1 ###\n",
                "\n",
                "    ### end 1 ###\n",
                "\n",
                "def fit_PCA(X, dim_t):\n",
                "    # pca = PCA(n_components=dim_l, whiten=True)\n",
                "    pca = PCA(n_components=dim_t)\n",
                "    pca.fit(X)\n",
                "    T_hat = pca.fit_transform(X)\n",
                "    return T_hat, pca.components_\n",
                "\n",
                "def solve_theta(T_hat, y):\n",
                "    pass\n",
                "    ## TODO: implement the function to solve \\theta from T and y\n",
                "    ## You can refer to part (c) if necessary\n",
                "    ### start 2 ###\n",
                "\n",
                "    ### end 2 ###\n",
                "\n",
                "def random_project_data(X, dim_t):\n",
                "    dim_d = X.shape[1]\n",
                "    W = orth_basis(dim_d, dim_t)\n",
                "    return X@W\n",
                "\n",
                "def loss_eval(w):\n",
                "    return mean_squared_error(X_val@w, y_val)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "N8qQNY4u38BH"
            },
            "source": [
                "Now let's run the code!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def vis_latent(L, X_hat, L_hat, L_rand, L_pca):\n",
                "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5)\n",
                "    fig.set_size_inches(26.5, 6.5)\n",
                "    fig.suptitle('Latent Variable: Clean Ground-Truth (1st) vs Noisy Ground-Truth (2nd) vs SVD Recovered (3rd) vs PCA Recovered (4th) vs Random Projection (5th)', fontsize=25)\n",
                "    ax1.plot(L[:, 0], L[:,1], \".\")\n",
                "    ax2.plot(X_hat[:, 0], X_hat[:,1], \".\")\n",
                "    ax3.plot(L_hat[:, 0], L_hat[:,1], \".\")\n",
                "    ax4.plot(L_pca[:, 0], L_pca[:,1], \".\")\n",
                "    ax5.plot(L_rand[:, 0], L_rand[:,1], \".\")\n",
                "\n",
                "def gen_aux_data_and_fit_model(N, sigma_n, dim_d = 50, dim_t_local = dim_t, dim_k=100, VIS = False):\n",
                "    q, Z, Z_clean = generate_Z(dim_k)\n",
                "\n",
                "    # random projection\n",
                "    T_rand = random_project_data(X_train, dim_t_local)\n",
                "\n",
                "    # PCA projection\n",
                "    T_pca, U_pca = fit_PCA(X_train, dim_t_local)\n",
                "    U_pca = U_pca.transpose()\n",
                "    theta_baseline = solve_theta(T_pca, y_train)\n",
                "    weight_baseline = U_pca@theta_baseline\n",
                "\n",
                "    # SVD\n",
                "    U = tSVD(X_train, Z)\n",
                "    T_hat = X_train@U\n",
                "    theta_hat = solve_theta(T_hat, y_train)\n",
                "    weight_ours = U@theta_hat\n",
                "\n",
                "    # baseline (access to clean Z)\n",
                "    U_clean = tSVD(X_train, Z_clean)\n",
                "    T_clean_hat = X_train@U_clean\n",
                "    theta_clean_hat = solve_theta(T_clean_hat, y_train)\n",
                "    weight_clean_ours = U_clean@theta_clean_hat\n",
                "\n",
                "    # perfect baseline\n",
                "    T_best = X_train@V[:, :dim_t]\n",
                "    theta_best = solve_theta(T_best, y_train)\n",
                "    weight_best = V[:, :dim_t]@theta_best\n",
                "\n",
                "    if VIS:\n",
                "        vis_latent(T, T_best, T_hat, T_rand, T_pca)\n",
                "\n",
                "    return loss_eval(weight_baseline), loss_eval(weight_ours), loss_eval(weight_best), loss_eval(weight_clean_ours)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tB_7n7ty4BPV"
            },
            "source": [
                "How does truncated SVD recover the latent variable $T$? Let's visualize the latent space.\n",
                "\n",
                "**Make sure you include this plot in your solution.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "_, _, _, _ = gen_aux_data_and_fit_model(N_train, sigma_n, dim_d, dim_t_local = 2, dim_k=40, VIS = True)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "byWId9PW6rv_"
            },
            "source": [
                "Let's change the dimension of auxiliary label $Z$ and observe the performance of different methods. Run this a few times so that you can get a sense of the variability of this approach as well.\n",
                "\n",
                "**Make sure you include this plot in your solution.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "diff_baselines, diff_ourss, diff_bests, diff_clean_ourss = [], [], [], []\n",
                "dim_ks = np.arange(1, 40, 1)\n",
                "for dim_k in dim_ks:\n",
                "    diff_baseline, diff_ours, diff_best, diff_clean_ours = gen_aux_data_and_fit_model(N_train, sigma_n, dim_d, dim_t_local = 5, dim_k=dim_k, VIS = False)\n",
                "    diff_baselines.append(diff_baseline)\n",
                "    diff_bests.append(diff_best)\n",
                "    diff_ourss.append(diff_ours)\n",
                "    diff_clean_ourss.append(diff_clean_ours)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(dim_ks, diff_baselines, '-.', label=\"PCA baseline\")\n",
                "plt.plot(dim_ks, diff_bests, '--.', label=\"best (access to V_{ux})\")\n",
                "plt.plot(dim_ks, diff_clean_ourss, '--.', label=\"baseline (access to clean aux labels)\")\n",
                "plt.plot(dim_ks, diff_ourss,label=\"ours\")\n",
                "plt.legend()\n",
                "plt.title(\"Coefficient Reconstruction Difference vs # Dimension of Z\")\n",
                "plt.xlabel(\"$k$\")\n",
                "plt.xticks(np.arange(1, 40, 2))\n",
                "plt.ylabel(\"MSE Loss (Validation)\")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "collapsed_sections": [],
            "name": "few-shot-learning-problem.ipynb",
            "provenance": [
                {
                    "file_id": "1ZEvGgTUrilQnh5YhiU5Uqce3ZDZbr1ls",
                    "timestamp": 1602692357536
                }
            ]
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}