{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [HW8] Problem: Backpropagation Algorithm for Neural Networks\n",
    "\n",
    "Import necessary Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mnist in /Users/peterwang/miniconda3/lib/python3.6/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /Users/peterwang/miniconda3/lib/python3.6/site-packages (from mnist) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/peterwang/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install mnist\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function used to compute prediction errors\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    # Compute the squared error between the prediction yp and the observation y\n",
    "    # This method should compute the cost per element such that the output is the\n",
    "    # same shape as y and yp\n",
    "    @staticmethod\n",
    "    def fx(y,yp):\n",
    "        # TODO: PART B #########################################################################\n",
    "        # Implement me\n",
    "        # ######################################################################################\n",
    "        ### start b1 ###\n",
    "\n",
    "        ### end b1 ###\n",
    "        pass\n",
    "\n",
    "    # Derivative of the cost function with respect to yp\n",
    "    @staticmethod\n",
    "    def dx(y,yp):\n",
    "        # TODO: PART B #########################################################################\n",
    "        # Implement me\n",
    "        # ######################################################################################\n",
    "        ### start b2 ###\n",
    "\n",
    "        ### end b2 ###\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function fully implemented as an example\n",
    "class SigmoidActivation(object):\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        return SigmoidActivation.fx(z) * (1 - SigmoidActivation.fx(z))\n",
    "\n",
    "# Hyperbolic tangent function\n",
    "class TanhActivation(object):\n",
    "\n",
    "    # Compute tanh for each element in the input z\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        # TODO: PART C #################################################################################\n",
    "        # Implement me\n",
    "        # ######################################################################################\n",
    "        ### start c1 ###\n",
    "\n",
    "        ### end c1 ###\n",
    "        pass\n",
    "\n",
    "    # Compute the derivative of the tanh function with respect to z\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        # TODO: PART C #################################################################################\n",
    "        # Implement me\n",
    "        # ######################################################################################\n",
    "        ### start c2 ###\n",
    "\n",
    "        ### end c2 ###\n",
    "        pass\n",
    "\n",
    "# Rectified linear unit\n",
    "class ReLUActivation(object):\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        # TODO: PART C #################################################################################\n",
    "        # Implement me\n",
    "        # ######################################################################################\n",
    "        ### start c3 ###\n",
    "\n",
    "        ### end c3 ###\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        # TODO: PART C #################################################################################\n",
    "        # Implement me\n",
    "        # ######################################################################################\n",
    "        ### start c4 ###\n",
    "\n",
    "        ### end c4 ###\n",
    "        pass\n",
    "\n",
    "# Linear activation\n",
    "class LinearActivation(object):\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        return np.ones(z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class represents a single hidden or output layer in the neural network\n",
    "class DenseLayer(object):\n",
    "\n",
    "    # numNodes: number of hidden units in the layer\n",
    "    # activation: the activation function to use in this layer\n",
    "    def __init__(self, numNodes, activation):\n",
    "        self.numNodes = numNodes\n",
    "        self.activation = activation\n",
    "\n",
    "    def getNumNodes(self):\n",
    "        return self.numNodes\n",
    "\n",
    "    # Initialize the weight matrix of this layer based on the size of the matrix W\n",
    "    def initialize(self, fanIn, scale=1.0):\n",
    "        s = scale * np.sqrt(6.0 / (self.numNodes + fanIn))\n",
    "        self.W = np.random.normal(0, s,\n",
    "                                   (self.numNodes,fanIn))\n",
    "        #self.b = np.zeros((self.numNodes,1))\n",
    "        self.b = np.random.uniform(-1,1,(self.numNodes,1))\n",
    "\n",
    "    # Apply the activation function of the layer on the input z\n",
    "    def a(self, z):\n",
    "        return self.activation.fx(z)\n",
    "\n",
    "    # Compute the linear part of the layer\n",
    "    # The input a is an n x k matrix where n is the number of samples\n",
    "    # and k is the dimension of the previous layer (or the input to the network)\n",
    "    def z(self, a):\n",
    "        #print('a:\\n'+str(a))\n",
    "        #print('Wa:\\n'+str(self.W.dot(a)))\n",
    "        return self.W.dot(a) + self.b # Note, this is implemented where we assume a is k x n\n",
    "\n",
    "    # Compute the derivative of the layer's activation function with respect to z\n",
    "    # where z is the output of the above function.\n",
    "    # This derivative does not contain the derivative of the matrix multiplication\n",
    "    # in the layer.  That part is computed below in the model class.\n",
    "    def dx(self, z):\n",
    "        return self.activation.dx(z)\n",
    "\n",
    "    # Update the weights of the layer by adding dW to the weights\n",
    "    def updateWeights(self, dW):\n",
    "        self.W = self.W + dW\n",
    "\n",
    "    # Update the bias of the layer by adding db to the bias\n",
    "    def updateBias(self, db):\n",
    "        self.b = self.b + db\n",
    "\n",
    "# This class handles stacking layers together to form the completed neural network\n",
    "class Model(object):\n",
    "\n",
    "    # inputSize: the dimension of the inputs that go into the network\n",
    "    def __init__(self, inputSize):\n",
    "        self.layers = []\n",
    "        self.inputSize = inputSize\n",
    "\n",
    "    # Add a layer to the end of the network\n",
    "    def addLayer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Get the output size of the layer at the given index\n",
    "    def getLayerSize(self, index):\n",
    "        if index >= len(self.layers):\n",
    "            return self.layers[-1].getNumNodes()\n",
    "        elif index < 0:\n",
    "            return self.inputSize\n",
    "        else:\n",
    "            return self.layers[index].getNumNodes()\n",
    "\n",
    "    # Initialize the weights of all of the layers in the network and set the cost\n",
    "    # function to use for optimization\n",
    "    def initialize(self, cost, initializeLayers=True):\n",
    "        self.cost = cost\n",
    "        if initializeLayers:\n",
    "            for i in range(0,len(self.layers)):\n",
    "                if i == len(self.layers) - 1:\n",
    "                    self.layers[i].initialize(self.getLayerSize(i-1))\n",
    "                else:\n",
    "                    self.layers[i].initialize(self.getLayerSize(i-1))\n",
    "\n",
    "    # Compute the output of the network given some input a\n",
    "    # The matrix a has shape n x k where n is the number of samples and\n",
    "    # k is the dimension\n",
    "    # This function returns\n",
    "    # yp - the output of the network\n",
    "    # a - a list of inputs for each layer of the newtork where\n",
    "    #     a[i] is the input to layer i\n",
    "    #     (note this does not include the network output!)\n",
    "    # z - a list of values for each layer after evaluating layer.z(a) but\n",
    "    #     before evaluating the nonlinear function for the layer\n",
    "    def evaluate(self, x):\n",
    "        curA = x.T\n",
    "        a = [curA]\n",
    "        z = []\n",
    "        for layer in self.layers:\n",
    "            z.append(layer.z(curA))\n",
    "            curA = layer.a(z[-1])\n",
    "            a.append(curA)\n",
    "        yp = a.pop()\n",
    "        return yp, a, z\n",
    "\n",
    "    # Compute the output of the network given some input a\n",
    "    # The matrix a has shape n x k where n is the number of samples and\n",
    "    # k is the dimension\n",
    "    def predict(self, a):\n",
    "        a,_,_ = self.evaluate(a)\n",
    "        return a.T\n",
    "\n",
    "    # Computes the gradients at each layer. y is the true labels, yp is the\n",
    "    # predicted labels, and z is a list of the intermediate values in each\n",
    "    # layer. Returns the gradients and the forward pass outputs (per layer).\n",
    "    #\n",
    "    # In particular, we compute dMSE/dz_i. The reasoning behind this is that\n",
    "    # in the update function for the optimizer, we do not give it the z values\n",
    "    # we compute from evaluating the network.\n",
    "    def compute_grad(self, x, y):\n",
    "        # Feed forward, computing outputs of each layer and\n",
    "        # intermediate outputs before the non-linearities\n",
    "        yp, a, z = self.evaluate(x)\n",
    "\n",
    "        # Compute the error\n",
    "        d = self.cost.dx(y.T, yp)\n",
    "        grad = []\n",
    "\n",
    "        # Backpropogate the error\n",
    "        idx = len(self.layers)\n",
    "        for i, (layer, curZ) in enumerate(zip(reversed(self.layers),reversed(z))):\n",
    "            # TODO: PART D #########################################################################\n",
    "            # Compute the gradient of the output of each layer with respect to the error\n",
    "            # grad[i] should correspond with the gradient of the output of layer i\n",
    "            #   before the activation is applied (dMSE / dz_i); be sure values are stored\n",
    "            #   in the correct ordering!\n",
    "            # ######################################################################################\n",
    "            pass\n",
    "            ### start d1 ###\n",
    "\n",
    "            ### end d1 ###\n",
    "\n",
    "        return grad, a\n",
    "\n",
    "\n",
    "    # Train the network given the inputs x and the corresponding observations y\n",
    "    # The network should be trained for numEpochs iterations using the supplied\n",
    "    # optimizer\n",
    "    def train(self, x, y, numEpochs, optimizer):\n",
    "\n",
    "        # Initialize some stuff\n",
    "        n = x.shape[0]\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "        hist = []\n",
    "        optimizer.initialize(self.layers)\n",
    "\n",
    "        # Run for the specified number of epochs\n",
    "        for epoch in range(0,numEpochs):\n",
    "\n",
    "            # Compute the forward pass and gradients\n",
    "            grad, a = self.compute_grad(x, y)\n",
    "\n",
    "            # Update the network weights\n",
    "            optimizer.update(self.layers, grad, a)\n",
    "\n",
    "            # Compute the error at the end of the epoch\n",
    "            yh = self.predict(x)\n",
    "            C = self.cost.fx(y, yh)\n",
    "            C = np.mean(C)\n",
    "            hist.append(C)\n",
    "        return hist\n",
    "\n",
    "    def trainBatch(self, x, y, batchSize, numEpochs, optimizer):\n",
    "\n",
    "        # Copy the data so that we don't affect the original one when shuffling\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "        hist = []\n",
    "        n = x.shape[0]\n",
    "\n",
    "        for epoch in np.arange(0,numEpochs):\n",
    "\n",
    "            # Shuffle the data\n",
    "            r = np.arange(0,x.shape[0])\n",
    "            x = x[r,:]\n",
    "            y = y[r,:]\n",
    "            e = []\n",
    "\n",
    "            # Split the data in chunks and run SGD\n",
    "            for i in range(0,n,batchSize):\n",
    "                end = min(i+batchSize,n)\n",
    "                batchX = x[i:end,:]\n",
    "                batchY = y[i:end,:]\n",
    "                e += self.train(batchX, batchY, 1, optimizer)\n",
    "            hist.append(np.mean(e))\n",
    "\n",
    "        return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent optimization\n",
    "# The learning rate is specified by eta\n",
    "class GDOptimizer(object):\n",
    "    def __init__(self, eta):\n",
    "        self.eta = eta\n",
    "\n",
    "    def initialize(self, layers):\n",
    "        pass\n",
    "\n",
    "    # This function performs one gradient descent step\n",
    "    # layers is a list of dense layers in the network\n",
    "    # g is a list of gradients going into each layer before the nonlinear activation\n",
    "    # a is a list of of the activations of each node in the previous layer going\n",
    "    def update(self, layers, g, a):\n",
    "        m = a[0].shape[1]\n",
    "        for layer, curGrad, curA in zip(layers, g, a):\n",
    "            pass\n",
    "            # TODO: PART F #########################################################################\n",
    "            # Compute the gradients for layer.W and layer.b using the gradient for the output of the\n",
    "            # layer curA and the gradient of the output curGrad\n",
    "            # Use the gradients to update the weight and the bias for the layer\n",
    "            #\n",
    "            # Normalize the learning rate by m (defined above), the number of training examples input\n",
    "            # (in parallel) to the network.\n",
    "            #\n",
    "            # It may help to think about how you would calculate the update if we input just one\n",
    "            # training example at a time; then compute a mean over these individual update values.\n",
    "            # ######################################################################################\n",
    "\n",
    "            ### start f1 ###\n",
    "\n",
    "            ### end f1 ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e) - Part (h)\n",
    "Note this is the code to run the network. You do not need to implement part (f) to run part (e).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # switch these statements to True to run the code for the corresponding parts\n",
    "    # Part H - switch digits here\n",
    "    N0 = 2\n",
    "    N1 = 8\n",
    "    # Part E\n",
    "    RIDGE = True\n",
    "    # Part F\n",
    "    SGD = True\n",
    "    # Part G\n",
    "    DIFF_SIZES = True\n",
    "\n",
    "\n",
    "    # Data loading code\n",
    "    y_train = mnist.train_labels()\n",
    "    y_test = mnist.test_labels()\n",
    "    X_train = (mnist.train_images()/255.0)\n",
    "    X_test = (mnist.test_images()/255.0)\n",
    "    train_idxs = np.logical_or(y_train == N0, y_train == N1)\n",
    "    test_idxs = np.logical_or(y_test == N0, y_test == N1)\n",
    "    y_train = y_train[train_idxs].astype('int')\n",
    "    y_test = y_test[test_idxs].astype('int')\n",
    "    X_train = X_train[train_idxs]\n",
    "    X_test = X_test[test_idxs]\n",
    "    y_train = (y_train == N1).astype('int')\n",
    "    y_test = (y_test == N1).astype('int')\n",
    "    y_train *= 2\n",
    "    y_test *= 2\n",
    "    y_train -= 1\n",
    "    y_test -= 1\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    y = y_train[:, np.newaxis]\n",
    "    x = X_train\n",
    "    yHats = {}\n",
    "    activations = dict(ReLU=ReLUActivation,\n",
    "                       tanh=TanhActivation,\n",
    "                       linear=LinearActivation)\n",
    "    lr = dict(ReLU=0.02,tanh=0.02,linear=0.005)\n",
    "    names = ['ReLU','linear','tanh']\n",
    "    print(\"training classifier between digits {0} and {1}\".format(N0, N1))\n",
    "\n",
    "    # Perform ridge regression on the last layer of the network\n",
    "    #### PART E ####\n",
    "    if RIDGE:\n",
    "        print('\\n----------------------------------------\\n')\n",
    "        print('Running ridge regression on last layer')\n",
    "        from sklearn.linear_model import Ridge\n",
    "        errors = {}\n",
    "        widths = [16, 128,256, 1024]\n",
    "        results = []\n",
    "        for key in names:\n",
    "            error = []\n",
    "            for width in widths:\n",
    "                activation = activations[key]\n",
    "                model = Model(x.shape[1])\n",
    "                model.addLayer(DenseLayer(width,activation()))\n",
    "                model.initialize(QuadraticCost())\n",
    "                ridge = Ridge(alpha=0.1)\n",
    "                X = model.predict(x)\n",
    "                ridge.fit(X,y)\n",
    "                y_hat_train = ridge.predict(model.predict(X_train))\n",
    "                y_hat_test = ridge.predict(model.predict(X_test))\n",
    "                y_pred_train = np.sign(y_hat_train)\n",
    "                y_pred_test = np.sign(y_hat_test)\n",
    "                error_train = np.mean(np.square(y_hat_train - y))/2\n",
    "                error_test = np.mean(np.square(y_hat_test - y_test))/2\n",
    "                errors[key] = error_train\n",
    "\n",
    "                mce_error_train = (y_pred_train.ravel() != y_train).sum()/len(y_train)\n",
    "                mce_error_test = (y_pred_test.ravel() != y_test).sum()/len(y_test)\n",
    "                result = {}\n",
    "                result['width'] = width\n",
    "                result['train MSE'] = error_train\n",
    "                result['test MSE'] = error_test\n",
    "                result['train MCE'] = mce_error_train\n",
    "                result['test MCE'] = mce_error_test\n",
    "                result['nonlinearity'] = key\n",
    "                results.append(result)\n",
    "        print(pd.DataFrame(results))\n",
    "\n",
    "    #### PART F ####\n",
    "    if SGD:\n",
    "        print('\\n----------------------------------------\\n')\n",
    "        print('Using SGD')\n",
    "        batchSizes = [1, 128, x.shape[0]]\n",
    "        results = []\n",
    "        for key in names:\n",
    "            for batchSize in batchSizes:\n",
    "                # Build the model\n",
    "                activation = activations[key]\n",
    "                model = Model(x.shape[1])\n",
    "                width = 4\n",
    "                model.addLayer(DenseLayer(width,activation()))\n",
    "                model.addLayer(DenseLayer(1,LinearActivation()))\n",
    "                model.initialize(QuadraticCost())\n",
    "                # Train the model and display the results\n",
    "                epochs = 10  # Make sure that the same number of gradients steps are taken\n",
    "                hist = model.trainBatch(x,y,1,epochs,GDOptimizer(eta=lr[key]))\n",
    "                y_hat_train = model.predict(x)\n",
    "                y_hat_test = model.predict(X_test)\n",
    "\n",
    "                y_pred_train = np.sign(y_hat_train)\n",
    "                y_pred_test = np.sign(y_hat_test)\n",
    "\n",
    "                error_train = np.mean(np.square(y_hat_train - y))/2\n",
    "                error_test = np.mean(np.square(y_hat_test - y_test))/2\n",
    "\n",
    "                mce_error_train = (y_pred_train.ravel() != y_train).sum()/len(y_train)\n",
    "                mce_error_test = (y_pred_test.ravel() != y_test).sum()/len(y_test)\n",
    "                result = {}\n",
    "                result['width'] = width\n",
    "                result['train MSE'] = error_train\n",
    "                result['test MSE'] = error_test\n",
    "                result['train MCE'] = mce_error_train\n",
    "                result['test MCE'] = mce_error_test\n",
    "                result['nonlinearity'] = key\n",
    "                result['epochs'] = epochs\n",
    "                result['batch_size'] = batchSize\n",
    "                results.append(result)\n",
    "        print(pd.DataFrame(results))\n",
    "    # Train with different sized networks\n",
    "    #### PART G ####\n",
    "    if DIFF_SIZES:\n",
    "        print('\\n----------------------------------------\\n')\n",
    "        print('Training with various sized network')\n",
    "        names = ['ReLU', 'tanh']\n",
    "        widths = [2, 4, 8, 16, 32]\n",
    "        errors = {}\n",
    "        results = []\n",
    "        for key in names:\n",
    "            error = []\n",
    "            for width in widths:\n",
    "                activation = activations[key]\n",
    "                model = Model(x.shape[1])\n",
    "                model.addLayer(DenseLayer(width,activation()))\n",
    "                model.addLayer(DenseLayer(1,LinearActivation()))\n",
    "                model.initialize(QuadraticCost())\n",
    "                epochs = 10\n",
    "                batch_size = 1\n",
    "                hist = model.trainBatch(x,y,batch_size,epochs,GDOptimizer(eta=lr[key]))\n",
    "                y_hat_train = model.predict(x)\n",
    "                y_hat_test = model.predict(X_test)\n",
    "                y_pred_train = np.sign(y_hat_train)\n",
    "                y_pred_test = np.sign(y_hat_test)\n",
    "\n",
    "                error_train = np.mean(np.square(y_hat_train - y))/2\n",
    "                error_test = np.mean(np.square(y_hat_test - y_test))/2\n",
    "                mce_error_train = (y_pred_train.ravel() != y_train).sum()/len(y_train)\n",
    "                mce_error_test = (y_pred_test.ravel() != y_test).sum()/len(y_test)\n",
    "                result = {}\n",
    "                result['width'] = width\n",
    "                result['train MSE'] = error_train\n",
    "                result['test MSE'] = error_test\n",
    "                result['train MCE'] = mce_error_train\n",
    "                result['test MCE'] = mce_error_test\n",
    "                result['nonlinearity'] = key\n",
    "                result['epochs'] = epochs\n",
    "                result['batch_size'] = batch_size\n",
    "                results.append(result)\n",
    "        print(pd.DataFrame(results))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
