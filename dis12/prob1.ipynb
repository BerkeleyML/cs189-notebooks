{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dis12 - Decision Boundary Visualization on Decision Tree, Random Forest, and Adaboost\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this discussion, we will first visualize the decision boundaries of different decision tree-based models, including basic decision tree, random tree, and Adaboost.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn\n",
                "import matplotlib.patches as patches\n",
                "%matplotlib inline\n",
                "seaborn.set(font_scale=2)\n",
                "seaborn.set_style(\"white\")\n",
                "\n",
                "from sklearn.preprocessing import normalize\n",
                "import numpy as np\n",
                "import ipywidgets as widgets\n",
                "from ipywidgets import interactive\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generate Data\n",
                "\n",
                "We first generate data points with label $y\\in \\{+1, -1\\}$ that also have all the $x_i$ from different classes being $r^{\\prime}$ apart, i.e.,\n",
                "\n",
                "$$\\| x_i - x_j \\|_{2} > r^{\\prime}, \\quad \\text{for}\\, y_i \\neq y_j.$$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cal_radius(x):\n",
                "    return np.sqrt(x[0] ** 2 + x[1] ** 2)\n",
                "\n",
                "\n",
                "# random points at least 2r apart\n",
                "m = 200\n",
                "np.random.seed(221)\n",
                "x_train = [np.random.uniform(size=(2))]\n",
                "\n",
                "r = 0.05\n",
                "epsilon = r/2\n",
                "\n",
                "while(len(x_train) < m):\n",
                "    p = np.random.uniform(size=(2))\n",
                "    if min(cal_radius(p-a) for a in x_train) > 1.0*r:\n",
                "        if np.abs(p[0] - p[1]) > r:\n",
                "            x_train.append(p)\n",
                "\n",
                "X_train = np.array(x_train)\n",
                "y_train = ((X_train[:, 1] - X_train[:, 0] > 0) * 2.0) - 1.0\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Visualize the data points.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(8, 8))\n",
                "ax.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=\"coolwarm\", s=70)\n",
                "ax.axis(\"equal\")\n",
                "ax.axis([0,1,0,1])\n",
                "ax.plot([0, 1.0], [0.0 + r, 1.0 + r], '--', color='black')\n",
                "ax.plot([0, 1.0], [0.0 - r, 1.0 - r], '--', color='black')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Setup the visualization functions.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_decision_boundary(clf, X, y, x, depth=None, num_trees=None):\n",
                "    '''Visualize the decision boundaries of classifiers'''\n",
                "    XX, YY = np.meshgrid(np.linspace(0, 1, 500), np.linspace(0, 1, 500))\n",
                "    X0 = np.stack([np.ravel(XX), np.ravel(YY)]).T\n",
                "    y0 = clf.predict(X0)\n",
                "    ZZ = y0.reshape(500,500)\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(8, 8))\n",
                "    if num_trees == None:\n",
                "        plt.title('Decision Boundary, depth={}'.format(depth), pad=20)\n",
                "    else:\n",
                "        plt.title('Decision Boundary, depth={}, number of trees={}'.format(depth, num_trees), pad=20)\n",
                "    ax.contourf(XX,YY,ZZ, cmap=\"coolwarm\", levels=np.linspace(-1000,1000,3))\n",
                "    ax.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", s=70)\n",
                "    ax.plot([0, 1.0], [0.0 + r, 1.0 + r], '--', color='black')\n",
                "    ax.plot([0, 1.0], [0.0 - r, 1.0 - r], '--', color='black')\n",
                "    ax.axis(\"equal\")\n",
                "    ax.axis([0,1,0,1])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_decision_box(clf, X, y, x, depth=None):\n",
                "    '''Visualize the decision boundaries as well as the boxes of decision tree'''\n",
                "    XX, YY = np.meshgrid(np.linspace(0, 1, 1000), np.linspace(0, 1, 1000))\n",
                "    X0 = np.stack([np.ravel(XX), np.ravel(YY)]).T\n",
                "    y0 = clf.apply(X0)\n",
                "    ZZ = y0.reshape(1000,1000)\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(8, 8))\n",
                "    plt.title('Boxes of Decision Tree, depth={}'.format(depth), pad=20)\n",
                "    ax.contour(XX,YY,ZZ, levels=np.unique(y0), colors='k')\n",
                "    ax.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", s=20)\n",
                "    ax.plot([0, 1.0], [0.0 + r, 1.0 + r], '--', color='black')\n",
                "    ax.plot([0, 1.0], [0.0 - r, 1.0 - r], '--', color='black')\n",
                "\n",
                "    XX, YY = np.meshgrid(np.linspace(0, 1, 500), np.linspace(0, 1, 500))\n",
                "    X0 = np.stack([np.ravel(XX), np.ravel(YY)]).T\n",
                "    y0 = clf.predict(X0)\n",
                "    ZZ = y0.reshape(500,500)\n",
                "    ax.contourf(XX,YY,ZZ, cmap=\"coolwarm\", levels=np.linspace(-1000,1000,3))\n",
                "    ax.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", s=70)\n",
                "    ax.axis(\"equal\")\n",
                "    ax.axis([0,1,0,1])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (I). Visualize in the 2d case\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Decision tree:**\n",
                "\n",
                "We first visualize the decision boundary of a standard decision tree. Besides the decision boundary, we also visualize the which leaf nodes the points belong to in this 2d case. **Since each leaf node corresponds to a 2d box**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_tree_depth():\n",
                "    return widgets.IntSlider(\n",
                "        value=5,\n",
                "        min=1,\n",
                "        max=10,\n",
                "        step=1,\n",
                "        description='depth of decision tree',\n",
                "        continuous_update=False)\n",
                "\n",
                "def generate_tree_num():\n",
                "    return widgets.IntSlider(\n",
                "        value=20,\n",
                "        min=1,\n",
                "        max=500,\n",
                "        step=1,\n",
                "        description='number of trees',\n",
                "        continuous_update=False)\n",
                "\n",
                "def generate_max_samples():\n",
                "    return widgets.IntSlider(\n",
                "        value=20,\n",
                "        min=1,\n",
                "        max=200,\n",
                "        step=1,\n",
                "        description='max samples',\n",
                "        continuous_update=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "def visualize_dt(depth_tree):\n",
                "    clf = DecisionTreeClassifier(max_depth=depth_tree)\n",
                "    clf = clf.fit(X_train, y_train)\n",
                "    visualize_decision_box(clf, X_train, y_train, x_train, depth_tree)\n",
                "    visualize_decision_boundary(clf, X_train, y_train, x_train, depth_tree)\n",
                "\n",
                "interactive_plot = interactive(visualize_dt, depth_tree=generate_tree_depth())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Random forest:**\n",
                "\n",
                "Question: Why happens to the boundary shape when you increase the number of `n_estimators`?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "def visualize_rf(max_depth, max_samples, n_estimators):\n",
                "    clf = RandomForestClassifier(max_depth=max_depth, max_samples=max_samples, random_state=0, n_estimators=n_estimators)\n",
                "    clf = clf.fit(X_train, y_train)\n",
                "    visualize_decision_boundary(clf, X_train, y_train, x_train, depth=max_depth, num_trees=n_estimators)\n",
                "\n",
                "interactive_plot = interactive(visualize_rf,\n",
                "                               max_depth=generate_tree_depth(),\n",
                "                               max_samples=generate_max_samples(),\n",
                "                               n_estimators=generate_tree_num())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Adaboost:**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "def visualize_adaboost(max_depth, n_estimators):\n",
                "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=max_depth),\n",
                "                             n_estimators=n_estimators,\n",
                "                             random_state=None,\n",
                "                             algorithm='SAMME')\n",
                "    clf.fit(X_train, y_train)\n",
                "    visualize_decision_boundary(clf, X_train, y_train, x_train, depth=max_depth, num_trees=n_estimators)\n",
                "\n",
                "interactive_plot = interactive(visualize_adaboost,\n",
                "                               max_depth=generate_tree_depth(),\n",
                "                               n_estimators=generate_tree_num())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (II). Visualize in the 2d case we have label noise in training data.\n",
                "\n",
                "In this part, we first random select a subset of the training sample and flip their labels. Then we visualize how the above three methods behavior under label noise.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(189)\n",
                "\n",
                "y_train_noise = y_train * 1.0\n",
                "random_idx = np.random.choice(m, int(len(y_train)*0.05), replace=False)\n",
                "y_train_noise[random_idx] = y_train[random_idx] * -1.0\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(8, 8))\n",
                "ax.scatter(X_train[:,0], X_train[:,1], c=y_train_noise, cmap=\"coolwarm\", s=70)\n",
                "ax.axis(\"equal\")\n",
                "ax.axis([0,1,0,1])\n",
                "ax.plot([0, 1.0], [0.0 + r, 1.0 + r], '--', color='black')\n",
                "ax.plot([0, 1.0], [0.0 - r, 1.0 - r], '--', color='black')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Decision tree:**\n",
                "\n",
                "Change the depth of the tree and check whether you could achieve zero training error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "def visualize_dt(depth_tree):\n",
                "    clf = DecisionTreeClassifier(max_depth=depth_tree)\n",
                "    clf = clf.fit(X_train, y_train_noise)\n",
                "    print('training error: ', 1.0 - clf.score(X_train, y_train_noise))\n",
                "    visualize_decision_box(clf, X_train, y_train, x_train, depth_tree)\n",
                "    visualize_decision_boundary(clf, X_train, y_train, x_train, depth_tree)\n",
                "\n",
                "interactive_plot = interactive(visualize_dt, depth_tree=generate_tree_depth())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Random forest:**\n",
                "\n",
                "Question: Set the number of trees to be large, why the behaviour of random forest is still reasonably good under label noise?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "def visualize_rf(max_depth, max_samples, n_estimators):\n",
                "    clf = RandomForestClassifier(max_depth=max_depth, max_samples=max_samples, random_state=0, n_estimators=n_estimators)\n",
                "    clf = clf.fit(X_train, y_train_noise)\n",
                "    print('training error: ', 1.0 - clf.score(X_train, y_train_noise))\n",
                "    visualize_decision_boundary(clf, X_train, y_train, x_train, depth=max_depth, num_trees=n_estimators)\n",
                "\n",
                "interactive_plot = interactive(visualize_rf,\n",
                "                               max_depth=generate_tree_depth(),\n",
                "                               max_samples=generate_max_samples(),\n",
                "                               n_estimators=generate_tree_num())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Adaboost:**\n",
                "\n",
                "Does Adaboost behave better than decision tree under label noise? In what sense it behave better? Evaluate the training error of the adaboost model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "def visualize_adaboost(max_depth, n_estimators):\n",
                "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=max_depth),\n",
                "                             n_estimators=n_estimators,\n",
                "                             random_state=None,\n",
                "                             algorithm='SAMME')\n",
                "    clf.fit(X_train, y_train_noise)\n",
                "    print('training error: ', 1.0 - clf.score(X_train, y_train_noise))\n",
                "    visualize_decision_boundary(clf, X_train, y_train, x_train, depth=max_depth, num_trees=n_estimators)\n",
                "\n",
                "interactive_plot = interactive(visualize_adaboost,\n",
                "                               max_depth=generate_tree_depth(),\n",
                "                               n_estimators=generate_tree_num())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Congrats! Hope you get a better understanding of the tree-based methods!**\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": {},
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}