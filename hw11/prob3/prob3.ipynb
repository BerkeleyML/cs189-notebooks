{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Kernel Logistic Regression and SVMs\n",
                "\n",
                "In this notebook, we will look at the behavior of logistic regression as compared to SVMs, in both the standard (i.e. linear kernel) and kernelized cases. We will work using toy data generated from various distributions.\n",
                "\n",
                "We will first generate some data from two overlapping Gaussians, and see how they are separated using both techniques.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn import svm\n",
                "from tqdm import *\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gen_gaussian_points(n, mean, sigma):\n",
                "    return np.random.normal(mean, sigma, [n, 2])\n",
                "\n",
                "\n",
                "N = 100\n",
                "\n",
                "class_a = gen_gaussian_points(N, [-1, -1], [1, 1])\n",
                "class_b = gen_gaussian_points(N, [1, 1], [1, 1])\n",
                "\n",
                "X = np.vstack([class_a, class_b])\n",
                "y = np.vstack([[-1]] * N + [[1]] * N).reshape((-1,))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def logistic_regression(X, y):\n",
                "    model = LogisticRegression()\n",
                "    model.fit(X, y)\n",
                "\n",
                "    return model.coef_\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we will generate some points from two different distributions and verify that logistic regression separates them well.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "w = logistic_regression(X, y).T\n",
                "print(w)\n",
                "plt.scatter(*class_a.T)\n",
                "plt.scatter(*class_b.T)\n",
                "plt.plot(np.linspace(-3, 3, 50), -np.linspace(-3, 3, 50) * w[0] / w[1])\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Of course, this approach only works when our points are roughly linearly separable. Otherwise, one approach is to consider some kernel function rather than acting on the features directly. One such kernel function, that we have seen before when looking at kernel least squares, is the polynomial kernel. We supply an implementation below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def polynomial_kernel(degree=2):\n",
                "    def kernel(X, Y=None):\n",
                "        # this line lets us compute the inner product between\n",
                "        # two different datasets without having to stack them\n",
                "        # into a single matrix\n",
                "        Y = X if Y is None else Y\n",
                "\n",
                "        # this is the actual computation\n",
                "        return (X @ Y.T + 1) ** degree\n",
                "    return kernel\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part 1\n",
                "Now, complete the implementation of the kernelized gradient descent step below, for logistic regression.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.special import expit\n",
                "\n",
                "def s(x):\n",
                "    return expit(x)\n",
                "\n",
                "def gradient_step(a, gamma, K, y):\n",
                "    \"\"\"\n",
                "    a is the vector of dual weights at time t\n",
                "    gamma is the step size\n",
                "    K is the kernel matrix Phi Phi^T for our dataset\n",
                "    y is a column of labels\n",
                "    \"\"\"\n",
                "    ### start gradient_step ###\n",
                "\n",
                "    ### end gradient_step ###\n",
                "\n",
                "def kernel_logistic_regression(X, y, kernel, *, gamma=0.001, iterations=100, a_init=None):\n",
                "    n, d = X.shape\n",
                "    K = kernel(X)\n",
                "    a = np.ones(n) / 100 if a_init is None else a_init # some arbitary initial values\n",
                "    for _ in range(iterations):\n",
                "        a = gradient_step(a, gamma, K, y)\n",
                "\n",
                "    return a\n",
                "\n",
                "def kernel_logistic_predict(a, X, X_test, kernel):\n",
                "    K_test = kernel(X_test, X)\n",
                "    return s(K_test @ a)\n",
                "\n",
                "def visualize_decision_boundary(classifier):\n",
                "    XX, YY = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n",
                "    X0 = np.stack([np.ravel(XX), np.ravel(YY)]).T\n",
                "    y0 = classifier(X0)\n",
                "    ZZ = y0.reshape(100, 100)\n",
                "    plt.contourf(XX, YY, ZZ, cmap=\"coolwarm\", levels=np.linspace(0,1,3))\n",
                "    plt.xlim(-4, 4)\n",
                "    plt.ylim(-4, 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how well this works on some data that is not linearly separable. We will construct a dataset with the classes separated by their radial distance from the origin:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gen_radial(n, mean, sigma):\n",
                "    dists = np.random.normal(mean, sigma, n).reshape([n, 1])\n",
                "    angles = np.random.uniform(0, 2 * np.pi, n)\n",
                "    return dists * np.array([np.cos(angles), np.sin(angles)]).T\n",
                "\n",
                "\n",
                "N = 100\n",
                "\n",
                "class_a = gen_radial(N, 3, 0.5)\n",
                "class_b = gen_radial(N, 1, 0.5)\n",
                "\n",
                "X = np.vstack([class_a, class_b])\n",
                "y = np.vstack([[0]] * N + [[1]] * N).reshape((-1,))\n",
                "plt.scatter(*class_a.T)\n",
                "plt.scatter(*class_b.T)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's try fitting a logistic classifier to this data, using a degree 2 polynomial kernel.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kernel = polynomial_kernel(degree=2)\n",
                "a = kernel_logistic_regression(X, y, kernel, iterations=300000, gamma=1e-2)\n",
                "classifier = lambda X_test: kernel_logistic_predict(a, X, X_test, kernel)\n",
                "visualize_decision_boundary(classifier)\n",
                "plt.scatter(*class_a.T)\n",
                "plt.scatter(*class_b.T)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part 2\n",
                "\n",
                "Try changing to a degree 1 kernel. Does it still successfully classify the training data? What if you make the degree of the kernel very large (say, 10)? How does this affect the accuracy and running time of the classifier? Also try varying the parameters of the distributions. Comment on your results.\n",
                "\n",
                "### start observations-1 ###\n",
                "\n",
                "### end observations-1 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This should be mostly expected. We saw last week that logistic regression produces a maximum-margin classifier in the case of separable data when trained using gradient descent, and that is indeed what we are observing.\n",
                "\n",
                "Last week, part of our argument essentially stated that it is only those points on the margin between the two classes that affect the logistic regression boundary. We can investigate whether that is true numerically, by looking at the magnitude of the dual weights for each of our points. We plot these magnitudes below:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_dual_weights(X, a):\n",
                "    a = np.abs(a)\n",
                "    idx = a.argsort()\n",
                "    X = X[idx]\n",
                "    a = a[idx]\n",
                "    plt.scatter(*X.T, c=a)\n",
                "    plt.colorbar()\n",
                "\n",
                "visualize_decision_boundary(classifier)\n",
                "plot_dual_weights(X, a)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Does the above plot match our expectations? (i.e. are the most heavily weighted points those at the margin of the classifier?)\n",
                "\n",
                "### start observations-2 ###\n",
                "\n",
                "### end observations-2 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3\n",
                "Now, we will look at the case when our data points are not separable. We will generate points from a similar distribution to before, just with a larger standard deviation.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_a = gen_radial(N, 3, 1)\n",
                "class_b = gen_radial(N, 1, 1)\n",
                "\n",
                "X = np.vstack([class_a, class_b])\n",
                "y = np.vstack([[0]] * N + [[1]] * N).reshape((-1,))\n",
                "plt.scatter(*class_a.T)\n",
                "plt.scatter(*class_b.T)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Classifying using kernel logistic regression, we obtain the following decision boundary.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kernel = polynomial_kernel(degree=2)\n",
                "a = kernel_logistic_regression(X, y, kernel, iterations=300000, gamma=1e-2)\n",
                "classifier = lambda X_test: kernel_logistic_predict(a, X, X_test, kernel)\n",
                "visualize_decision_boundary(classifier)\n",
                "plt.scatter(*class_a.T)\n",
                "plt.scatter(*class_b.T)\n",
                "plt.xlim(-4,4)\n",
                "plt.ylim(-4,4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You should see that while the classifier correctly splits most of our training points by class, it incorrectly classifies some points. We can now look at the dual weights of each of our training points, just like before.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_dual_weights(X, a):\n",
                "    a = np.abs(a)\n",
                "    idx = a.argsort()\n",
                "    X = X[idx]\n",
                "    a = a[idx]\n",
                "    plt.scatter(*X.T, c=a)\n",
                "    plt.colorbar()\n",
                "\n",
                "visualize_decision_boundary(classifier)\n",
                "plot_dual_weights(X, a)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Notice that the points on the margin have the highest weights, as before. But there are also some points _not_ on the margin with large dual weights. What distinguishes these points? Why did logistic regression assign them a high weight?\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### start observations-3 ###\n",
                "These are the points that are misclassified by logistic regression, so they are applying a strong \"pull\" to move the decision boundary closer so they are correctly classified. Hence, they have a high weight in the final solution.\n",
                "### end observations-3\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4\n",
                "Next, we will look at an alternative method of classifying points, that explicitly seeks to maximize the margin between the decision boundary and the test points: the SVM. We classify our earlier distribution using an SVM below, with a linear kernel (i.e. working using just the raw features):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn import svm\n",
                "\n",
                "class_a = gen_radial(N, 3, 0.5)\n",
                "class_b = gen_radial(N, 1, 0.5)\n",
                "\n",
                "X = np.vstack([class_a, class_b])\n",
                "y = np.vstack([[0]] * N + [[1]] * N).reshape((-1,))\n",
                "\n",
                "model = svm.LinearSVC() # SVC = support vector classifier, which we just call an SVM\n",
                "model.fit(X, y)\n",
                "visualize_decision_boundary(lambda X_test: model.predict(X_test))\n",
                "plt.scatter(*class_a.T)\n",
                "plt.scatter(*class_b.T)\n",
                "plt.plot()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comment on the accuracy of the linear SVM as compared to logistic regrsesion with polynomial kernels of various degrees. Did the linear SVM do any better? Should we have expected it to?\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### start observations-4 ###\n",
                "\n",
                "### end observations-4 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "SVMs are very similar to logistic regression, in that they both try to construct a linear decision boundary to classify our data, focusing on the points near the boundary and disregarding those further away.\n",
                "\n",
                "Logistic regression essentially reduces the \"importance\" of data points the further they are from the decision boundary. In contrast, SVMs can be thought of as *immediately* ignoring all points that are correctly classified, unless they are right on the margin. This is a similar effect to what we have just seen with logistic regression - SVMs exhibit this property exactly even when the data is not linearly separable.\n",
                "\n",
                "Considering SVMs from the perspective of the dual, we see as a consequence that the only nonzero coefficients in the dual solution correspond to either points with nonzero slack, or points that lie on the margin. Since in most real-world cases, we expect the majority of training points to be correctly classified, this means that the dual weights of an SVM are _sparse_, with only a few nonzero weights.\n",
                "\n",
                "Imagine that we have $n$ training points, $a$ of which are on the SVM margin, and a further $b$ of which are inside the margin and so have nonzero slack. How many weights in the _primal_ weight vector $\\vec{w}$ computed after training an SVM can be nonzero? What about in the _dual_ weight vector $\\vec{a}$? (Recall that $\\vec{w} = \\mathbf{X}\\vec{a}$).\n",
                "\n",
                "Then answer the same questions again, but for weights trained using logistic regression, not using an SVM. What does this tell you about the computational complexity of classification using SVMs versus logistic regression? (i.e. given a model, consider the time complexity to classify a single test point in both cases)\n",
                "\n",
                "### start calculation ###\n",
                "\n",
                "### end calculation ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5\n",
                "\n",
                "Taking inspiration from the sparse solutions from the SVM, we can try to obtain similarly sparse solutions from logistic regression directly, by simply rounding the smallest dual weights to zero. Notice how it only makes sense to do this with the dual solution, since discarding the smallest dual weights can be interpreted as discarding the contributions of points very far into the correct side, which we know don't have a significant impact on logistic regression anyway. **Explain why discarding the smallest weights in the primal weight vector, rather than the dual, obtained by logistic regression could cause issues**.\n",
                "\n",
                "### start explanation ###\n",
                "\n",
                "### end explanation ###\n",
                "\n",
                "The below cell plots the number of components rounded to zero against the training accuracy, using the nonlinear classification problem we saw earlier and solving using logistic regression with a quadratic kernel.\n",
                "\n",
                "Note that we take a small number of gradient steps after rounding, since the effects of the discarded points (though small) may have shifted our classifier slightly in aggregate, so a couple of gradient steps will help to \"re-align\" our classifier while using only a subset of the original training points, and yielding a solution essentially identical to the original non-sparse one.\n",
                "\n",
                "Run the cell and comment on your observations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def accuracy(y_pred, y):\n",
                "    binary_preds = y_pred > 0.5\n",
                "    return np.sum(y == binary_preds) / n\n",
                "\n",
                "# a small number of gradient steps taken after discarding the small dual weights,\n",
                "# far fewer than those taken in the initial training\n",
                "NUM_REWEIGHT_ITERATIONS = 10\n",
                "\n",
                "kernel = polynomial_kernel(degree=2)\n",
                "a = kernel_logistic_regression(X, y, kernel, iterations=30000, gamma=1e-2)\n",
                "n, d = X.shape\n",
                "\n",
                "a_deleted = a.copy()\n",
                "deleted_indices = []\n",
                "updated_as = []\n",
                "updated_Xs = []\n",
                "accuracies = []\n",
                "for num_zeroed_components in range(n):\n",
                "    if np.nonzero(a_deleted)[0].size == 0:\n",
                "        break\n",
                "    [[i, *_]] = np.where(np.abs(a_deleted) == np.min(np.abs(a_deleted[np.nonzero(a_deleted)])))\n",
                "    a_deleted[i] = 0\n",
                "    deleted_indices.append(i)\n",
                "    a_updated = kernel_logistic_regression(\n",
                "        X[a_deleted != 0],\n",
                "        y[a_deleted != 0],\n",
                "        kernel,\n",
                "        iterations=NUM_REWEIGHT_ITERATIONS,\n",
                "        gamma=1e-2,\n",
                "        a_init=a_deleted[a_deleted != 0],\n",
                "    )\n",
                "    updated_Xs.append(X[a_deleted != 0])\n",
                "    updated_as.append(a_updated)\n",
                "    classes = kernel_logistic_predict(a_updated, X[a_deleted != 0], X, kernel)\n",
                "    accuracies.append(accuracy(classes, y))\n",
                "\n",
                "plt.plot(accuracies)\n",
                "plt.xlabel(\"Number of dual weights rounded to 0\")\n",
                "plt.ylabel(\"Training accuracy\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### start observations-5 ###\n",
                "\n",
                "### end observations-5 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 6\n",
                "Another way to visualize this effect is to watch points get deleted from the model as we increase the desired level of sparsity. Run the below cell and drag the slider to the right. Which points have the least contribution to the logistic regression solution, and so get deleted first? When does the model start to break down?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot(i):\n",
                "    remaining_indices = deleted_indices[i:]\n",
                "    pts = X[remaining_indices]\n",
                "    labels = y[remaining_indices]\n",
                "    class_a = pts[labels == 0]\n",
                "    class_b = pts[labels == 1]\n",
                "\n",
                "    visualize_decision_boundary(lambda X_test: kernel_logistic_predict(updated_as[i], updated_Xs[i], X_test, kernel))\n",
                "\n",
                "    plt.scatter(*class_a.T)\n",
                "    plt.scatter(*class_b.T)\n",
                "    plt.xlim(-4, 4)\n",
                "    plt.ylim(-4, 4)\n",
                "\n",
                "\n",
                "interact(plot, i=IntSlider(min=0, max=n, continuous_update=False))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comment on the behavior you observe above. Is selecting a sparse solution from logistic regression a feasible alternative to the SVM? Why or why not?\n",
                "\n",
                "### start observations-6 ###\n",
                "\n",
                "### end observations-6 ###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 7 (no work required)\n",
                "In the final part of this notebook, we will look at an application of Kernel SVM to classify the CIFAR-10 dataset, an image classification dataset. We first import necessary Python packages and define relevant functions.\n",
                "\n",
                "This is intended just as a demo of kernel SVM on real-world data, that will be expanded on next week. For now, just look at the results!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import *\n",
                "\n",
                "def multiscale(X_train):\n",
                "    X_train0 = np.reshape(X_train, (X_train.shape[0], -1))\n",
                "    X_train2 = np.reshape(X_train[:,::2,::2,:], (X_train.shape[0], -1))\n",
                "    return np.hstack((X_train0, X_train2))\n",
                "def normalize(X):\n",
                "    return (X - np.mean(X, axis=1, keepdims=True) )/np.std(X, axis=1, keepdims=True)\n",
                "def svm_alg(c, g, xTrain, yTrain, xVal, yVal, *, kernel):\n",
                "    svc_rbf = svm.SVC(probability = False, kernel = kernel, C = c, gamma = g)\n",
                "\n",
                "    # Fit the classifier on the training set.\n",
                "    svc_rbf.fit(xTrain, yTrain)\n",
                "\n",
                "    print(\"c = {}\".format(c))\n",
                "\n",
                "    # Find the prediction and accuracy on the training set.\n",
                "    Yhat_svc_rbf_train = svc_rbf.predict(xTrain)\n",
                "    acc = np.mean(Yhat_svc_rbf_train == yTrain)\n",
                "    print('Train Accuracy = {0:f}'.format(acc))\n",
                "\n",
                "    # Find the prediction and accuracy on the test set.\n",
                "    Yhat_svc_rbf_test = svc_rbf.predict(xVal)\n",
                "    acc2 = np.mean(Yhat_svc_rbf_test == yVal)\n",
                "    print('Test Accuracy = {0:f}'.format(acc2))\n",
                "    return acc, acc2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will load a subset of CIFAR10 dataset.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = np.load(\"X_train.npy\")\n",
                "X_val = np.load(\"X_val.npy\")\n",
                "y_train = np.load(\"y_train.npy\")\n",
                "y_val = np.load(\"y_val.npy\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(X_train.shape)\n",
                "print(X_val.shape)\n",
                "print(y_train.shape)\n",
                "print(y_val.shape)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's visualize some images.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize some examples from the dataset.\n",
                "# We show a few examples of training images from each class.\n",
                "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
                "num_classes = len(classes)\n",
                "samples_per_class = 7\n",
                "for y, cls in enumerate(classes):\n",
                "    idxs = np.flatnonzero(y_train == y)\n",
                "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
                "    for i, idx in enumerate(idxs):\n",
                "        plt_idx = i * num_classes + y + 1\n",
                "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
                "\n",
                "        plt.imshow(X_train[idx].astype('uint8'))\n",
                "        plt.axis('off')\n",
                "        if i == 0:\n",
                "            plt.title(cls)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (a)\n",
                "Run the SVM, using the RBF kernel. Note that, to classify points with more than two classes, we employ the \"one versus rest\" classification technique discussed in lecture. So we are really training $k - 1$ binary classifiers (where we have $k$ classes) and combining them together into a single $k$-fold classifier.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocessing: reshape the image data into rows\n",
                "X_train_ = np.reshape(X_train, (X_train.shape[0], -1))\n",
                "X_val_ = np.reshape(X_val, (X_val.shape[0], -1))\n",
                "\n",
                "# normalize data\n",
                "X_train_ = normalize(X_train_)\n",
                "X_val_ = normalize(X_val_)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "acc_train_svm = []\n",
                "acc_test_svm = []\n",
                "c_svm = [0.1, 0.5, 1, 2, 10]\n",
                "\n",
                "for c in tqdm(c_svm):\n",
                "    acc, acc2 = svm_alg(c, 'auto', X_train_, y_train, X_val_, y_val, kernel=\"rbf\")\n",
                "    acc_train_svm.append(acc)\n",
                "    acc_test_svm.append(acc2)\n",
                "\n",
                "\n",
                "plt.plot(c_svm, acc_train_svm,'r.-', label=\"Training\")\n",
                "plt.plot(c_svm, acc_test_svm,'go-', label=\"Test\")\n",
                "plt.xlabel('c')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title(\"Plot of accuracy vs c for training and test data\")\n",
                "plt.legend()\n",
                "plt.grid()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (b)\n",
                "Run SVM with multi-scale RBF kernel.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# multiscale\n",
                "X_train_ = multiscale(X_train)\n",
                "X_val_ = multiscale(X_val)\n",
                "\n",
                "# normalize data\n",
                "X_train_ = normalize(X_train_)\n",
                "X_val_ = normalize(X_val_)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "acc_train_svm_mul = []\n",
                "acc_test_svm_mul = []\n",
                "c_svm = [0.1, 0.5, 1, 2, 10]\n",
                "\n",
                "for c in tqdm(c_svm):\n",
                "    acc, acc2 = svm_alg(c, 'auto', X_train_, y_train, X_val_, y_val, kernel=\"rbf\")\n",
                "    acc_train_svm_mul.append(acc)\n",
                "    acc_test_svm_mul.append(acc2)\n",
                "\n",
                "plt.plot(c_svm, acc_train_svm,'r.-', label=\"Train\")\n",
                "plt.plot(c_svm, acc_test_svm,'g^-', label=\"Val\")\n",
                "plt.plot(c_svm, acc_train_svm_mul,'b^--', label=\"Train_ms\")\n",
                "plt.plot(c_svm, acc_test_svm_mul,'kx:', label=\"Val_ms\")\n",
                "plt.xlabel('c')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title(\"Plot of accuracy vs c for training and test data\")\n",
                "plt.legend()\n",
                "plt.grid()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}