{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HW12 - Adversarial Examples\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this problem, we will first visualize the decision boundaries of different models, and then study the robustness of the linear models as well as the kernel ridge regression model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.autograd import Variable\n",
                "from torchvision import datasets, transforms\n",
                "import torch.nn.functional as F\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn\n",
                "import matplotlib.patches as patches\n",
                "%matplotlib inline\n",
                "seaborn.set(font_scale=2)\n",
                "seaborn.set_style(\"white\")\n",
                "\n",
                "from sklearn.preprocessing import normalize\n",
                "import numpy as np\n",
                "import ipywidgets as widgets\n",
                "from ipywidgets import interactive\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (I). 2D Toy Example\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generate Data\n",
                "\n",
                "We first generate data points with label $y\\in \\{+1, -1\\}$ that also have all the $x_i$ being $r^{\\prime}$ apart, i.e.,\n",
                "\n",
                "$$\\| x_i - x_j \\|_{2} > r^{\\prime}, \\quad \\text{for}\\, i \\neq j.$$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cal_radius(x):\n",
                "    return np.sqrt(x[0] ** 2 + x[1] ** 2)\n",
                "\n",
                "\n",
                "# random points at least 2r apart\n",
                "m = 50\n",
                "np.random.seed(221)\n",
                "x_train = [np.random.uniform(size=(2))]\n",
                "\n",
                "r = 0.1\n",
                "epsilon = r/2\n",
                "\n",
                "while(len(x_train) < m):\n",
                "    p = np.random.uniform(size=(2))\n",
                "    if min(cal_radius(p-a) for a in x_train) > 1.1*r:\n",
                "        x_train.append(p)\n",
                "\n",
                "X_train = torch.Tensor(np.array(x_train))\n",
                "torch.manual_seed(1)\n",
                "y_train = (torch.rand(m)+0.5).long()\n",
                "y_train = (X_train[:, 1] - X_train[:, 0] > 0).long()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Visualize the data points with $\\|\\cdot\\|_{2}$ perturbation balls around training samples.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(8, 8))\n",
                "ax.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=\"coolwarm\", s=70)\n",
                "ax.axis(\"equal\")\n",
                "ax.axis([0,1,0,1])\n",
                "for a in x_train:\n",
                "    ax.add_patch(patches.Circle((a[0], a[1]), r*0.5, fill=False, edgecolor='black'))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training a Neural Network\n",
                "We first train a one-hidden-layer network with a width of 200, and then visualize the decision boundary of the learned NN.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def standard_train(X, y):\n",
                "    net = nn.Sequential(\n",
                "        nn.Linear(2, 200),\n",
                "        nn.ReLU(),\n",
                "        nn.Linear(200,200),\n",
                "        nn.ReLU(),\n",
                "        nn.Linear(200,2)\n",
                "    )\n",
                "\n",
                "    opt = optim.Adam(net.parameters(), lr=1e-3)\n",
                "    for i in range(1000):\n",
                "        out = net(Variable(X))\n",
                "        l = nn.CrossEntropyLoss()(out, Variable(y))\n",
                "        err = (out.max(1)[1].data != y).float().mean()\n",
                "        opt.zero_grad()\n",
                "        (l).backward()\n",
                "        opt.step()\n",
                "    print('loss: ', l.data.item(), 'training error: ', err.item())\n",
                "    return net.eval()\n",
                "\n",
                "\n",
                "def visualize_dnn(net, X, y, x):\n",
                "    XX, YY = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
                "    X0 = Variable(torch.Tensor(np.stack([np.ravel(XX), np.ravel(YY)]).T))\n",
                "    y0 = net(X0)\n",
                "    ZZ = (y0[:,0] - y0[:,1]).resize(100,100).data.numpy()\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(8, 8))\n",
                "    plt.title('Deep Neural Network', pad=20)\n",
                "    ax.contourf(XX,YY,-ZZ, cmap=\"coolwarm\", levels=np.linspace(-1000,1000,3))\n",
                "    ax.scatter(X.numpy()[:,0], X.numpy()[:,1], c=y.numpy(), cmap=\"coolwarm\", s=70)\n",
                "    ax.axis(\"equal\")\n",
                "    ax.axis([0,1,0,1])\n",
                "\n",
                "    for a in x:\n",
                "        ax.add_patch(patches.Circle((a[0], a[1]), r*0.5, fill=False))\n",
                "\n",
                "net = standard_train(X_train, y_train)\n",
                "visualize_dnn(net, X_train, y_train, x_train)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Random fourier features\n",
                "We consider first transforming the input $x_i$ to fourier features $z_i$ with dimension $2d$, i.e., $z_i \\in\\mathbb{R}^{2d}$,\n",
                "\n",
                "\\begin{align*}\n",
                "z_{i}^{2k} &= \\text{cos}(0.1 \\cdot 2 \\cdot \\pi \\cdot k \\cdot h_{k}^{\\top}x_{i}),\\\\\n",
                "z_{i}^{2k+1} &= \\text{sin}(0.1 \\cdot 2 \\cdot \\pi \\cdot k  \\cdot h_{k}^{\\top}x_{i}),\n",
                "\\end{align*}\n",
                "\n",
                "where $h_{k}\\in \\mathbb{R}^{2\\times 1}$ is a random vector and $\\|h_{k}\\|_{2}=1$.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Process data.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_train = y_train * 2.0 - 1.0\n",
                "X_train_np = X_train.numpy()\n",
                "y_train_np = y_train.numpy()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Define functions**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def featurization(X_train, H):\n",
                "    '''Transform data to Fourier features'''\n",
                "    X_feature = []\n",
                "    feature_d = H.shape[0]\n",
                "    Pi = np.pi\n",
                "    for k in range(feature_d):\n",
                "        X_feature.append(np.cos(0.1*2*Pi*k*X_train@H[k].transpose()))\n",
                "        X_feature.append(np.sin(0.1*2*Pi*k*X_train@H[k].transpose()))\n",
                "    X_feature = np.stack(X_feature, axis=1)\n",
                "    return X_feature\n",
                "\n",
                "def ridge_solver(X, y, lambda_reg):\n",
                "    return np.linalg.inv(X.transpose()@X + lambda_reg * np.diag(np.ones(X.shape[1])))@X.transpose()@y\n",
                "\n",
                "def visualize_ridge_fourier(H, X_feature, X, y, x):\n",
                "    XX, YY = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
                "    X0 = np.stack([np.ravel(XX), np.ravel(YY)]).T\n",
                "    # Perform feature transformation\n",
                "    X0_feature = featurization(X0, H)\n",
                "    # Learn linear model using ridge regression\n",
                "    w_ridge = ridge_solver(X_feature, y, lambda_reg=1e-5)\n",
                "    y0 = X0_feature@w_ridge * (-1.0)\n",
                "    ZZ = y0.reshape(100,100)\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(8, 8))\n",
                "    plt.title('Ridge Regression Using Fourier Features', pad=20)\n",
                "    ax.contourf(XX,YY,-ZZ, cmap=\"coolwarm\", levels=np.linspace(-1000,1000,3))\n",
                "    ax.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", s=70)\n",
                "    ax.axis(\"equal\")\n",
                "    ax.axis([0,1,0,1])\n",
                "\n",
                "    for a in x:\n",
                "        ax.add_patch(patches.Circle((a[0], a[1]), r*0.5, fill=False))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_fourier_dimension_widget():\n",
                "    return widgets.IntSlider(\n",
                "        value=10,\n",
                "        min=5,\n",
                "        max=200,\n",
                "        step=5,\n",
                "        description='number of Fourier features',\n",
                "        continuous_update=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_fourier(d):\n",
                "    feature_d = d\n",
                "    Pi = np.pi\n",
                "    H = np.random.randn(feature_d, 2)\n",
                "    H = normalize(H, axis=1, norm='l2')\n",
                "    X_feature_train = featurization(X_train, H)\n",
                "    visualize_ridge_fourier(H, X_feature_train, X_train_np, y_train_np, x_train)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Visualize the decision boundry of the learned ridge regression on Fourier features**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "interactive_plot = interactive(visualize_fourier, d=generate_fourier_dimension_widget())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Kernel Ridge Regression\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Recall the rbf kernel,\n",
                "\n",
                "$$K_{\\text{rbf}}(x_i, x_j) = \\exp(-\\gamma\\|x_i - x_j\\|_{2}^2),$$\n",
                "\n",
                "we visualize the decision boundary of kernel ridge regression model with different $\\gamma$ parameter.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.kernel_ridge import KernelRidge\n",
                "\n",
                "\n",
                "def visualize_kernel_ridge(X, y, x, gamma):\n",
                "    clf_kernel_ridge = KernelRidge(alpha=1e-5, kernel='rbf', gamma=gamma)\n",
                "    clf_kernel_ridge.fit(X, y)\n",
                "    XX, YY = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
                "    X0 = np.stack([np.ravel(XX), np.ravel(YY)]).T\n",
                "    y0 = clf_kernel_ridge.predict(X0)\n",
                "    ZZ = y0.reshape(100,100)\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(8, 8))\n",
                "    plt.title('Kernel Ridge Regression', pad=20)\n",
                "    ax.contourf(XX,YY,ZZ, cmap=\"coolwarm\", levels=np.linspace(-1000,1000,3))\n",
                "    ax.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", s=70)\n",
                "    ax.axis(\"equal\")\n",
                "    ax.axis([0,1,0,1])\n",
                "\n",
                "    for a in x:\n",
                "        ax.add_patch(patches.Circle((a[0], a[1]), r*0.5, fill=False))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_gamma_widget():\n",
                "    return widgets.FloatLogSlider(\n",
                "        value=1.0,\n",
                "        base=10.0,\n",
                "        min=-3,\n",
                "        max=6,\n",
                "        step=1,\n",
                "        description='$\\gamma$:',\n",
                "        continuous_update= False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_kernel_gamma(gamma):\n",
                "    visualize_kernel_ridge(X_train_np, y_train_np, x_train, gamma)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Visualize the decision boundry of the learned kernel ridge regression**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "interactive_plot = interactive(visualize_kernel_gamma, gamma=generate_gamma_widget())\n",
                "interactive_plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (II). MNIST Binary Classification - digit '1' and digit '3'\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we study the binary classification problem on a subset of the MNIST dataset: we are distinguishing digit `1` and digit `3`.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Load training data**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_mnist = np.load('X_mnist_1_3_train.npy')\n",
                "y_train_mnist = np.load('y_mnist_1_3_train.npy')\n",
                "X_test_mnist = np.load('X_mnist_1_3_test.npy')\n",
                "y_test_mnist = np.load('y_mnist_1_3_test.npy')\n",
                "\n",
                "# transform label to {+1, -1}\n",
                "y_train_mnist = y_train_mnist * 2.0 - 1.0\n",
                "y_test_mnist = y_test_mnist * 2.0 - 1.0\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Learn linear model via logistic regression.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "clf_LR = LogisticRegression(random_state=0).fit(X_train_mnist, y_train_mnist)\n",
                "print('Accuracy on Original Test Examples', clf_LR.score(X_test_mnist, y_test_mnist))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now you need to apply the results you derived in Part (a) to construct the adversarial perturbation and write the code.\n",
                "\n",
                "(Hint (1): You only need to construct one perturbation $\\delta$ for all examples.)\n",
                "\n",
                "(Hint (2): You could you the weights ($\\theta$) of the learned linear models.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "weights_lr = clf_LR.coef_\n",
                "epsilon = 0.1\n",
                "# TODO: You need to write code to construct the adversarial perturbation\n",
                "#       Name the perturbation as 'delta_adv'\n",
                "#       The constructed adversarial perturbation is R^{d}\n",
                "\n",
                "### start compute_delta_adv ###\n",
                "\n",
                "### end compute_delta_adv ###\n",
                "\n",
                "# Repeat the perturbation for n times and construct the perturbation matrix with same dimension as X_train_mnist\n",
                "# For each data point, multiply the computed 'delta_adv' with label y_i \\in {+1, -1}\n",
                "delta_adv_mtx =  np.tile(delta_adv, (1,y_test_mnist.shape[0])).transpose()\n",
                "sign_y_test_mnist_mtx = np.tile(y_test_mnist, (delta_adv.shape[0], 1)).transpose()\n",
                "delta_adv_mtx = np.multiply(delta_adv_mtx, sign_y_test_mnist_mtx)\n",
                "\n",
                "print('Accuracy on Adversarial Perturbed Test Examples', clf_LR.score(X_test_mnist+delta_adv_mtx, y_test_mnist))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To see if this is good or bad, we random perturb the inputs and see what happens.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "delta_random_mtx = epsilon * np.sign(np.random.randn(2000, 784))\n",
                "print('Accuracy on Random Perturbed Test Examples', clf_LR.score(X_test_mnist+delta_random_mtx, y_test_mnist))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Next, we take one particular test example, and visualize the original as well as the perturbed images.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('True Label: ', y_test_mnist[0])\n",
                "print('Prediction on Original Sample: ', clf_LR.predict(X_test_mnist)[0])\n",
                "print('Prediction on Adversarial Perturbed Sample: ', clf_LR.predict(X_test_mnist+delta_adv_mtx)[0])\n",
                "print('Prediction on Random Perturbed Sample: ', clf_LR.predict(X_test_mnist+delta_random_mtx)[0])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axs = plt.subplots(1,3, figsize=(35,10))\n",
                "axs[0].set_title('Original Image', fontsize=35, pad=20)\n",
                "axs[0].matshow(np.resize(X_test_mnist[0], (28, 28)), cmap=\"gray\")\n",
                "axs[1].set_title('Adversarial Perturbed Image', fontsize=35, pad=20)\n",
                "axs[1].matshow(np.resize(X_test_mnist[0]+delta_adv_mtx[0], (28, 28)), cmap=\"gray\")\n",
                "axs[2].set_title('Random Perturbed Image', fontsize=35, pad=20)\n",
                "axs[2].matshow(np.resize(X_test_mnist[0]+delta_random_mtx[0], (28, 28)), cmap=\"gray\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we evalute the kernel ridge regression model by using the adversarial perturbation (constructed on linear models).\n",
                "\n",
                "Try different $\\gamma$, say $\\gamma \\in \\{0.1, 0.01, 0.001, 0.0001\\}$.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.kernel_ridge import KernelRidge\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "clf_kernel_ridge = KernelRidge(alpha=1e-5, kernel='rbf', gamma=0.001)\n",
                "clf_kernel_ridge.fit(X_train_mnist, y_train_mnist)\n",
                "\n",
                "y_predict = np.sign(clf_kernel_ridge.predict(X_test_mnist))\n",
                "print('Accuracy on Original Test Examples: ', accuracy_score(y_test_mnist, y_predict))\n",
                "y_predict_adv = np.sign(clf_kernel_ridge.predict(X_test_mnist + delta_adv_mtx))\n",
                "print('Accuracy on Adversarial Perturbed Examples: ', accuracy_score(y_test_mnist, y_predict_adv))\n",
                "y_predict_random = np.sign(clf_kernel_ridge.predict(X_test_mnist + delta_random_mtx))\n",
                "print('Accuracy on Random Perturbed Examples: ', accuracy_score(y_test_mnist, y_predict_random))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### About finding adversarial examples of the nonlinear model\n",
                "\n",
                "In general, it is not easy to find the close-form solution for nonlinear models compared with the linear model we considered above. For example, if we want to construct adversarial examples for deep neural networks, we could perform projected gradient acsent on the input space iteratively (For more information, refer to this [paper](https://arxiv.org/pdf/1706.06083.pdf).)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Congrats! Hope you learned something from this Jupyter notebook.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "anaconda-cloud": {},
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}