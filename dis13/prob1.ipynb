{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dis 13: Convolutional Neural Networks\n",
    "\n",
    "CS189 Fall 2020  \n",
    "Please feel free to use the Zoom function to call for help or create a queue in [https://oh.eecs189.org/](https://oh.eecs189.org/).  \n",
    "This discussion goes over the convolution operator, what a filter captures and how to handcraft one, the concept of weight sharing, and visualization in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "seaborn.set(font_scale=2)\n",
    "seaborn.set_style(\"white\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install CV2 (OpenCV for Python) for loading images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install PyTorch if needed. This might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Convolution\n",
    "(5 mins)\n",
    "\n",
    "For a starter, let's consider convolution in one dimension. Convolution can be viewed as a function that takes a signal $I$ and a mask $G$ (also called _filter_ or _kernel_), and the discrete convolution at point $t$ of the signal with the mask is\n",
    "$$\n",
    "(I ∗ G)[t] = \\sum_{k=-\\infty}^\\infty I[k]G[t − k]\n",
    "$$\n",
    "If the mask $G$ is nonzero in only a finite range, then the summation can be reduced to just the range in which the mask is nonzero, which makes computing a convolution on a computer possible. The animation below illustrates this operation.\n",
    "\n",
    "<img src=\"1dconv.gif\" width=400 height=400/>\n",
    "\n",
    "(Figure 1: [source](https://e2eml.school/convolution_one_d.html))\n",
    "\n",
    "As an example, we can use convolution to compute a derivative approximation with finite differences. The derivative approximation of the signal is $I′[t] \\approx (I[t + 1] − I[t − 1])/2$. **Design a mask $G$ such that $(I ∗ G)[t] \\approx I′[t]$.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operator can be extended to an input of arbitrary dimension. For example, 2D inputs or images have the following form: \n",
    "$$\n",
    "(I ∗ G)[x, y] = \\sum_{k=-\\infty}^\\infty \\sum_{l=-\\infty}^\\infty I[k, l]G[x − k, y - l]\n",
    "$$\n",
    "\n",
    "The animation shows the input (blue), the output (green), and the 3x3 flipped filter \"sliding\" through the input:\n",
    "\n",
    "<img src=\"2dconv.gif\" width=300 height=300/>\n",
    "\n",
    "(Figure 2: [source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1))\n",
    "\n",
    "\n",
    "The type of mask that we came up with earlier is often used to compute _image gradient_ (not to be confused with gradient descent) which is heavily used in image processing and, particularly, edge detection. _Sobel filter_ is one of the most popular, and it has the following form:\n",
    "\n",
    "$$\n",
    "G_x = \\begin{bmatrix} \n",
    "+1 & 0 & -1 \\\\\n",
    "+2 & 0 & -2 \\\\\n",
    "+1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "\\quad \\mathrm{and} \\quad\n",
    "G_y = \\begin{bmatrix} \n",
    "+1 & +2 & +1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -2 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "[//]: # (**What kind of pattern do you think this filter will get the largest response from when we convolve it with an image?** In other words, think of a 3x3 patch with each pixel between 0 and 1 that maximizes the output of the convolution.\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br/><br/><br/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that this filter is similar to the finite difference approximation above. The filters are used to detect **a vertical and a horizontal edge**, respectively. See the figure below showing the convolution output of the Sobel filters for vertical (bottom right) and horizontal (bottom left) gradients and the magnitude of the gradients (top right). Notice the brick pattern.\n",
    "\n",
    "<img src=\"sobel.png\" width=500 height=500/>\n",
    "\n",
    "(Figure 3: [source](https://en.wikipedia.org/wiki/Sobel_operator))\n",
    "\n",
    "This operation is one of the steps in _Canny edge detection algorithm_ proposed by Professor John Canny (our current CS chair!) in 1986.\n",
    "\n",
    "[//]: # \"Suppose we The output is maximized and minimized (most negative) with the input being \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 & a & 1 \\\\\n",
    "0 & b & 1 \\\\\n",
    "0 & c & 1\n",
    "\\end{bmatrix}\n",
    "\\quad \\mathrm{and} \\quad\n",
    "\\begin{bmatrix} \n",
    "1 & a & 0 \\\\\n",
    "1 & b & 0 \\\\\n",
    "1 & c & 0\n",
    "\\end{bmatrix}\n",
    "\\quad \\mathrm{respectively}\n",
    "$$\n",
    "\n",
    "where $a$, $b$, and $c$ can be any arbitrary constant.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Convolutional neural networks\n",
    "(10 mins)\n",
    "\n",
    "In this part, we will implement a 2D convolution for an image input with only one color channel (grayscale).\n",
    "Note that technically the filter must be flipped along both axes by the convolution operation, but from this point onwards, we will use the neural network convention where the flipping is ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Suppose our input image has a size of 40x40, what is the output size after convolving it with a kernel of size 5x5 with no padding and stride set to 1?**\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br>\n",
    "\n",
    "\n",
    "- **More generally, let the input size be $h$ by $h$ and the kernel size be $k$ by $k$, what is the output size in terms of $h$ and $k$?** You may assume that $k$ is an odd number.\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br>\n",
    "\n",
    "\n",
    "Sometimes we can save computation and take a bigger step when sliding the kernel through the input. This is what _stride_ means. For the convolution we have seen so far, the stride is 1. When the stride is 2, we skip one pixel when sliding the kernel. See the animation below.\n",
    "\n",
    "<img src=\"stride.gif\" width=300 height=300/>\n",
    "\n",
    "(Figure 4: [source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1))\n",
    "\n",
    "Without the stride, the output size would have been 3x3, but in this case, the output size is only 2x2. Roughly, for a stride $s$, the output size is scaled down by a factor of $s$. Using strides saves computation by reducing the number of arithmetic operations as well as the size of the output, which is passed along to the next layer. \n",
    "\n",
    "- **What is the output size in terms of $h$, $k$ and $s$?** (skip this if you are running out of time)\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Next, implement the 2d convolution function below.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(inpt, kernel, stride):\n",
    "    \"\"\"\n",
    "    inpt:   (np.array) input, shape = [height, width, channels]\n",
    "    kernel: (np.array) kernel for convolution (already flipped), \n",
    "            shape = [kernel_height, kernel_width, channels]\n",
    "    stride: (int) stride for both x and y directions\n",
    "    \"\"\"\n",
    "    assert inpt.ndim == 3\n",
    "    assert kernel.ndim == 3\n",
    "    assert inpt.shape[-1] == kernel.shape[-1]\n",
    "    \n",
    "    height = np.floor((inpt.shape[0] - kernel.shape[0]) / stride) + 1\n",
    "    width = np.floor((inpt.shape[1] - kernel.shape[1]) / stride) + 1\n",
    "    height, width = int(height), int(width)\n",
    "    output = np.zeros((height, width))\n",
    "    \n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            ### start convolution ###\n",
    "            \n",
    "            ### end convolution ###\n",
    "    \n",
    "    return output\n",
    "\n",
    "def show_img(img, figsize=None):\n",
    "    if figsize is not None:\n",
    "        plt.figure(figsize=figsize)\n",
    "    if img.ndim == 2:\n",
    "        h, w = img.shape\n",
    "    elif img.ndim == 3:\n",
    "        h, w, _ = img.shape\n",
    "    else:\n",
    "        raise AttributeError('Wrong image dimension.')\n",
    "    ax = plt.gca()\n",
    "    if np.any(img < 0):\n",
    "        # Plot heatmap for response maps or filters\n",
    "        plt.imshow(img.reshape(h, w), cmap='hot', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "    else:\n",
    "        # Plot image normally\n",
    "        plt.imshow(img.reshape(h, w), cmap='gray')\n",
    "    ax.set_xticks(np.arange(-.5, h, 1))\n",
    "    ax.set_yticks(np.arange(-.5, w, 1))\n",
    "    ax.set_xticklabels(np.arange(h + 1), fontsize=10)\n",
    "    ax.set_yticklabels(np.arange(w + 1), fontsize=10)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to apply it to the image below. **What pattern is `kernel1` and `kernel2` below trying to detect? Is the output expected?** Try changing `stride` as well.\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((20, 20, 1))\n",
    "img[5:9, 12:16] = 1\n",
    "img[6:8, 13:15] = 0\n",
    "show_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel1 = np.array([[-1, -1, -1],\n",
    "                    [ 1,  1,  1],\n",
    "                    [-1, -1, -1]])\n",
    "# Reshape to 3x3x1\n",
    "kernel1 = kernel1[:, :, np.newaxis]\n",
    "# Set stride here\n",
    "stride = 1\n",
    "\n",
    "out = conv2d(img, kernel1, stride)\n",
    "show_img(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel2 = np.array([[-1, 1, -1],\n",
    "                    [-1, 1, -1],\n",
    "                    [-1, 1, -1]])\n",
    "# Reshape to 3x3x1\n",
    "kernel2 = kernel2[:, :, np.newaxis]\n",
    "# Set stride here\n",
    "stride = 1\n",
    "\n",
    "out = conv2d(img, kernel2, stride)\n",
    "show_img(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part (c): Design your own filter\n",
    "(15 mins)\n",
    "\n",
    "The example above shows one way to detect horizontal and vertical edges that can be combined to detect the square. Now we will try a different set of filters by using **two** convolutional layers. \n",
    "\n",
    "Generally speaking, for image data or data with 2D structure plus one extra dimension for channels (total of three dimensions), each convolutional layer maps 3D input with shape `[in_height, in_width, in_channels]` to 3D output with shape `[out_height, out_width, out_channels]`. The corresponding filter is a 4D tensor with shape `[kernel_height, kernel_width, in_channels, out_channels]`. \n",
    "\n",
    "This may seem confusing at first, but you can briefly forget about all the `height` and `width` dimensions, and you will see that this is just like a fully connected layer where the weight has shape of `[in_dim, out_dim]`. Similarly to a single parameter in a fully connected layer, each 2D kernel maps one channel of the input to one channel of the output. Thus, you will need `in_dim x out_dim` of these kernels, and whatever maps to the same output channel gets summed together like one neuron. \n",
    "\n",
    "The animation below gives an example of a convolution layer with 3-channel input and 2-channel output. You can see that there are $3 \\times 2 = 6$ of these little 2D kernels, and the outputs of three of them (`Filter W0` or `Filter W1`) are summed up to produce one of the two output channels.\n",
    "\n",
    "<img src=\"4d.gif\" width=600 height=600/>\n",
    "\n",
    "(Figure 5: If you'd like to pause the animation and verify the calculation, please see the [source](https://cs231n.github.io/convolutional-networks/))\n",
    "\n",
    "Another way to visual this process is to see the input and the kernel as a 3D volume or a cube with the same number of channels or depth. With the figure below, the kernel is slided through the input with the 2D convolution, producing one channel of the output. To produce $c$-channel outputs, we need $c$ of these 3D kernels.\n",
    "\n",
    "<img src=\"3d.gif\" width=300 height=300/>\n",
    "\n",
    "(Figure 6: [source](https://predictiveprogrammer.com/famous-convolutional-neural-network-architectures-1/))\n",
    "\n",
    "Once you feel comfortable with the concept of multiple channels. Let's try to apply it. **Design 2 convolutional layers to detect the 4x4 squares in the given image (grayscale or one channel) under the following conditions:**\n",
    "- The first layer will have 4 3x3 kernels (or a 3x3 kernel with 4 channels) to detect the 4 corners of the 4x4 sqaure. A corner is an 'L' shape with height and width of 3 pixels (think about why a smaller 2x2 corner does not work with the second layer with 2x2 kernels). With the notation above, the kernel size is `[3, 3, 1, 4]`.\n",
    "- Then, the second layer has 4 2x2 kernels applied on top of the outputs from the first layer and sums them up to 1 channel. The kernel size is `[2, 2, 4, 1]`.\n",
    "- Just use $-1$, $0$, or $1$ to set the weights in the kernels.\n",
    "- For simplicity, we will also use a ReLU activation function with some fixed bias (4.5) on the first layer. Stride is set to 1.\n",
    "\n",
    "Below is the shape of the input and the outputs we are working with:\n",
    "\n",
    "$$\n",
    "\\textrm{Input:} [32 \\times 32 \\times 1] \\to \\textrm{Output1:} [30 \\times 30 \\times 4] \\to \\textrm{Output2:} [29 \\times 29 \\times 1]\n",
    "$$\n",
    "\n",
    "You have to think about how to detect the corners such that only the 2x2 kernels are sufficient for combining them. Below is the given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 1 - cv2.imread(\"stick.jpg\").mean(-1, keepdims=True) / 255.\n",
    "show_img(img, (7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill in filters for the first layer: `k11, k12, k13, k14`.**\n",
    "\n",
    "When you run the cell, you should see the filters you designed (top row), the convolution output (middle row), and the output after ReLU (bottom row). You should only see a few dots (about 6 - 7) in each of the four outputs in the bottom row. This is the expected output on the bottom row.\n",
    "\n",
    "<img src=\"partc1.png\" width=500 height=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start first_layer ###\n",
    "# Fill in these four filters (starter code)\n",
    "k11 = [[1, 1, 1],\n",
    "       [1, 1, 1],\n",
    "       [1, 1, 1]]\n",
    "k12 = [[1, 1, 1],\n",
    "       [1, 1, 1],\n",
    "       [1, 1, 1]]\n",
    "k13 = [[1, 1, 1],\n",
    "       [1, 1, 1],\n",
    "       [1, 1, 1]]\n",
    "k14 = [[1, 1, 1],\n",
    "       [1, 1, 1],\n",
    "       [1, 1, 1]]\n",
    "### end first_layer ###\n",
    "\n",
    "out1 = []\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15, 10))\n",
    "for i, kernel in enumerate([k11, k12, k13, k14]):\n",
    "    # Convolution\n",
    "    output = conv2d(img, np.array(kernel)[:, :, np.newaxis], 1)\n",
    "    # ReLU activation\n",
    "    output_relu = np.maximum(output - 4.5, 0)\n",
    "    out1.append(output_relu)\n",
    "    \n",
    "    # Plot\n",
    "    axs[0, i].imshow(kernel, cmap='gray')\n",
    "    axs[0, i].set_xticks(np.arange(-.5, 3, 1))\n",
    "    axs[0, i].set_yticks(np.arange(-.5, 3, 1))\n",
    "    axs[0, i].set_xticklabels(np.arange(4), fontsize=10)\n",
    "    axs[0, i].set_yticklabels(np.arange(4), fontsize=10)\n",
    "    axs[0, i].grid()\n",
    "    axs[0, i].set_title('Layer 1, filter %d (k1%d)' %\n",
    "                        (i + 1, i + 1), fontsize=12)\n",
    "    \n",
    "    im1 = axs[1, i].imshow(output, cmap='hot', interpolation='nearest')\n",
    "    cbar = fig.colorbar(im1, ax=axs[1, i])\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    axs[1, i].axis('off')\n",
    "    axs[1, i].set_title('Output %d' % (i + 1), fontsize=12)\n",
    "    \n",
    "    im2 = axs[2, i].imshow(output_relu, cmap='hot', interpolation='nearest')\n",
    "    cbar = fig.colorbar(im2, ax=axs[2, i])\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    axs[2, i].axis('off')\n",
    "    axs[2, i].set_title('Output after ReLU %d' % (i + 1), fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now fill in the filter for the second layer `k2`.**\n",
    "\n",
    "You may write out each individual 2x2 filter as before or just set some of the weights to $1$. After running the code, you should see three high responses at the locations of the three squares. This is the expected output.\n",
    "\n",
    "<img src=\"partc2.png\" width=300 height=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = np.zeros((2, 2, 4))\n",
    "### start second_layer ###\n",
    "\n",
    "### end second_layer ###\n",
    "\n",
    "out2 = 0\n",
    "for i, inpt in enumerate(out1):\n",
    "    out2 += conv2d(inpt[:, :, np.newaxis], k2[:, :, i:i + 1], 1)\n",
    "    \n",
    "show_img(out2, (7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- We have seen a way to handcraft features or patterns of interest. In neural networks and deep learning, the filters are \"learned\" or optimized through gradient descent and backpropgation in the same way as the fully connected layers that you have seen earlier.\n",
    "- This is an example of how early layers detect low-level features such as lines and edges, and the later layers combine this low-level information to locate a high-level pattern such as faces, textures, or a square in this case. We will see in part (e) that this structure is also somtimes learned by neural networks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d) : Why convolutional layers?\n",
    "(5 min)\n",
    "\n",
    "Now we will look into the motivation behind convolutional layers and how they are different from the fully connected layers. \n",
    "\n",
    "### Translational invariance\n",
    "From the previous part, we should have seen that the convolutional layer is very convenient for locating recurring patterns that present anywhere in the image. This property is often referred to as _translational invariance_ or _shift invariance_. This property is automatically enforced by convolutional layers but not by fully connected layers. This design choice or \"architectural\" choice can be regarded as a way humans impose their prior knowledge of the data into building the models. This assumption is particularly suitable to image data. For example, a dog move to the right by three pixels is still a dog. No semantic meaning is changed by translation.\n",
    "\n",
    "### Weight sharing\n",
    "Let's consider the example above with 32x32-pixel inputs and 3x3 kernel with four channels. Suppose that we want to use a fully connected layer to imitate this convolutional layer. Given a fixed input and an output (forget about invariance for now), **how many parameters do we need in the fully connected layer?** **How does it compare to those of the convolutional layer?** You should see that the convolutional layer requires much fewer parameters which save computational costs and prevent overfitting.\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e): Feature visualization\n",
    "(5 min)\n",
    "\n",
    "### Kernel visualization\n",
    "\n",
    "We will first visualize the weights in the kernels of a large neural network trained on the ImageNet dataset. We will be using PyTorch.\n",
    "\n",
    "Build AlexNet on CPU with randomly initialized weights and take only its feature extraction part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import alexnet\n",
    "net = alexnet(False).features.eval().cpu()\n",
    "\n",
    "# This is another network you can play around with\n",
    "# from vgg import vgg16\n",
    "# net = vgg16(False).features.eval().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the architecture of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the feature extraction part. It is a `Sequential` object so you can iterate through each layer just like a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine the parameters (`weight` and `bias`) in each layer and its shape. For convolutional layers, `weight` has shape `[output_channels, input_channels, kernel_height, kernel_width]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net[0].weight.shape)\n",
    "print(net[0].bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the weights of a few filters from the first layer. Since the first layer has three input channels, we can visualize it as color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i, weight in enumerate(net[0].weight[:20]):\n",
    "    kernel = weight.detach().numpy()\n",
    "    # Move the channel dimension to last\n",
    "    kernel = np.moveaxis(kernel, 0, -1)\n",
    "    # Scale to range [0, 1]\n",
    "    kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
    "    axs[i // 10, i % 10].imshow(kernel)\n",
    "    axs[i // 10, i % 10].axis('off')\n",
    "fig.suptitle('Filters from the first layer', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot deeper layers. **Pick other convolutional layers by changing `layer`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a convolutional layer from the indices we printed out earlier\n",
    "layer = 3   # This is the second convolutional layer. Try 3, 6, 8, 10.\n",
    "\n",
    "fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i, weight in enumerate(net[layer].weight[:20]):\n",
    "    kernel = weight[0].detach().numpy()\n",
    "    # Scale to range [0, 1]\n",
    "    kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
    "    axs[i // 10, i % 10].imshow(kernel, cmap='gray')\n",
    "    axs[i // 10, i % 10].axis('off')\n",
    "fig.suptitle('Filters from layer %d' % layer, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build AlexNet with pre-trained weights and examine its weights again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build alexnet with pretrained weights\n",
    "net = alexnet(True).features.eval().cpu()\n",
    "\n",
    "fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i, weight in enumerate(net[0].weight[:20]):\n",
    "    kernel = weight.detach().numpy()\n",
    "    # Move the channel dimension to last\n",
    "    kernel = np.moveaxis(kernel, 0, -1)\n",
    "    # Scale to range [0, 1]\n",
    "    kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
    "    axs[i // 10, i % 10].imshow(kernel)\n",
    "    axs[i // 10, i % 10].axis('off')\n",
    "fig.suptitle('Filters from the first layer', fontsize=16)\n",
    "plt.show()\n",
    "    \n",
    "layer = 3   # This is the second convolutional layer\n",
    "\n",
    "fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i, weight in enumerate(net[layer].weight[:20]):\n",
    "    kernel = weight[0].detach().numpy()\n",
    "    # Scale to range [0, 1]\n",
    "    kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
    "    axs[i // 10, i % 10].imshow(kernel, cmap='gray')\n",
    "    axs[i // 10, i % 10].axis('off')\n",
    "fig.suptitle('Filters from layer %d' % layer, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you notice any difference between the randomly initialized and the pre-trained weights?**\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br><br/><br>\n",
    "\n",
    "\n",
    "Feel free to go back, load VGG model instead, and play around with it. The features are not as obvious there, but they are still pretty interesting to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response maps\n",
    "\n",
    "We can also visualize the response maps or the activations of each convolutional layer. This what you were asked to do in HW13 Q2(d), but here, the network structure is different so the code will also be different. There are also multiple ways to fetch the response maps. This is the way that creates a wrapper (`FeatureVis`) and does not require modifying the original network object, though a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVis(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, net, layer):\n",
    "        \"\"\"\n",
    "        net:   (torch.nn.Sequential) base network\n",
    "        layer: (int) layer to get response map from\n",
    "        \"\"\"\n",
    "        assert layer > 0\n",
    "        super(FeatureVis, self).__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net[0](x)\n",
    "        for i in range(self.layer - 1):\n",
    "            out = self.net[i + 1](out)\n",
    "        return out\n",
    "    \n",
    "# Function for loading and preprocessing images\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    images_orig = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if not 'n0' in filename:\n",
    "            continue\n",
    "        img = cv2.imread(os.path.join(folder, filename)) \n",
    "        if img is not None:         \n",
    "            # Convert to RGB and normalize to [0, 1]\n",
    "            img = img[:,:,::-1] / 255.\n",
    "            # Resize to 224x224\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            images_orig.append(img)\n",
    "            # Standardization\n",
    "            images.append((img - mean) / std)\n",
    "    images = np.moveaxis(np.array(images), -1, 1)\n",
    "    images = torch.from_numpy(images).float()\n",
    "    return images, images_orig\n",
    "\n",
    "# Function for plotting\n",
    "def plot_response_maps(img_index):\n",
    "    # Plot image\n",
    "    plt.imshow(imgs_orig[img_index])\n",
    "    plt.axis('off')\n",
    "    plt.title('Original image', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the response maps\n",
    "    fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
    "    for i, resp in enumerate(responses[img_index][:20]):\n",
    "        resp = resp.detach().numpy()\n",
    "        # Scale to range [0, 1]\n",
    "        resp = (resp - resp.min()) / (resp.max() - resp.min())\n",
    "        axs[i // 10, i % 10].imshow(resp, cmap='gray')\n",
    "        axs[i // 10, i % 10].axis('off')\n",
    "    fig.suptitle('Responses maps', fontsize=16)\n",
    "    \n",
    "slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=11,\n",
    "    step=1,\n",
    "    description='Image:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layer you want to examine here (1, 4, 7)\n",
    "layer = 1\n",
    "\n",
    "vis = FeatureVis(net, layer)\n",
    "imgs, imgs_orig = load_images_from_folder('./')\n",
    "responses = vis(imgs)\n",
    "\n",
    "interactive_plot = interactive(plot_response_maps, img_index=slider)\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the differences between \"shallow\" and \"deep\" response maps?**\n",
    "\n",
    "**Your answer:**\n",
    "<br/><br><br/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other visualization techniques \n",
    "\n",
    "Use backpropagated gradients to visualize important features or parts of the inputs. Historically, this also directly connected to and inspired adversarial examples (see the original paper on adversarial examples on neural networks [link](https://arxiv.org/pdf/1312.6199.pdf)).\n",
    "\n",
    "- Guided backpropagation ([link](https://arxiv.org/pdf/1412.6806.pdf)): \n",
    "\n",
    "<img src=\"guidebprop.png\" width=500 height=500/>\n",
    "\n",
    "- Grad-CAM and Guided Grad-CAM ([link](https://arxiv.org/pdf/1610.02391.pdf)):\n",
    "\n",
    "<img src=\"visual.png\" width=500 height=500/>\n",
    "\n",
    "### Texture bias\n",
    "\n",
    "Recent research finds that sometimes neural networks do not learn the features that we expect or features that humans do not necessarily use to the same prediction. Geirhos et al. (2019) ([link](https://openreview.net/pdf?id=Bygh9j09KX)) shows that neural networks are biased towards textures for classification rather than shapes or outlines.\n",
    "\n",
    "<img src=\"texture.png\" width=500 height=500/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
