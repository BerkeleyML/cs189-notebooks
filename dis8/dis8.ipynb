{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Discussion: neural networks and gradient descent\n",
                "\n",
                "The high level view for today:\n",
                "- Neural networks are a learning model, i.e., a function parametrized in a certain way\n",
                "- The parameters of the neural network are the weights between nodes, the $w_{ji}^{(\\ell)}$\n",
                "- Things like the number of layers and the nonlinearities used to compute the output of the layers (e.g., tanh or relu) are *hyperparameters*\n",
                "- Gradient descent is an iterative mechanism to learn the $w_{ji}^{(\\ell)}$ from data\n",
                "- Backpropagation is an algorithm used to compute the gradients of the *error function* with respect to the weights\n",
                "- Things like *mini-batch gradient descent* and *stochastic gradient descent* are techniques used to make faster updates to the weights\n",
                "\n",
                "\n",
                "### Neural networks\n",
                "- A neural network is a function $h : \\mathcal{X} \\to \\mathcal{Y}$ that we can represent using an acyclic (for now!) directed graph\n",
                "- Suppose the input space is $d$-dimensional. Then $\\mathcal{X} = \\mathbb{R}^d$\n",
                "- The inputs, which we label $x^{(0)} \\in \\mathbb{R}^d$, have no incoming edges. At the zeroth layer, we have $d^{(0)} = d$ nodes\n",
                "- The nodes with edges from the inputs are said to belong to the first layer\n",
                "- The nodes with edges from the $(\\ell -1)$-th layer are said to belong to the $\\ell$-th layer\n",
                "- The edge from the $i$-th node of the $(\\ell -1)$-th to the $j$-th node of the $\\ell$-layer is annotated with a weight $w_{ji}^{(\\ell)}$\n",
                "- Suppose at the $\\ell$-th layer we have $d^{(\\ell)}$ nodes\n",
                "\n",
                "- Here we have an example of a neural network with two inputs and two layers: one with three nodes, and the output layer with one node\n",
                "<img src=\"nnSample.png\" />\n",
                "\n",
                "\n",
                "- The output $x_j^{(\\ell)}$ of the $j$-th node at the $\\ell$-th layer is given by\n",
                "\n",
                "$x_j^{(\\ell)} = g\\left(s_j^{(\\ell)}\\right)$, where $s_j^{(\\ell)} = \\sum_{i = 1}^{d^{(\\ell - 1)}} w_{ji}^{(\\ell)} x_i^{(\\ell - 1)}$,\n",
                "\n",
                "where $g$ is a nonlinear function called *activation function*, usually tanh or relu\n",
                "\n",
                "- If we think of the $w_{ji}^{(\\ell)}$ as the elements of a matrix $w^{(\\ell)}$ of size $d^{(\\ell)} \\times d^{(\\ell - 1)}$, we can write\n",
                "\n",
                "$s^{(\\ell)} = w^{(\\ell)} x^{(\\ell - 1)}$\n",
                "\n",
                "- Observe that to compute the output of a node at layer $\\ell$ we need to compute the outputs of all previous layers $1, \\ldots, \\ell - 1$. Thus evaluation of the nodes in the neural network propagates forward\n",
                "\n",
                "- **How many weights do we have to train in a neural network with $6$ inputs and $3$ layers with $d^{(1)} = 3$, $d^{(2)} = 4$, and $d^{(3)} = 2$?**\n",
                "- **What is the computational cost of evaluating a function implemented as a neural network with $L$ layers and $d^{(\\ell)}$ nodes in layer $\\ell$?**\n",
                "\n",
                "### The error loss\n",
                "- We need a function to quantify how well we are doing. This is the error loss\n",
                "- The error loss $e$ is used to adjust the weights through gradient descent. We update the weights according to the rule\n",
                "\n",
                "$w_{ji}^{(\\ell)} \\gets w_{ji}^{(\\ell)} - \\eta \\frac{\\partial e}{\\partial w_{ji}^{(\\ell)}}$,\n",
                "\n",
                "where $\\eta$ is a hyperparameter called *learning rate* and *step size*. Note that this rule updates the weights in the direction that decreases the error\n",
                "- We train the neural network using $N$ samples $(x_i, y_i)$, where $x_i \\in \\mathbb{R}^{d^{(0)}}$ and $y_i \\in \\mathbb{R}^{d^{(L)}}$\n",
                "- Suppose $e_p$ allows us to evaluate the contribution to the total error loss from a single sample $(x_i, y_i)$\n",
                "- We have a choice. We could update the weights by minimizing the error for all training samples:\n",
                "\n",
                "$e = \\frac{1}{N} \\sum_{i = 1}^N e_p \\left(h(x_i), y_i\\right)$\n",
                "\n",
                "This is called *batch gradient descent*\n",
                "\n",
                "A variant of this method is to choose random subsets of the samples and make an update:\n",
                "\n",
                "$e = \\frac{1}{|S|} \\sum_{i \\in S} e_p \\left(h(x_i), y_i\\right)$,\n",
                "\n",
                "where $S \\subseteq \\{1, \\ldots, N\\}$. This is called *mini-batch gradient descent*. Of course, at each individual step, we keep varying $S$ in order to make use of all our data\n",
                "\n",
                "Another option is to just pick a random sample $(x_r, y_r)$, compute the gradient there, and update the weights. In that case, the error would be\n",
                "\n",
                "$e = e_p \\left(h(x_r), y_r\\right)$\n",
                "\n",
                "This goes by the name *stochastic gradient descent*\n",
                "\n",
                "- **Based on what you understand now, what advantages and disadvantages do you imagine the three methods above have?**\n",
                "\n",
                "### Updating the weights\n",
                "- The algorithm used to compute $\\frac{\\partial e}{\\partial w_{ji}^{(\\ell)}}$ is Backpropagation\n",
                "- We define\n",
                "\n",
                "$\\delta_j^{(\\ell)} = \\frac{\\partial e}{\\partial s_j^{(\\ell)}}$\n",
                "\n",
                "Remember that $s^{(\\ell)} = w^{(\\ell)} x^{(\\ell - 1)}$\n",
                "\n",
                "- This quantity is directly related to the gradient we want. The relation is\n",
                "\n",
                "$\\frac{\\partial e}{\\partial w_{ji}^{(\\ell)}} = \\delta_j^{(\\ell)} x_i^{(\\ell - 1)}$\n",
                "\n",
                "- We already have all the $x_i^{(\\ell - 1)}$ as a result of forward propagation (evaluation of the neural network)\n",
                "- Now, observe that we can directly compute $\\delta^{(L)}$. To compute $\\delta$ at other layers, we use the expression\n",
                "\n",
                "$\\delta_i^{(\\ell)} = g'\\left(s_i^{(\\ell)}\\right) \\sum_{j = 1}^{d^{(\\ell + 1)}} w_{ji}^{(\\ell + 1)} \\delta_j^{(\\ell + 1)}$\n",
                "\n",
                "Thus, to compute $\\delta^{(\\ell)}$, we need to compute $\\delta^{(k)}$ for $\\ell + 1 \\le k \\le L$\n",
                "- In matrix notation, we have\n",
                "\n",
                "$\\delta^{(\\ell)} = g'\\left(s^{(\\ell)}\\right) \\circ \\left( w^{(\\ell + 1)} \\right)^{\\top} \\delta^{(\\ell + 1)}$,\n",
                "\n",
                "where $\\circ$ stands for the Hadamard (i.e., element-wise) product\n",
                "\n",
                "- The following class defines a neural network implementing the equations we just discussed. Observe that we are coding the neural network from scratch (i.e., we are not using ML libraries)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "class neuralNetwork:\n",
                "\n",
                "    # nodesPerLayer is an array containing the number of nodes for each layer: If we pass [2,3], we get a 1-layer NN. The input layer will contain 2 nodes; the first layer 3 nodes.\n",
                "    def __init__(self, nodesPerLayer, localFunc='relu'):\n",
                "        L = len(nodesPerLayer) - 1\n",
                "        self.nodesPerLayer = nodesPerLayer\n",
                "        self.L = L\n",
                "        self.x = {i:{} for i in range(0,L+1)}\n",
                "        self.s = {i:{} for i in range(1,L+1)}\n",
                "        self.W = {i:1.0/(nodesPerLayer[i-1])*np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) for i in range(1,L+1)}\n",
                "        self.W_grad = {i:np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1,L+1)}\n",
                "        self.delta = {i:{} for i in range(1,L+1)}\n",
                "        # Activation function g\n",
                "        if localFunc == 'relu':\n",
                "            # The following two lines are wrong! They just implement the identity function\n",
                "            self.innerLayerActivation = lambda a: a\n",
                "            self.innerLayerActivationDer = lambda a : 1.0\n",
                "            # Add code to correctly compute relu and its derivative\n",
                "            ### start relu ###\n",
                "\n",
                "            ### end relu ###\n",
                "        elif localFunc == 'tanh':\n",
                "            self.innerLayerActivation = np.tanh\n",
                "            self.innerLayerActivationDer = lambda a : 1.0 - np.tanh(a)**2\n",
                "        else:\n",
                "            assert False\n",
                "        # activation function for output layers. Notice this is just the identity\n",
                "        self.outLayerActivation = lambda a : a\n",
                "        self.outLayerActivationDer = lambda a : 1.0\n",
                "        # Pointwise error e_p\n",
                "        self.errorFunc = lambda a, b: 0.5*(a - b)**2\n",
                "        self.errorFuncDer = lambda a, b: (a - b)\n",
                "\n",
                "    # Here we go from input to output in order to evaluate the function h\n",
                "    def forwardPropagate(self, xSamples):\n",
                "        (d,n) = xSamples.shape\n",
                "        assert d == self.nodesPerLayer[0]\n",
                "        # This is the zero-th layer. It consists of the inputs\n",
                "        self.x[0] = xSamples\n",
                "        L = self.L\n",
                "        for k in range(1, L):\n",
                "            # This is the k-th layer\n",
                "            self.s[k] = self.W[k] @ self.x[k-1]\n",
                "            self.x[k] = self.innerLayerActivation(self.s[k])\n",
                "        # output layer activation\n",
                "        self.s[L] = self.W[L] @ self.x[L-1]\n",
                "        self.x[L] = self.outLayerActivation(self.s[L])\n",
                "\n",
                "    # Here we go from output to input in order to compute gradients\n",
                "    def backPropagate(self, ySamples):\n",
                "        (d,n) = ySamples.shape\n",
                "        # base step: set the deltas at the last layer\n",
                "        self.delta[self.L]  = self.outLayerActivationDer(self.s[self.L]) * self.errorFuncDer(self.x[self.L], ySamples)\n",
                "        self.W_grad[self.L] = 1.0/n * (self.delta[self.L] @ self.x[self.L-1].transpose())\n",
                "        # now we work our way backwards\n",
                "        for k in reversed(range(1,self.L)):\n",
                "            self.delta[k] = self.innerLayerActivationDer(self.s[k]) * (self.W[k+1].transpose() @ self.delta[k+1])\n",
                "            self.W_grad[k] = 1.0/n * (self.delta[k] @ self.x[k-1].transpose())\n",
                "\n",
                "    # Given a set of n xSamples and ySamples, adjust the weights through backprop\n",
                "    def adjustWeights(self, xSamples, ySamples, learningRate):\n",
                "        self.forwardPropagate(xSamples)\n",
                "        self.backPropagate(ySamples)\n",
                "        for k in range(1,self.L+1):\n",
                "            self.W[k] -= learningRate*self.W_grad[k]\n",
                "\n",
                "    def evaluate(self, xSamples):\n",
                "        self.forwardPropagate(xSamples)\n",
                "        return self.x[self.L]\n",
                "\n",
                "    def getError(self, xSamples, ySamples):\n",
                "        predictions = self.evaluate(xSamples)\n",
                "        return self.errorFunc(predictions, ySamples).mean()\n",
                "\n",
                "    def getGradients(self):\n",
                "        return self.W_grad\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- **Looking at the code above, how do you think we can implement batch gradient descent vs. stochastic gradient descent vs. mini batch gradient descent? In other words, how should we call `adjustWeights`?**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup for example\n",
                "\n",
                "- Throughout this discussion, we will learn a 1D function $f(x) = x^2 + \\text{noise}$\n",
                "- The following cell sets up our training data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "from matplotlib import animation, rc\n",
                "from IPython.display import HTML\n",
                "from copy import copy\n",
                "\n",
                "seed = 100\n",
                "np.random.seed(seed)\n",
                "xSamples = np.reshape(np.linspace(-10, 10, 100), (1,100))\n",
                "ySamples = xSamples ** 2 + 0.1*np.random.randn(xSamples.shape[0], xSamples.shape[1])\n",
                "\n",
                "fig, ax = plt.subplots(1,1)\n",
                "ax.set_title('Data we will approximate')\n",
                "ax.set_xlabel(r'$x$'); ax.set_ylabel(r'$y$')\n",
                "ax.plot(xSamples[0,:],ySamples[0,:], '.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- We will train a neural network with 2 inputs, 4 hidden nodes in a single layer, and 1 output. We use one of our 2 inputs as a constant 1\n",
                "- Our objective is to closely observe the neural network as it is trained\n",
                "- The following cell sets up the plots that we will visualize\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PLOTS\n",
                "fig, ax = plt.subplots(3,2,figsize=(12,16));\n",
                "\n",
                "# first plot: original function and approximation\n",
                "ax[0,0].set_xlim(( -10, 10))\n",
                "ax[0,0].set_ylim((0, 100))\n",
                "origData, = ax[0,0].plot([], [], '.')\n",
                "nnApprox, = ax[0,0].plot([], [], lw=2)\n",
                "ax[0,0].legend(['Original data', 'Neural network'])\n",
                "ax[0,0].set_xlabel(r'$x$')\n",
                "ax[0,0].set_ylabel(r'$y$')\n",
                "# second plot: error vs. iterations\n",
                "ax[0,1].set_xlim(( -100, 10000))\n",
                "ax[0,1].set_ylim((0.5, 3.5))\n",
                "errPlot, = ax[0,1].plot([], [], lw=2)\n",
                "ax[0,1].set_title('Approximation error')\n",
                "ax[0,1].set_xlabel('Iterations')\n",
                "fig.canvas.draw()\n",
                "labels = [r\"$10^{\"+i.get_text()+\"}$\" for i in ax[0,1].get_yticklabels()]\n",
                "ax[0,1].set_yticklabels(labels)\n",
                "ax[0,1].set_ylabel('Training error')\n",
                "# third plot: gradients in first layer (first argument)\n",
                "ax[1,0].set_xlim(( -100, 10000))\n",
                "ax[1,0].set_ylim((-20, 20))\n",
                "lineL1G1 = [ax[1,0].plot([],[],lw=1)[0] for i in range(5)]\n",
                "ax[1,0].set_title('Gradients in first layer')\n",
                "ax[1,0].set_xlabel('Iterations')\n",
                "ax[1,0].set_ylabel('Gradient values')\n",
                "legend = [r'$\\frac{\\partial e}{\\partial w_{'+str(i)+'1}^{(1)}}$' for i in range(1,5)]\n",
                "legend += [r'$\\Vert \\frac{\\partial e}{\\partial w_{*1}^{(1)}} \\Vert$']\n",
                "ax[1,0].legend(legend, prop={'size': 16})\n",
                "# fourth plot: gradients in first layer (second argument)\n",
                "ax[1,1].set_xlim(( -100, 10000))\n",
                "ax[1,1].set_ylim((-20, 20))\n",
                "lineL1G2 = [ax[1,1].plot([],[],lw=1)[0] for i in range(5)]\n",
                "ax[1,1].set_title('Gradients in first layer')\n",
                "ax[1,1].set_xlabel('Iterations')\n",
                "ax[1,1].set_ylabel('Gradient values')\n",
                "legend = [r'$\\frac{\\partial e}{\\partial w_{'+str(i)+'2}^{(1)}}$' for i in range(1,5)]\n",
                "legend += [r'$\\Vert \\frac{\\partial e}{\\partial w_{*2}^{(1)}} \\Vert$']\n",
                "ax[1,1].legend(legend, prop={'size': 16},loc='upper right')\n",
                "# fifth plot: gradients in second layer\n",
                "ax[2,0].set_xlim(( -100, 10000))\n",
                "ax[2,0].set_ylim((-20, 20))\n",
                "lineL2G = [ax[2,0].plot([],[],lw=1)[0] for i in range(5)]\n",
                "ax[2,0].set_title('Gradients in second layer')\n",
                "ax[2,0].set_xlabel('Iterations')\n",
                "ax[2,0].set_ylabel('Gradient values')\n",
                "legend = [r'$\\frac{\\partial e}{\\partial w_{1'+str(i)+'}^{(2)}}$' for i in range(1,5)]\n",
                "legend += [r'$\\Vert \\frac{\\partial e}{\\partial w_{1*}^{(2)}} \\Vert$']\n",
                "ax[2,0].legend(legend, prop={'size': 16},loc='upper right')\n",
                "# sixth plot: weights in second layer\n",
                "ax[2,1].set_xlim(( -100, 10000))\n",
                "ax[2,1].set_ylim((-1, 12))\n",
                "lineL2W = [ax[2,1].plot([],[],lw=1)[0] for i in range(4)]\n",
                "ax[2,1].set_title('Weights in second layer')\n",
                "ax[2,1].set_xlabel('Iterations')\n",
                "ax[2,1].set_ylabel('Weight values')\n",
                "ax[2,1].legend([r'$w_{1'+str(i)+'}^{(2)}$' for i in range(1,5)],\n",
                "               prop={'size': 10},loc=\"upper left\")\n",
                "\n",
                "\n",
                "def init():\n",
                "    nnApprox.set_data([], [])\n",
                "    origData.set_data([], [])\n",
                "    errPlot.set_data([], [])\n",
                "    for thisLine in lineL1G1: thisLine.set_data([], [])\n",
                "    for thisLine in lineL1G2: thisLine.set_data([], [])\n",
                "    for thisLine in lineL2G:   thisLine.set_data([], [])\n",
                "    for thisLine in lineL2W:   thisLine.set_data([], [])\n",
                "    return [nnApprox,origData,errPlot] + lineL1G1 + lineL1G2 + lineL2G + lineL2W\n",
                "\n",
                "def animate(i):\n",
                "    m = 1 if i <10 else 100\n",
                "    for j in range(m):\n",
                "        error.append(np.log10(nNwk.getError(newX, ySamples)))\n",
                "        if descent == 'batch':\n",
                "            # use the entire training set to update the weights\n",
                "            nNwk.adjustWeights(newX, ySamples, eta)\n",
                "        elif descent == 'sgd':\n",
                "            # use one training sample to update the weights\n",
                "            (d,n) = newX.shape\n",
                "            sampleIndex = np.random.randint(n)\n",
                "            nNwk.adjustWeights(newX[:,[sampleIndex]], ySamples[:,[sampleIndex]], eta)\n",
                "        elif descent == 'minibatch':\n",
                "            # use several training samples to update the weights\n",
                "            (d,n) = newX.shape\n",
                "            sampleIndex = np.random.choice(n, miniBatchSize)\n",
                "            nNwk.adjustWeights(newX[:,sampleIndex], ySamples[:,sampleIndex], eta)\n",
                "        else:\n",
                "            assert False\n",
                "        gradW1 = nNwk.getGradients()[1]\n",
                "        gradL1.append(gradW1)\n",
                "        gradW2 = nNwk.getGradients()[2]\n",
                "        gradL2.append(gradW2)\n",
                "        W2.append(copy(nNwk.W[2]))\n",
                "    outputs = nNwk.evaluate(newX)\n",
                "    #plt.plot(np.transpose(xSamples), np.transpose(outputs))\n",
                "\n",
                "    x = xSamples[0,:]\n",
                "    y = outputs[0,:]\n",
                "    nnApprox.set_data(x, y)\n",
                "    origData.set_data(xSamples, ySamples)\n",
                "    errPlot.set_data([k for k in range(1,len(error)+1)], error)\n",
                "    # first layer gradients\n",
                "    grads = np.array(gradL1)\n",
                "    for j, thisLine in enumerate(lineL1G1):\n",
                "        if j < len(lineL1G1) - 1:\n",
                "            yVals = grads[:,j,0]\n",
                "        else:\n",
                "            yVals = np.linalg.norm(grads[:,:,[0]],axis=1)[:,0]\n",
                "        thisLine.set_data([k for k in range(1,len(yVals)+1)], yVals)\n",
                "\n",
                "    for j, thisLine in enumerate(lineL1G2):\n",
                "        if j < len(lineL1G2) - 1:\n",
                "            yVals = grads[:,j,1]\n",
                "        else:\n",
                "            yVals = np.linalg.norm(grads[:,:,[1]],axis=1)[:,0]\n",
                "        thisLine.set_data([k for k in range(1,len(yVals)+1)], yVals)\n",
                "\n",
                "    # second layer gradients\n",
                "    grads = np.array(gradL2)\n",
                "    for j, thisLine in enumerate(lineL2G):\n",
                "        if j < len(lineL2G) - 1:\n",
                "            yVals = grads[:,0,j]\n",
                "        else:\n",
                "            yVals = np.linalg.norm(grads[:,[0],:],axis=2)[:,0]\n",
                "        thisLine.set_data([k for k in range(1,len(yVals)+1)], yVals)\n",
                "\n",
                "    # second layer weights\n",
                "    weights = np.array(W2)\n",
                "    for j, thisLine in enumerate(lineL2W):\n",
                "        yVals = weights[:,0,j]\n",
                "        thisLine.set_data([k for k in range(1,len(yVals)+1)], yVals)\n",
                "\n",
                "    return [nnApprox,origData,errPlot] + lineL1G1 + lineL1G2 + lineL2G + lineL2W\n",
                "\n",
                "\n",
                "rc('animation', html='html5')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Batch gradient descent\n",
                "\n",
                "- Now we will observe how a neural network learns a 1D function $f(x) = x^2 + \\text{noise}$ using batch gradient descent\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# build a neural network with 2 input layers, some middle layers, and 1 output layer\n",
                "seed = 10\n",
                "np.random.seed(seed)\n",
                "nNwk = neuralNetwork([2,4,1])\n",
                "newX = np.concatenate([np.ones(xSamples.shape), xSamples], axis=0)\n",
                "error = []\n",
                "gradL1 = []; gradL2 = []\n",
                "W2 = []\n",
                "descent = 'batch'\n",
                "eta = 0.001\n",
                "ax[1,0].set_ylim((-20, 20))\n",
                "ax[1,1].set_ylim((-20, 20))\n",
                "ax[2,0].set_ylim((-20, 20))\n",
                "\n",
                "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
                "                               frames=100, interval=50, blit=True)\n",
                "\n",
                "anim\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig.savefig('gradientDescDisc-GD-a.png')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Considering the animations above, we can identify three regions in the plot of the training error. **What is the significance of those regions?**\n",
                "- The only randomness of the training algorithm is in the initialization of the weights of the neural network. **Change the random seed using the following cell, execute the code, and discuss what happens**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Now using a different seed\n",
                "newSeed = 10\n",
                "### start seed ###\n",
                "\n",
                "### end seed ###\n",
                "np.random.seed(newSeed)\n",
                "nNwk = neuralNetwork([2,4,1])\n",
                "newX = np.concatenate([np.ones(xSamples.shape), xSamples], axis=0)\n",
                "error = []\n",
                "gradL1 = []; gradL2 = []\n",
                "W2 = []\n",
                "descent = 'batch'\n",
                "eta = 0.001\n",
                "ax[1,0].set_ylim((-20, 20))\n",
                "ax[1,1].set_ylim((-20, 20))\n",
                "ax[2,0].set_ylim((-20, 20))\n",
                "\n",
                "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
                "                               frames=100, interval=50, blit=True)\n",
                "\n",
                "anim\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig.savefig('gradientDescDisc-GD-b.png')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stochastic gradient descent\n",
                "\n",
                "- We will learn the same function as before, but this time we will use stochastic gradient descent\n",
                "- This means that we will update gradients using a single sample chosen at random\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 10\n",
                "np.random.seed(seed)\n",
                "nNwk = neuralNetwork([2,4,1])\n",
                "descent = 'sgd'\n",
                "eta = 0.001\n",
                "error = []\n",
                "gradL1 = []; gradL2 = []\n",
                "W2 = []\n",
                "ax[1,0].set_ylim((-1000, 1000))\n",
                "ax[1,1].set_ylim((-1000, 1000))\n",
                "ax[2,0].set_ylim((-1000, 1000))\n",
                "\n",
                "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
                "                               frames=100, interval=50, blit=True)\n",
                "\n",
                "anim\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig.savefig('gradientDescDisc-SGD-a.png')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- **The following cell fits a naural network using the seed you chose for batch gradient descent. Generate the visualization and discuss what happens**\n",
                "- **How does the behavior of stochastic gradient descent compare with that of gradient descent?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use different seeds below\n",
                "np.random.seed(newSeed)\n",
                "nNwk = neuralNetwork([2,4,1])\n",
                "descent = 'sgd'\n",
                "eta = 0.001\n",
                "error = []\n",
                "gradL1 = []; gradL2 = []\n",
                "W2 = []\n",
                "ax[1,0].set_ylim((-1000, 1000))\n",
                "ax[1,1].set_ylim((-1000, 1000))\n",
                "ax[2,0].set_ylim((-1000, 1000))\n",
                "\n",
                "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
                "                               frames=100, interval=50, blit=True)\n",
                "\n",
                "anim\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig.savefig('gradientDescDisc-SGD-b.png')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mini-batch gradient descent\n",
                "\n",
                "- Now we will use mini-batch gradient descent to learn the same data as before\n",
                "- This means that we will update gradients using several samples chosen at random\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 10\n",
                "np.random.seed(seed)\n",
                "nNwk = neuralNetwork([2,4,1])\n",
                "descent = 'minibatch'\n",
                "miniBatchSize = 20\n",
                "\n",
                "eta = 0.001\n",
                "error = []\n",
                "gradL1 = []; gradL2 = []\n",
                "W2 = []\n",
                "ax[1,0].set_ylim((-50, 50))\n",
                "ax[1,1].set_ylim((-500, 500))\n",
                "ax[2,0].set_ylim((-500, 500))\n",
                "\n",
                "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
                "                               frames=100, interval=50, blit=True)\n",
                "\n",
                "anim\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig.savefig('gradientDescDisc-minibatch-a.png')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Now you have seen the three descent strategies. **Would you be surprised if someone told you that mini-batch gradient descent is the most popular technique? Why?**\n",
                "- **Finally, add more hidden nodes to the neural network using the code below and see what happens**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up the plots\n",
                "fig, ax = plt.subplots(1,2,figsize=(12,4));\n",
                "\n",
                "# first plot: original function and approximation\n",
                "ax[0].set_xlim(( -10, 10))\n",
                "ax[0].set_ylim((0, 100))\n",
                "origData, = ax[0].plot([], [], '.')\n",
                "nnApprox, = ax[0].plot([], [], lw=2)\n",
                "ax[0].legend(['Original data', 'Neural network'])\n",
                "ax[0].set_xlabel(r'$x$')\n",
                "ax[0].set_ylabel(r'$y$')\n",
                "# second plot: error vs. iterations\n",
                "ax[1].set_xlim(( -100, 10000))\n",
                "ax[1].set_ylim((-1.0, 3.5))\n",
                "errPlot, = ax[1].plot([], [], lw=2)\n",
                "ax[1].set_title('Approximation error')\n",
                "ax[1].set_xlabel('Iterations')\n",
                "fig.canvas.draw()\n",
                "labels = [r\"$10^{\"+i.get_text()+\"}$\" for i in ax[1].get_yticklabels()]\n",
                "ax[1].set_yticklabels(labels)\n",
                "ax[1].set_ylabel('Training error')\n",
                "\n",
                "\n",
                "def init():\n",
                "    nnApprox.set_data([], [])\n",
                "    origData.set_data([], [])\n",
                "    errPlot.set_data([], [])\n",
                "    return [nnApprox,origData,errPlot]\n",
                "\n",
                "def animate(i):\n",
                "    m = 1 if i <10 else 100\n",
                "    for j in range(m):\n",
                "        error.append(np.log10(nNwk.getError(newX, ySamples)))\n",
                "        if descent == 'batch':\n",
                "            # use the entire training set to update the weights\n",
                "            nNwk.adjustWeights(newX, ySamples, eta)\n",
                "        elif descent == 'sgd':\n",
                "            # use one training sample to update the weights\n",
                "            (d,n) = newX.shape\n",
                "            sampleIndex = np.random.randint(n)\n",
                "            nNwk.adjustWeights(newX[:,[sampleIndex]], ySamples[:,[sampleIndex]], eta)\n",
                "        elif descent == 'minibatch':\n",
                "            # use several training samples to update the weights\n",
                "            (d,n) = newX.shape\n",
                "            sampleIndex = np.random.choice(n, miniBatchSize)\n",
                "            nNwk.adjustWeights(newX[:,sampleIndex], ySamples[:,sampleIndex], eta)\n",
                "        else:\n",
                "            assert False\n",
                "        gradW1 = nNwk.getGradients()[1]\n",
                "        gradL1.append(gradW1)\n",
                "        gradW2 = nNwk.getGradients()[2]\n",
                "        gradL2.append(gradW2)\n",
                "        W2.append(copy(nNwk.W[2]))\n",
                "    outputs = nNwk.evaluate(newX)\n",
                "    #plt.plot(np.transpose(xSamples), np.transpose(outputs))\n",
                "\n",
                "    x = xSamples[0,:]\n",
                "    y = outputs[0,:]\n",
                "    nnApprox.set_data(x, y)\n",
                "    origData.set_data(xSamples, ySamples)\n",
                "    errPlot.set_data([k for k in range(1,len(error)+1)], error)\n",
                "\n",
                "    return [nnApprox,origData,errPlot]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# now carry out the approximation. Change the number of layers below\n",
                "seed = 10\n",
                "np.random.seed(seed)\n",
                "\n",
                "nNwk = neuralNetwork([2,4,1])\n",
                "### start moreLayers ###\n",
                "\n",
                "### end moreLayers ###\n",
                "descent = 'minibatch'\n",
                "miniBatchSize = 20\n",
                "\n",
                "eta = 0.001\n",
                "error = []\n",
                "\n",
                "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
                "                               frames=100, interval=50, blit=True)\n",
                "\n",
                "anim\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig.savefig('gradientDescDisc-minibatch-moreLayers.png')\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}