{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 3: Probabilistic View of Linear Regression\n",
    "\n",
    "[CS189/289A - Fall 2020]\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome! This discussion material will cover parts of Problem 2 and 4 in HW3. It aims to give you a better understanding of an interpretation of regression as a probabilistic model and should get you started on the homework problems. We will first start with this Jupyter notebook and play around with multivariate Gaussian distribution (2D). We will consider a linear regression problem from a probabilistic view and examine the likelihood function and the posterior function.\n",
    "\n",
    "## (a) Isocontour of Gaussian PDF\n",
    "\n",
    "We will warm-up by getting all of you more familiar with multivariate Gaussian distribution by observing the contour of its density in 2D. Run the following code and use adjust the following parameters: mean of x and y coordinates (`mu_x` and `mu_y`), and its covariance matrix, $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$, given by\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\Sigma_{xx} & \\Sigma_{xy} \\\\\n",
    "\\Sigma_{xy} & \\Sigma_{yy}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For some values of $\\Sigma$, you may get a matrix that is no longer positive semidefinite. If that happens, move $\\Sigma_{xy}$ closer to zero or just try a different set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(mu_x, mu_y, s_xx, s_yy, s_xy):\n",
    "    # Generate grid of points at which to evaluate pdf\n",
    "    x = np.linspace(-5, 5, 500)\n",
    "    y = np.linspace(-5, 5, 500)\n",
    "    X,Y = np.meshgrid(x, y)\n",
    "    pos = np.array([Y, X]).T\n",
    "    S = [[s_xx, s_xy], [s_xy, s_yy]]\n",
    "    rv = scipy.stats.multivariate_normal([mu_x, mu_y], S)\n",
    "    Z = rv.pdf(pos)\n",
    "    \n",
    "    e, v = np.linalg.eig(S)\n",
    "    v = v.T * np.sqrt(e.reshape(2, 1))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(X, Y, Z)\n",
    "    plt.arrow(mu_x, mu_y, v[0, 0], v[0, 1], width=0.1, color='r')\n",
    "    plt.arrow(mu_x, mu_y, v[1, 0], v[1, 1], width=0.1, color='r')\n",
    "    plt.colorbar()\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(plot_contour, mu_x=(-4, 4), mu_y=(-4, 4), \n",
    "                               s_xx=(0.1, 5), s_yy=(0.1, 5), s_xy=(-5, 5))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "- __What happens when you change the mean__\n",
    "- __What happens when you change the diagonal entries of the covariance matrix? (also notice the color bar on the right)__\n",
    "- __What happens when you change the off-diagonal entries?__\n",
    "\n",
    "The red arrows show the two orthogonal directions with the largest and the smallest variance. They can be seen as the transformed coordinates! If you have data points drawn from a standard Gaussian distribution (i.e. $\\Sigma = I$), they can be linearly transformed as if they are drawn from any Gaussian distribution with covariance matrix $\\Sigma$. The transformation matrix is given by \n",
    "\n",
    "$$T = VS^{1/2}$$\n",
    "\n",
    "where $V$ is the eigenvectors of $\\Sigma$, and $S$ is the diagonal matrix of eigenvalues of $\\Sigma$. In other words, SVD of the covariance matrix $\\Sigma$ is given by\n",
    "\n",
    "$$\\Sigma = VSV^\\top = TT^\\top$$\n",
    "\n",
    "The red arrows are then generated by transforming the standard unit vectors, $[0, 1], [1, 0]$, by the transformation matrix $T$.\n",
    "\n",
    "__Why does this make sense mathematically?__ Consider PDF of an standard Gaussian: $f(x) = \\frac{1}{2\\pi} \\exp\\left\\{\\|x\\|_2^2\\right\\}$. We want to find PDF of the transformed distribution $g(z)$ where $z = Tx$. More generally, for any PDF $f(x)$, $g(z)$ is given by\n",
    "\n",
    "$$g(z) = f(x(z)) \\left| \\det\\left( \\frac{dx}{dz} \\right)\\right|$$\n",
    "\n",
    "Now see what $g(z)$ looks like for our Gaussian distribution.\n",
    "\\begin{align}\n",
    "g(z) &= \\frac{1}{2\\pi} \\left| \\det{T^{-1}} \\right| \\cdot \\exp\\left\\{\\|T^{-1}z\\|_2^2\\right\\} \\\\\n",
    "&= \\frac{1}{2\\pi} \\left| \\frac{1}{\\det{T}} \\right| \\cdot \\exp\\left\\{ z^\\top (T^{-1})^\\top T^{-1} z \\right\\} \\\\\n",
    "&= \\frac{1}{2\\pi} \\left| \\frac{1}{\\sqrt{\\det{\\Sigma}}} \\right| \\cdot \\exp\\left\\{ z^\\top (S^{-1/2}V^\\top)^\\top S^{-1/2}V^\\top z \\right\\} \\\\\n",
    "&= \\frac{1}{2\\pi\\sqrt{\\det{\\Sigma}}} \\cdot \\exp\\left\\{ z^\\top V S^{-1}V^\\top z \\right\\} \\\\\n",
    "&= \\frac{1}{2\\pi\\sqrt{\\det{\\Sigma}}} \\cdot \\exp\\left\\{ z^\\top \\Sigma^{-1} z \\right\\} \n",
    "\\end{align}\n",
    "\n",
    "This shows that transforming $x$ by $T$ is equivalent to sampling $z = Tx$ from a new Gaussian distribution with covariance matrix $\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Linear Regression (Likelihood)\n",
    "\n",
    "Given the following variables: a data point $x \\in \\mathbb{R}^d$ and its label $y^* \\in \\mathbb{R}$. $x$ and $y^*$ are related by the following linear function.\n",
    "\n",
    "$$y^* = x^\\top w^*$$\n",
    "\n",
    "However, in this case, we can only observe a noisy version of $y^*$:\n",
    "\n",
    "$$y = y^* + \\epsilon = x^\\top w^* + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. In other words, we are given a training set of $n$ samples: $\\{(x_1,y_1),\\dots,(x_n,y_n)\\}$. This is the same setting as in Problem 2 of HW3 where we will later show the probabilistic interpretation of least square and ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set style for prettier plots\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "def generate_data(n, d, sigma):\n",
    "    np.random.seed(1)\n",
    "    if d == 1:\n",
    "        w = np.array([1.])\n",
    "    elif d == 2:\n",
    "        w = np.array([-0.5, 0.6])\n",
    "    else:\n",
    "        w = np.ones(d)\n",
    "    X = np.random.rand(n, d) * 2 - 1\n",
    "    y = X.dot(w) + np.random.randn(n) * sigma\n",
    "    return y, X, w\n",
    "    \n",
    "num_samples = 200\n",
    "d = 1\n",
    "sigma = 0.1\n",
    "y, X, w = generate_data(num_samples, d, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data we have\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probability of $y$ given $x$\n",
    "\n",
    "Using the linear model given above, compute the conditional distribution $y$ given $x$, $P(y \\mid x)$, as a function of $w$. Answer the following questions and __fill in the code below__.\n",
    "- __What family of distribution is the conditional probability?__\n",
    "- __What are its mean and variance?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional(y, x, w):\n",
    "    # TODO: Evaluate the conditional probability of y given x at some w.\n",
    "    ### start 1 ##\n",
    "    \n",
    "    ### end 1 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood (ML)\n",
    "\n",
    "We want to find the best weight $w$ that estimates the true unknown $w^*$. Given $n$ data samples $\\{(x_1, y_1),\\dots,(x_n, y_n)\\}$, we can compute the likelihood function:\n",
    "\n",
    "$$\\text{Likelihood}(w) := p(y_1,\\dots,y_n \\mid x_1,\\dots,x_n) = \\Pi_{i=1}^n ~p(y_i \\mid x_i)$$\n",
    "\n",
    "ML solution is the weight $w$ that, as the name suggests, maximizes the likelihood of the observed data. Run the following code to plot the likelihood and the log likelihood functions. Likelihood tends to get very small very quickly so often we compute the log conditionals and sum them instead.\n",
    "\n",
    "$$\\text{Log-Likelihood}(w) := \\log p(y_1,\\dots,y_n \\mid x_1,\\dots,x_n) = \\sum_{i=1}^n ~ \\log p(y_i \\mid x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "W = np.linspace(-5, 5, N)\n",
    "\n",
    "def compute_likelihood_1d(X, y, W, n):\n",
    "    likelihood = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        likelihood[i] = conditional(y[:n], X[:n], np.array([W[i]])).prod()\n",
    "    return likelihood\n",
    "\n",
    "def plot_likelihood_1d(n):\n",
    "    likelihood = compute_likelihood_1d(X, y, W, n)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    w_ml = W[likelihood.argmax()]\n",
    "    ml = likelihood.max()\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(W, likelihood, color='blue')\n",
    "    plt.vlines(w_ml, 0, ml, color='black', linestyle='--', label=r'$w_{ML}$')\n",
    "    plt.xlabel('w')\n",
    "    plt.ylabel('Likelihood')\n",
    "    plt.xticks(np.arange(-5, 5, 1))\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(W, np.log(likelihood), color='blue')\n",
    "    plt.vlines(w_ml, np.log(likelihood).min(), np.log(ml), color='black', \n",
    "               linestyle='--', label=r'$w_{ML}$')\n",
    "    plt.xlabel('w')\n",
    "    plt.ylabel('Log Likelihood')\n",
    "    plt.xticks(np.arange(-5, 5, 1))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print('ML solution: w_ml = %.8f' % w_ml)\n",
    "\n",
    "interactive_plot = interactive(plot_likelihood_1d, n=(1, 100, 5))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If your code is correct, with $n$ equal to 1, you should see a wide normal distribution with a peak density of around 0.4.__\n",
    "\n",
    "Answer the following questions:\n",
    "- __What is the ML solution? In other words, which value of $w$ maximizes the likelihood?__\n",
    "- __What do you notice when $n$ increases/decreases?__ (also see the y axis)\n",
    "- __Why the shape of the log likelihood curve does not seem to change with $n$?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Linear Regression (Posterior)\n",
    "\n",
    "Using the same model and data as in part (b), we want to take a different approach in finding the optimal $w^*$. Let's now assume that $w$ is a random variable drawn also from a Gaussian distribution with mean $\\mu_w$ and variance $\\sigma_w^2$, i.e. $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2 I)$. This is an extra condition we impose on $w$, a _prior_ knowledge we have about $w$ before even seeing any data samples! So in this case, the _prior_ distribution of $w$ is $\\mathcal{N}(\\mu_w, \\sigma_w^2 I)$.\n",
    "\n",
    "The posterior distribution of $w$, given the same training data, is \n",
    "\n",
    "$$p(w \\mid y_1,\\dots,y_n, x_1,\\dots,x_n)$$\n",
    "\n",
    "It is the distribution of $w$ now that we see the training data. From Bayes' theorem, we can write the posterior in term of the likelihood and the prior.\n",
    "\n",
    "$$\n",
    "p(w \\mid y_1,\\dots,y_n, x_1,\\dots,x_n) ~=~ \\frac{p(y_1,\\dots,y_n \\mid x_1,\\dots,x_n,w) \\cdot p(w)}{p(y_1,\\dots,y_n)}\n",
    "$$\n",
    "\n",
    "We will derive the posterior analytically in the next section as well as in Problem 1 of the homework. For now, we will plot it and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "W = np.linspace(-5, 5, N)\n",
    "\n",
    "def plot_posterior_1d(n, mu_w, sigma_w):\n",
    "    likelihood = compute_likelihood_1d(X, y, W, n)\n",
    "    prior = np.exp(-0.5 * (W - mu_w) ** 2 / sigma_w ** 2) / np.sqrt(2 * np.pi)\n",
    "    py = np.exp(- (y[:n] - w * X[:n].flatten()) ** 2) / np.sqrt(2 * np.pi)\n",
    "    posterior = likelihood * prior / py.prod()\n",
    "    w_map = W[posterior.argmax()]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(6, 6))\n",
    "    ax1.plot(W, posterior, color='blue', label='Posterior')\n",
    "    ax1.vlines(w_map, 0, posterior.max(),  color='black', \n",
    "               label=r'$w_{MAP}$', linestyle='--')\n",
    "    ax1.set_xlabel('w')\n",
    "    ax1.set_ylabel('Posterior')\n",
    "    ax1.set_xticks(np.arange(-5, 5, 1))\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.legend(loc=2)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(W, prior, color='red', label='Prior')\n",
    "    ax2.set_ylabel('Prior')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2.legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('MAP solution: w_map = %.8f' % w_map)\n",
    "\n",
    "interactive_plot = interactive(plot_posterior_1d, n=(1, 25), \n",
    "                               mu_w=(-5, 5), sigma_w=(0.1, 5, 0.5))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive plot may have some delay. Give it some time :)\n",
    "\n",
    "Answer the following questions:\n",
    "- __Set mean of the prior $\\mu_w = 1$, what is the MAP solution?__\n",
    "- __Set $\\mu_w = -1$ with a large $\\sigma_w$ and large $n$, what is the MAP solution?__\n",
    "- __With a small $\\sigma_w$ and small $n$, how does changing $\\mu_w$ affect MAP solution?__\n",
    "- __What is the effect of $n$ and $\\sigma_w$ on the posterior?__\n",
    "\n",
    "Well done! You have reached the end of the Jupyter notebook part. Go back to the worksheet and get a head start on the analytical parts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
