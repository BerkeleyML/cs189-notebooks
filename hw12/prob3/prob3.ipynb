{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "import numpy as np\n",
                "import random\n",
                "from tqdm.notebook import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "from copy import deepcopy\n",
                "from zipfile import ZipFile\n",
                "import re\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we use transcripts of sessions of the European parlament as our input. source: http://www.statmt.org/europarl/\n",
                "zfile = ZipFile('europarl_raw.zip')\n",
                "fNames = zfile.namelist()\n",
                "contents = [b''.join(zfile.open(fName).readlines()) for fName in fNames if re.match(r'.*ep.*en',fName)]\n",
                "rawText = r'\\n'.join([string.decode('utf-8') for string in contents])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# now we extract all words and place them in a list, performing some processing steps\n",
                "# to reduce our vocabulary\n",
                "# making all words lowercase and removing punctuation\n",
                "tokens = [word.lower() for word in nltk.wordpunct_tokenize(rawText) if word.isalpha()]\n",
                "# now we get rid of stop words\n",
                "# (words that appear often and don't add much to the meaning of a sentence)\n",
                "from nltk.corpus import stopwords\n",
                "# READ THIS!!!! If the following line gives you errors, run the line\n",
                "#nltk.download('stopwords')\n",
                "stops = set(stopwords.words('english'))\n",
                "tokens = [token for token in tokens if not token in stops]\n",
                "numTokens = len(tokens)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# compute the frequency of ocurrence of each word in vocabulary\n",
                "vocab, vocabFreqs = np.unique(tokens,return_counts=True)\n",
                "vocabSize = len(vocab)\n",
                "print(\"The vocabulary has {} words\".format(vocabSize))\n",
                "idx = vocab.argsort()\n",
                "vocabFreqs = vocabFreqs[idx] * 1.0/numTokens\n",
                "# map words to their indices in the vocab list\n",
                "word2Ind = {word:i for i, word in enumerate(vocab)}\n",
                "# generate a new array of tokens that maps each word to its index in the dictionary\n",
                "# Note this is equivalent to a one hot encoding of the words!!\n",
                "indexedTokens = [word2Ind[token] for token in tokens]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# length of embedding (each word is mapped to a vector of this length)\n",
                "d = 50\n",
                "# define the embeddings\n",
                "seed = 10000\n",
                "np.random.seed(seed)\n",
                "centerEmb = 2.0/(d + vocabSize)* np.random.randn(vocabSize, d)\n",
                "contextEmb = deepcopy(centerEmb).transpose()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part C\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# the hyperparameter L sets the length of the window we are using\n",
                "L = 3\n",
                "eta = 0.001\n",
                "seed = 100\n",
                "random.seed(seed)\n",
                "epbar = tqdm(range(3))\n",
                "iterable = range(numTokens)\n",
                "itbar = tqdm(iterable)\n",
                "for epoch in epbar:\n",
                "    epbar.set_description(\"Processing epoch %d\" % epoch)\n",
                "    itbar.refresh()\n",
                "    itbar.reset()\n",
                "    for i in iterable:\n",
                "        itbar.update()\n",
                "        centerIdx = random.randint(L,numTokens-L-1)\n",
                "        center = indexedTokens[centerIdx]\n",
                "        # positive examples -> every word in the window is a context\n",
                "        for j in range(centerIdx-L,centerIdx+L+1):\n",
                "            if j == 0 : next\n",
                "            context = indexedTokens[j]\n",
                "            ### start code ###\n",
                "\n",
                "            ### end code ###\n",
                "        # negative examples -> choose words randomly from the vocab and call them negative examples\n",
                "        negIdx = random.choices(range(vocabSize),weights=vocabFreqs,k=5)\n",
                "        for j in negIdx:\n",
                "            context = j\n",
                "            e = np.exp(-(centerEmb[[center],:] @ contextEmb[:,[context]])[0][0])\n",
                "            coeff = - 1.0 / (1 + e)\n",
                "            centerEmb[[center],:] -= eta * coeff * contextEmb[:,[context]].transpose()\n",
                "            contextEmb[:,[context]] -= eta * coeff * centerEmb[[center],:].transpose()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Since we will use cosine distance, we first normalize all the vectors\n",
                "centerEmb = centerEmb / np.linalg.norm(centerEmb, axis=1,keepdims=True)\n",
                "contextEmb = contextEmb / np.linalg.norm(contextEmb, axis=0,keepdims=True)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part D\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "word = 'diplomat'\n",
                "idx = np.where(vocab == word)[0][0]\n",
                "v = centerEmb[[idx],:]\n",
                "# find 5 closest words (in angle) to word\n",
                "a = np.abs(v @ contextEmb)[0]\n",
                "ind = np.argpartition(a, -5)[-5:]\n",
                "vocab[ind]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part F\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we will introduce a new word in exactly the same contexts as an existing word\n",
                "origWord = 'rights'\n",
                "newWord = 'eskubideak' # Basque for rights -> note that we don't even need to use this string to find its embedding\n",
                "origVocabIdx = word2Ind[origWord] # index of our word in the dictionary\n",
                "\n",
                "# find all places where the original word appeared in the string\n",
                "origTokenIndices = np.where(np.array(indexedTokens) == origVocabIdx)[0]\n",
                "\n",
                "# define the embedding vectors for the new word. We will only train the center embedding\n",
                "newCenterEmb = 2.0/(d + vocabSize) * np.random.randn(vocabSize, d)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we train the new vectors\n",
                "seed = 100\n",
                "random.seed(seed)\n",
                "epbar = tqdm(range(3))\n",
                "iterable = range(len(origTokenIndices))\n",
                "itbar = tqdm(iterable)\n",
                "for epoch in epbar:\n",
                "    epbar.set_description(\"Processing epoch %d\" % epoch)\n",
                "    itbar.refresh()\n",
                "    itbar.reset()\n",
                "    for i in iterable:\n",
                "        itbar.update()\n",
                "        centerIdx = random.choice(origTokenIndices)\n",
                "        # positive examples -> every word in the window is a context\n",
                "        for j in range(centerIdx-L,centerIdx+L+1):\n",
                "            if j == 0 : next\n",
                "            context = indexedTokens[j]\n",
                "            ### start code ###\n",
                "\n",
                "            ### end code ###\n",
                "        # negative examples -> choose words randomly from the vocab and call them negative examples\n",
                "        negIdx = random.choices(range(vocabSize),weights=vocabFreqs,k=5)\n",
                "        for j in negIdx:\n",
                "            context = j\n",
                "            e = np.exp(-(newCenterEmb @ contextEmb[:,[context]])[0][0])\n",
                "            coeff = - 1.0 / (1 + e)\n",
                "            newCenterEmb -= eta * coeff * contextEmb[:,[context]].transpose()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# now find the words closest to the new word\n",
                "# find 5 closest words (in angle) to word\n",
                "a = np.abs(newCenterEmb @ contextEmb)[0]\n",
                "ind = np.argpartition(a, -5)[-5:]\n",
                "print(\"word = score: \" + ', '.join(['{} = {:.2f}'.format(vocab[i],a[i]) for i in ind]))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part G\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.decomposition import PCA\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "zfile = ZipFile('glove.6B.50d.zip')\n",
                "fName = zfile.namelist()[0]\n",
                "vocab = []\n",
                "vectors = []\n",
                "for line in zfile.open(fName).readlines():\n",
                "    lst = line.split()\n",
                "    vocab.append(lst[0].decode('utf-8'))\n",
                "    vector = np.array(lst[1:],dtype=np.float)\n",
                "    vectors.append(vector)\n",
                "vectors = np.array(vectors)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define a map from word to index in vocab array\n",
                "word2Ind = {word:i for i, word in enumerate(vocab)}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define list of words\n",
                "wordList = ['chair', 'banana', 'apple', 'car', 'wheel',\n",
                "            'table', 'desk', 'building', 'gas']\n",
                "\n",
                "#\n",
                "indices = [word2Ind[word] for word in wordList]\n",
                "theseVecs = vectors[indices,:]\n",
                "pca = PCA(n_components = 2)\n",
                "transf = pca.fit_transform(theseVecs)\n",
                "fig, ax = plt.subplots()\n",
                "ax.scatter(transf[:,0],transf[:,1],marker='')\n",
                "ax.set_xlabel('PCA dim 1')\n",
                "ax.set_ylabel('PCA dim 2')\n",
                "\n",
                "for i, txt in enumerate(wordList):\n",
                "    ax.annotate(txt, (transf[i,0], transf[i,1]))\n",
                "\n",
                "fig.savefig('glove-scatter.png')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part H\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tellAnalogy(an1a, an1b, an2b):\n",
                "    analogy = vectors[[word2Ind[an1a]],:] - \\\n",
                "        vectors[[word2Ind[an1b]], :] + \\\n",
                "        vectors[[word2Ind[an2b]], :]\n",
                "\n",
                "    distances = np.linalg.norm(vectors - analogy,axis=1)\n",
                "    idx = np.argmin(distances)\n",
                "    print('{} is to {} as {} is to {}'.format(an1b, an1a,an2b, vocab[idx]))\n",
                "\n",
                "\n",
                "# two examples\n",
                "an1a = 'madrid'\n",
                "an1b = 'spain'\n",
                "an2b = 'ghana'\n",
                "tellAnalogy(an1a,an1b,an2b)\n",
                "\n",
                "an1a = 'uncle'\n",
                "an1b = 'nephew'\n",
                "an2b = 'niece'\n",
                "tellAnalogy(an1a,an1b,an2b)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}