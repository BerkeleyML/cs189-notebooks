{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HW12 - Decision Trees and Random Forests\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "import numpy as np\n",
                "from numpy import genfromtxt\n",
                "import scipy.io\n",
                "from scipy import stats\n",
                "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
                "from sklearn.base import BaseEstimator, ClassifierMixin\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "import random\n",
                "random.seed(246810)\n",
                "np.random.seed(246810)\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eps = 1e-5  # a small number\n",
                "# Vectorized function for hashing for np efficiency\n",
                "def w(x):\n",
                "    return np.int(hash(x)) % 1000\n",
                "\n",
                "h = np.vectorize(w)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (a). Pre-process the data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
                "    # fill_mode = False\n",
                "\n",
                "    # Temporarily assign -1 to missing data\n",
                "    data[data == b''] = '-1'\n",
                "\n",
                "    # Hash the columns (used for handling strings)\n",
                "    onehot_encoding = []\n",
                "    onehot_features = []\n",
                "    for col in onehot_cols:\n",
                "        counter = Counter(data[:, col])\n",
                "        for term in counter.most_common():\n",
                "            if term[0] == b'-1':\n",
                "                continue\n",
                "            if term[-1] <= min_freq:\n",
                "                break\n",
                "            onehot_features.append(term[0])\n",
                "            onehot_encoding.append((data[:, col] == term[0]).astype(np.float))\n",
                "        data[:, col] = '0'\n",
                "    onehot_encoding = np.array(onehot_encoding).T\n",
                "    data = np.hstack(\n",
                "        [np.array(data, dtype=np.float),\n",
                "         np.array(onehot_encoding)])\n",
                "\n",
                "    # Replace missing data with the mode value. We use the mode instead of\n",
                "    # the mean or median because this makes more sense for categorical\n",
                "    # features such as gender or cabin type, which are not ordered.\n",
                "    if fill_mode:\n",
                "        for i in range(data.shape[-1]):\n",
                "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
                "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
                "            data[(data[:, i] > -1 - eps) *\n",
                "                 (data[:, i] < -1 + eps)][:, i] = mode\n",
                "\n",
                "    return data, onehot_features\n",
                "\n",
                "\n",
                "def evaluate(clf, print_splits=True):\n",
                "    print(\"Cross validation\", cross_val_score(clf, X, y, cv=3))\n",
                "    if hasattr(clf, \"decision_trees\"):\n",
                "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
                "        first_splits = [\n",
                "            (features[term[0]], term[1]) for term in counter.most_common()\n",
                "        ]\n",
                "        if print_splits == True:\n",
                "            print(\"First splits\", first_splits)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setup dataset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# dataset = \"titanic\"\n",
                "dataset = \"spam\"\n",
                "params = {\n",
                "    \"max_depth\": 5,\n",
                "    \"min_samples_leaf\": 10,\n",
                "}\n",
                "N = 200\n",
                "\n",
                "if dataset == \"titanic\":\n",
                "    # Load titanic data\n",
                "    path_train = 'titanic_training.csv'\n",
                "    data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
                "    path_test = 'titanic_testing_data.csv'\n",
                "    test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
                "    y = data[1:, 0]  # label = survived\n",
                "    path_y_test = 'titanic_testing_labels_only.txt'\n",
                "    y_test = genfromtxt(path_y_test, delimiter=',', dtype=None)\n",
                "    class_names = [\"Died\", \"Survived\"]\n",
                "\n",
                "    labeled_idx = np.where(y != b'')[0]\n",
                "    y = np.array(y[labeled_idx], dtype=np.int)\n",
                "    print(\"Part (a): preprocessing the titanic dataset\")\n",
                "    X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
                "    X = X[labeled_idx, :]\n",
                "    Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
                "    assert X.shape[1] == Z.shape[1]\n",
                "    features = list(data[0, 1:]) + onehot_features\n",
                "\n",
                "elif dataset == \"spam\":\n",
                "    features = [\n",
                "        \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
                "        \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
                "        \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
                "        \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
                "        \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
                "        \"square_bracket\", \"ampersand\"\n",
                "    ]\n",
                "    assert len(features) == 32\n",
                "\n",
                "    # Load spam data\n",
                "    path_train = 'spam_data.mat'\n",
                "    data = scipy.io.loadmat(path_train)\n",
                "    X = data['training_data']\n",
                "    y = np.squeeze(data['training_labels'])\n",
                "    Z = data['test_data']\n",
                "    path_y_test = 'spam_test_labels_only.txt'\n",
                "    y_test = genfromtxt(path_y_test, delimiter=',', dtype=None)\n",
                "    class_names = [\"Ham\", \"Spam\"]\n",
                "\n",
                "else:\n",
                "    raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
                "\n",
                "print('==============================================')\n",
                "print(\"Features\", features)\n",
                "print('==============================================')\n",
                "print(\"Train/test size\", X.shape, Z.shape)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (b). Implement basic decision tree\n",
                "\n",
                "Implement the\n",
                "\n",
                "**information gain**, i.e., entropy of the parent node minus the weighted sum of entropy of the child nodes\n",
                "\n",
                "**Gini purification**, i.e., Gini impurity of the parent node minus the weighted sum of Gini impurities of the child nodes splitting rules for greedy decision tree learning.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecisionTree:\n",
                "\n",
                "    def __init__(self, max_depth=3, feature_labels=None):\n",
                "        self.max_depth = max_depth\n",
                "        self.features = feature_labels\n",
                "        self.left, self.right = None, None  # for non-leaf nodes\n",
                "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
                "        self.data, self.pred = None, None  # for leaf nodes\n",
                "\n",
                "    @staticmethod\n",
                "    def entropy(y):\n",
                "        if y.size == 0:\n",
                "            return 0\n",
                "        p0 = np.where(y < 0.5)[0].size / y.size\n",
                "        if np.abs(p0) < 1e-10 or np.abs(1 - p0) < 1e-10:\n",
                "            return 0\n",
                "        # TODO: compute entropy_value\n",
                "\n",
                "        ### start entropy_code ###\n",
                "\n",
                "        ### end entropy_code ###\n",
                "\n",
                "        return entropy_value\n",
                "\n",
                "    @staticmethod\n",
                "    def information_gain(X, y, thresh):\n",
                "        base = DecisionTree.entropy(y)\n",
                "        y0 = y[np.where(X < thresh)[0]]\n",
                "        p0 = y0.size / y.size\n",
                "        y1 = y[np.where(X >= thresh)[0]]\n",
                "        p1 = y1.size / y.size\n",
                "        # TODO: compute entropy_children\n",
                "\n",
                "        ### start information_gain_code ###\n",
                "\n",
                "        ### end information_gain_code ###\n",
                "\n",
                "        return base - entropy_children\n",
                "\n",
                "    @staticmethod\n",
                "    def gini_impurity(X, y, thresh):\n",
                "        if y.size == 0:\n",
                "            return 0\n",
                "        p0 = np.where(y < 0.5)[0].size / y.size\n",
                "        if np.abs(p0) < 1e-10 or np.abs(1 - p0) < 1e-10:\n",
                "            return 0\n",
                "        # TODO: compute entropy_value\n",
                "\n",
                "        ### start gini_impurity_code ###\n",
                "\n",
                "        ### end gini_impurity_code ###\n",
                "\n",
                "        return gini_impurity_value\n",
                "\n",
                "    @staticmethod\n",
                "    def gini_purification(X, y, thresh):\n",
                "        base = DecisionTree.gini_impurity(y)\n",
                "        y0 = y[np.where(X < thresh)[0]]\n",
                "        p0 = y0.size / y.size\n",
                "        y1 = y[np.where(X >= thresh)[0]]\n",
                "        p1 = y1.size / y.size\n",
                "        # TODO: compute entropy_children\n",
                "\n",
                "        ### start gini_purification_code ###\n",
                "\n",
                "        ### end gini_purification_code ###\n",
                "\n",
                "        return base - gini_impurity_children\n",
                "\n",
                "    def split(self, X, y, idx, thresh):\n",
                "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
                "        y0, y1 = y[idx0], y[idx1]\n",
                "        return X0, y0, X1, y1\n",
                "\n",
                "    def split_test(self, X, idx, thresh):\n",
                "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
                "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
                "        X0, X1 = X[idx0, :], X[idx1, :]\n",
                "        return X0, idx0, X1, idx1\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        if self.max_depth > 0:\n",
                "            # compute entropy gain for all single-dimension splits,\n",
                "            # thresholding with a linear interpolation of 10 values\n",
                "            gains = []\n",
                "            # The following logic prevents thresholding on exactly the minimum\n",
                "            # or maximum values, which may not lead to any meaningful node\n",
                "            # splits.\n",
                "            thresh = np.array([\n",
                "                np.linspace(\n",
                "                    np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10)\n",
                "                for i in range(X.shape[1])\n",
                "            ])\n",
                "            for i in range(X.shape[1]):\n",
                "                gains.append([\n",
                "                    self.information_gain(X[:, i], y, t) for t in thresh[i, :]\n",
                "                ])\n",
                "\n",
                "            gains = np.nan_to_num(np.array(gains))\n",
                "            self.split_idx, thresh_idx = np.unravel_index(\n",
                "                np.argmax(gains), gains.shape)\n",
                "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
                "            X0, y0, X1, y1 = self.split(\n",
                "                X, y, idx=self.split_idx, thresh=self.thresh)\n",
                "            if X0.size > 0 and X1.size > 0:\n",
                "                self.left = DecisionTree(\n",
                "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
                "                self.left.fit(X0, y0)\n",
                "                self.right = DecisionTree(\n",
                "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
                "                self.right.fit(X1, y1)\n",
                "            else:\n",
                "                self.max_depth = 0\n",
                "                self.data, self.labels = X, y\n",
                "                self.pred = stats.mode(y).mode[0]\n",
                "        else:\n",
                "            self.data, self.labels = X, y\n",
                "            self.pred = stats.mode(y).mode[0]\n",
                "        return self\n",
                "\n",
                "    def predict(self, X):\n",
                "        if self.max_depth == 0:\n",
                "            return self.pred * np.ones(X.shape[0])\n",
                "        else:\n",
                "            X0, idx0, X1, idx1 = self.split_test(\n",
                "                X, idx=self.split_idx, thresh=self.thresh)\n",
                "            yhat = np.zeros(X.shape[0])\n",
                "            yhat[idx0] = self.left.predict(X0)\n",
                "            yhat[idx1] = self.right.predict(X1)\n",
                "            return yhat\n",
                "\n",
                "    def __repr__(self):\n",
                "        if self.max_depth == 0:\n",
                "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
                "        else:\n",
                "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
                "                                           self.thresh, self.left.__repr__(),\n",
                "                                           self.right.__repr__())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (c). Train a shallow decision tree on the Titanic and Spam dataset and visualize your tree.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Train and evaluate the decision tree\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic decision tree\n",
                "max_depth = 3\n",
                "print('==============================================')\n",
                "print(\"Part (a, c): simplified decision tree\")\n",
                "dt = DecisionTree(max_depth=max_depth, feature_labels=features)\n",
                "dt.fit(X, y)\n",
                "print('==============================================')\n",
                "print(\"Tree structure\", dt.__repr__())\n",
                "\n",
                "# Basic decision tree\n",
                "print('==============================================')\n",
                "print(\"Part (c): sklearn's decision tree\")\n",
                "params = {\n",
                "    \"max_depth\": max_depth,\n",
                "    \"min_samples_leaf\": 10,\n",
                "}\n",
                "clf = DecisionTreeClassifier(random_state=0, **params)\n",
                "clf.fit(X, y)\n",
                "evaluate(clf)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Visualize your tree**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydot import graph_from_dot_data\n",
                "import io\n",
                "out = io.StringIO()\n",
                "export_graphviz(clf, out_file=out, feature_names=features, class_names=class_names)\n",
                "# For OSX, may need the following for dot: brew install gprof2dot\n",
                "graph = graph_from_dot_data(out.getvalue())\n",
                "graph_from_dot_data(out.getvalue())[0].write_pdf(\"{}-tree-depth{}.pdf\".format(dataset, max_depth))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (d). Implement bagged trees\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
                "\n",
                "    def __init__(self, params=None, n=200):\n",
                "        if params is None:\n",
                "            params = {}\n",
                "        self.params = params\n",
                "        self.n = n\n",
                "        self.decision_trees = [\n",
                "            DecisionTreeClassifier(random_state=i, **self.params)\n",
                "            for i in range(self.n)\n",
                "        ]\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        for i in range(self.n):\n",
                "            idx = np.random.randint(0, X.shape[0], X.shape[0])\n",
                "            newX, newy = X[idx, :], y[idx]\n",
                "            self.decision_trees[i].fit(newX, newy)\n",
                "        return self\n",
                "\n",
                "    def predict(self, X):\n",
                "        yhat = [self.decision_trees[i].predict(X) for i in range(self.n)]\n",
                "        # TODO: compute yhat_avg for BaggedTrees\n",
                "\n",
                "        ### start code ###\n",
                "\n",
                "        ### end code ###\n",
                "\n",
                "        return yhat_avg\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (e). Apply bagged trees to the titanic and spam datasets.\n",
                "\n",
                "Find and state the most common splits made at the root node of the trees.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bagged trees\n",
                "print(\"Part (e): bagged trees\")\n",
                "bt = BaggedTrees(params, n=N)\n",
                "bt.fit(X, y)\n",
                "evaluate(bt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (f). Implement random forests\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RandomForest(BaggedTrees):\n",
                "\n",
                "    def __init__(self, params=None, n=200, m=1):\n",
                "        if params is None:\n",
                "            params = {}\n",
                "        params['max_features'] = m\n",
                "        super().__init__(params=params, n=n)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (g). Apply bagged random forests to the titanic and spam datasets.\n",
                "\n",
                "Find and state the most common splits made at the root node of the trees.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random forest\n",
                "print(\"Part (g): random forest\")\n",
                "rf = RandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
                "rf.fit(X, y)\n",
                "evaluate(rf)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (h). Implement AdaBoost algorithm\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BoostedRandomForest(RandomForest):\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
                "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
                "        i = 0\n",
                "        while i < self.n:\n",
                "            idx = np.random.choice(X.shape[0], size=X.shape[0], p=self.w)\n",
                "            newX, newy = X[idx, :], y[idx]\n",
                "            self.decision_trees[i].fit(newX, newy)\n",
                "            wrong = np.abs((y - self.decision_trees[i].predict(X)))\n",
                "            error = wrong.dot(self.w) / np.sum(self.w)\n",
                "            self.a[i] = 0.5 * np.log((1 - error) / error)\n",
                "            # Update w\n",
                "            wrong_idx = np.where(wrong > 0.5)[0]\n",
                "            right_idx = np.where(wrong <= 0.5)[0]\n",
                "            # TODO: fill in the code for updating 'self.w'\n",
                "\n",
                "            ### start code ###\n",
                "\n",
                "        ### end code ###\n",
                "\n",
                "        return yhat_BoostedRandomForest\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Boosted random forest\n",
                "print(\"Part (h): boosted random forest\")\n",
                "boosted = BoostedRandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
                "boosted.fit(X, y)\n",
                "evaluate(boosted)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (i). Summarize the performance evaluation of: a single decision tree, bagged trees, random forests, and boosted trees.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    \"max_depth\": 5,\n",
                "    \"min_samples_leaf\": 10,\n",
                "}\n",
                "N = 200\n",
                "max_depth = 5\n",
                "\n",
                "# Constant classifier\n",
                "print('==============================================')\n",
                "print(\"Part (0): constant classifier\")\n",
                "print('constant classifier test accuracy: ', accuracy_score(np.zeros_like(y_test), y_test))\n",
                "\n",
                "# Basic decision tree\n",
                "print('==============================================')\n",
                "print(\"Part (c): simplified decision tree\")\n",
                "dt = DecisionTree(max_depth=max_depth, feature_labels=features)\n",
                "dt.fit(X, y)\n",
                "print('Basic decision tree test accuracy: ', accuracy_score(dt.predict(Z), y_test))\n",
                "\n",
                "# Basic decision tree\n",
                "print('==============================================')\n",
                "print(\"Part (c): sklearn's decision tree\")\n",
                "params = {\n",
                "    \"max_depth\": max_depth,\n",
                "    \"min_samples_leaf\": 10,\n",
                "}\n",
                "clf = DecisionTreeClassifier(random_state=0, **params)\n",
                "clf.fit(X, y)\n",
                "evaluate(clf, print_splits=False)\n",
                "print('sklearn decision tree test accuracy: ', accuracy_score(clf.predict(Z), y_test))\n",
                "\n",
                "\n",
                "# Bagged trees\n",
                "print('==============================================')\n",
                "print(\"Part (e): bagged trees\")\n",
                "bt = BaggedTrees(params, n=N)\n",
                "bt.fit(X, y)\n",
                "evaluate(bt, print_splits=False)\n",
                "print('bagged trees test accuracy: ', accuracy_score(bt.predict(Z), y_test))\n",
                "\n",
                "\n",
                "# Random forest\n",
                "print('==============================================')\n",
                "print(\"Part (g): random forest\")\n",
                "rf = RandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
                "rf.fit(X, y)\n",
                "evaluate(rf, print_splits=False)\n",
                "print('random forest test accuracy: ', accuracy_score(rf.predict(Z), y_test))\n",
                "\n",
                "\n",
                "# Boosted random forest\n",
                "print('==============================================')\n",
                "print(\"Part (h): boosted random forest\")\n",
                "boosted = BoostedRandomForest(params, n=N, m=np.int(np.sqrt(X.shape[1])))\n",
                "boosted.fit(X, y)\n",
                "evaluate(boosted, print_splits=False)\n",
                "print('boosted random forest test accuracy: ', accuracy_score(boosted.predict(Z), y_test))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Part (j). For the spam dataset only: Describe what kind of data are the most challenging to classify and which are the easiest.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if dataset == 'titanic':\n",
                "    import sys\n",
                "    sys.exit()\n",
                "# The following is for the spam dataset only.\n",
                "# Sample code for determining which data are easier/harder to classify\n",
                "print(\"\\n\\nPart (i): easy/hard examples\")\n",
                "A = np.argsort(boosted.w)\n",
                "tail = 1000\n",
                "bits = np.array([np.sum(X[A[i], :-7]) for i in range(X.shape[0])])\n",
                "print(\n",
                "    \"Number of easiest %s samples containing >= 2 feature counts:\" % tail,\n",
                "    np.sum(bits[:tail] >= 2))\n",
                "print(\"Number of hardest %s samples containing < 2 feature counts:\" % tail,\n",
                "      np.sum(bits[-tail:] < 2))\n",
                "bits = np.array([np.sum(X[A[i], :]) for i in range(X.shape[0])])\n",
                "print(\n",
                "    \"Number of easiest %s samples containing >= 5 feature counts:\" % tail,\n",
                "    np.sum(bits[:tail] >= 5))\n",
                "print(\"Number of hardest %s samples containing < 5 feature counts:\" % tail,\n",
                "      np.sum(bits[-tail:] < 5))\n",
                "idxes = 200 - np.where(bits[-200:] > 7)[0]\n",
                "print(\"Most challenging samples: Examples among the 200 data most likely \"\n",
                "      \"to be sampled which contained at least 7 feature counts\")\n",
                "for idx in idxes:\n",
                "    print('Example:', class_names[y[A[-idx]]],\n",
                "          [x for x in zip(features, X[A[-idx], :]) if x[1] > 0],\n",
                "          bits[-idx])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}